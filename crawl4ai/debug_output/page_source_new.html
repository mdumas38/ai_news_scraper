<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <title>
   Artificial Intelligence
  </title>
  <meta content="width=device-width, initial-scale=1" name="viewport"/>
  <link href="/static/browse/0.3.4/images/icons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
  <link href="/static/browse/0.3.4/images/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
  <link href="/static/browse/0.3.4/images/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
  <link href="/static/browse/0.3.4/images/icons/site.webmanifest" rel="manifest"/>
  <link color="#5bbad5" href="/static/browse/0.3.4/images/icons/safari-pinned-tab.svg" rel="mask-icon"/>
  <meta content="#da532c" name="msapplication-TileColor"/>
  <meta content="#ffffff" name="theme-color"/>
  <link href="/static/browse/0.3.4/css/arXiv.css?v=20240822" media="screen" rel="stylesheet" type="text/css">
   <link href="/static/browse/0.3.4/css/arXiv-print.css?v=20200611" media="print" rel="stylesheet" type="text/css">
    <link href="/static/browse/0.3.4/css/browse_search.css" media="screen" rel="stylesheet" type="text/css">
     <script language="javascript" src="/static/browse/0.3.4/js/accordion.js">
     </script>
     <link href="/static/browse/0.3.4/css/slider.css?v=1.14" media="screen" rel="stylesheet" type="text/css">
      <script src="//code.jquery.com/jquery-latest.min.js" type="text/javascript">
      </script>
      <script src="/static/browse/0.3.4/js/donate.js?v=1.11" type="text/javascript">
      </script>
      <script src="/static/browse/0.3.4/js/mathjaxToggle.min.js" type="text/javascript">
      </script>
      <script language="javascript" type="text/javascript">
       mathjaxToggle();
      </script>
     </link>
    </link>
   </link>
  </link>
 </head>
 <body class="with-cu-identity">
  <aside class="slider-wrapper bps-banner forum grey">
   <a class="close-slider bps-banner" href="#">
    <img alt="close this message" src="/static/browse/0.3.4/images/icons/close-slider.png"/>
   </a>
   <div class="columns">
    <img alt="arXiv Accessibility Forum 2024" class="bps-banner-image" role="presentation" src="/static/browse/0.3.4/images/events/arxiv-forum-logotype-title.png"/>
    <div class="copy-donation bps-banner">
     <h2 id="forum-session-title">
     </h2>
     <p id="forum-session-link-small">
     </p>
    </div>
    <div class="amount-donation bps-banner">
     <div class="donate-cta" id="forum-session-link-box">
     </div>
    </div>
   </div>
  </aside>
  <script src="/static/browse/0.3.4/js/rotate_forum_info.js?v=1.1" type="text/javascript">
  </script>
  <div class="flex-wrap-footer">
   <header>
    <a class="is-sr-only" href="#content">
     Skip to main content
    </a>
    <!-- start desktop header -->
    <div class="columns is-vcentered is-hidden-mobile" id="cu-identity">
     <div class="column" id="cu-logo">
      <a href="https://www.cornell.edu/">
       <img alt="Cornell University" src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg"/>
      </a>
     </div>
     <div class="column banner-minimal forum">
      <!-- /from July 7 at 1:00 AM to Sept 2 at 1:00 AM -->
      <p>
       This week: the arXiv Accessibility Forum
      </p>
      <a href="https://accessibility2024.arxiv.org/schedule" target="_blank">
       Forum Schedule
      </a>
     </div>
     <div class="column" id="support-ack">
      <span id="support-ack-url">
       We gratefully acknowledge support from the Simons Foundation,
       <a href="https://info.arxiv.org/about/ourmembers.html">
        member institutions
       </a>
       , and all contributors.
      </span>
      <a class="btn-header-donate" href="https://info.arxiv.org/about/donate.html">
       Donate
      </a>
     </div>
    </div>
    <div class="is-hidden-mobile" id="header">
     <a aria-hidden="true" href="{url_path('ignore_me')}">
     </a>
     <div class="header-breadcrumbs">
      <a href="/">
       <img alt="arxiv logo" src="/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg" style="height:40px;"/>
      </a>
      <span>
       &gt;
      </span>
      <a href="/list/cs.AI/recent">
       cs.AI
      </a>
     </div>
     <div class="search-block level-right">
      <form action="https://arxiv.org/search" class="level-item mini-search" method="GET">
       <div class="field has-addons">
        <div class="control">
         <input aria-label="Search term or terms" class="input is-small" name="query" placeholder="Search..." type="text"/>
         <p class="help">
          <a href="https://info.arxiv.org/help">
           Help
          </a>
          |
          <a href="https://arxiv.org/search/advanced">
           Advanced Search
          </a>
         </p>
        </div>
        <div class="control">
         <div class="select is-small">
          <select aria-label="Field to search" name="searchtype">
           <option selected="selected" value="all">
            All fields
           </option>
           <option value="title">
            Title
           </option>
           <option value="author">
            Author
           </option>
           <option value="abstract">
            Abstract
           </option>
           <option value="comments">
            Comments
           </option>
           <option value="journal_ref">
            Journal reference
           </option>
           <option value="acm_class">
            ACM classification
           </option>
           <option value="msc_class">
            MSC classification
           </option>
           <option value="report_num">
            Report number
           </option>
           <option value="paper_id">
            arXiv identifier
           </option>
           <option value="doi">
            DOI
           </option>
           <option value="orcid">
            ORCID
           </option>
           <option value="author_id">
            arXiv author ID
           </option>
           <option value="help">
            Help pages
           </option>
           <option value="full_text">
            Full text
           </option>
          </select>
         </div>
        </div>
        <input name="source" type="hidden" value="header"/>
        <button class="button is-small is-cul-darker">
         Search
        </button>
       </div>
      </form>
     </div>
    </div>
    <!-- /end desktop header -->
    <div class="mobile-header">
     <div class="columns is-mobile">
      <div class="column logo-arxiv">
       <a href="https://arxiv.org/">
        <img alt="arXiv logo" src="/static/browse/0.3.4/images/arxiv-logomark-small-white.svg" style="height:60px;"/>
       </a>
      </div>
      <div class="column logo-cornell">
       <a href="https://www.cornell.edu/">
        <picture>
         <source media="(min-width: 501px)" sizes="400w" srcset="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w"/>
         <source srcset="/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x"/>
         <img alt="Cornell University Logo" src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg"/>
        </picture>
       </a>
      </div>
      <div class="column nav" id="toggle-container" role="menubar">
       <button class="toggle-control">
        <svg class="icon filter-white" viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg">
         <title>
          open search
         </title>
         <path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z">
         </path>
        </svg>
       </button>
       <div class="mobile-toggle-block toggle-target">
        <form action="https://arxiv.org/search" class="mobile-search-form" method="GET">
         <div class="field has-addons">
          <input aria-label="Search term or terms" class="input" name="query" placeholder="Search..." type="text">
           <input name="source" type="hidden" value="header"/>
           <input name="searchtype" type="hidden" value="all"/>
           <button class="button">
            GO
           </button>
          </input>
         </div>
        </form>
       </div>
       <button class="toggle-control">
        <svg class="icon filter-white" role="menu" viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg">
         <title>
          open navigation menu
         </title>
         <path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z">
         </path>
        </svg>
       </button>
       <div class="mobile-toggle-block toggle-target">
        <nav aria-labelledby="mobilemenulabel" class="mobile-menu">
         <h2 id="mobilemenulabel">
          quick links
         </h2>
         <ul>
          <li>
           <a href="https://arxiv.org/login">
            Login
           </a>
          </li>
          <li>
           <a href="https://info.arxiv.org/help">
            Help Pages
           </a>
          </li>
          <li>
           <a href="https://info.arxiv.org/about">
            About
           </a>
          </li>
         </ul>
        </nav>
       </div>
      </div>
     </div>
    </div>
    <!-- /end mobile-header -->
   </header>
   <main>
    <div id="content">
     <div id="content-inner">
      <div id="dlpage">
       <h1>
        Artificial Intelligence
       </h1>
       <ul>
        <li>
         <a href="#item0">
          New submissions
         </a>
        </li>
        <li>
         <a href="#item11">
          Cross-lists
         </a>
        </li>
        <li>
         <a href="#item66">
          Replacements
         </a>
        </li>
       </ul>
       <p>
        See
        <a aria-labelledby="recent-cs.AI" href="/list/cs.AI/recent" id="recent-cs.AI">
         recent
        </a>
        articles
       </p>
       <div class="paging">
        Total of 116 entries
       </div>
       <div class="morefewer">
        Showing up to 2000 entries per page:
        <a href="/list/cs.AI/new?skip=0&amp;show=1000">
         fewer
        </a>
        |
        <span style="color: #454545">
         more
        </span>
        |
        <span style="color: #454545">
         all
        </span>
       </div>
       <dl id="articles">
        <h3>
         New submissions for Monday, 9 September 2024 (showing 10 of 10 entries )
        </h3>
        <dt>
         <a name="item1">
          [1]
         </a>
         <a href="/abs/2409.03797" id="2409.03797" title="Abstract">
          arXiv:2409.03797
         </a>
         [
         <a aria-labelledby="pdf-2409.03797" href="/pdf/2409.03797" id="pdf-2409.03797" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03797" href="https://arxiv.org/html/2409.03797v1" id="html-2409.03797" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03797" href="/format/2409.03797" id="oth-2409.03797" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API Calls
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Basu,+K">
            Kinjal Basu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Abdelaziz,+I">
            Ibrahim Abdelaziz
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bradford,+K">
            Kelsey Bradford
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Crouse,+M">
            Maxwell Crouse
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kate,+K">
            Kiran Kate
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kumaravel,+S">
            Sadhana Kumaravel
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Goyal,+S">
            Saurabh Goyal
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Munawar,+A">
            Asim Munawar
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rizk,+Y">
            Yara Rizk
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+X">
            Xin Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lastras,+L">
            Luis Lastras
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kapanipathi,+P">
            Pavan Kapanipathi
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Artificial Intelligence (cs.AI)
           </span>
           ; Computation and Language (cs.CL)
          </div>
          <p class="mathjax">
           Autonomous agent applications powered by large language models (LLMs) have recently risen to prominence as effective tools for addressing complex real-world tasks. At their core, agentic workflows rely on LLMs to plan and execute the use of tools and external Application Programming Interfaces (APIs) in sequence to arrive at the answer to a user's request. Various benchmarks and leaderboards have emerged to evaluate an LLM's capabilities for tool and API use; however, most of these evaluations only track single or multiple isolated API calling capabilities. In this paper, we present NESTFUL, a benchmark to evaluate LLMs on nested sequences of API calls, i.e., sequences where the output of one API call is passed as input to a subsequent call. NESTFUL has a total of 300 human annotated samples divided into two types - executable and non-executable. The executable samples are curated manually by crawling Rapid-APIs whereas the non-executable samples are hand picked by human annotators from data synthetically generated using an LLM. We evaluate state-of-the-art LLMs with function calling abilities on NESTFUL. Our results show that most models do not perform well on nested APIs in NESTFUL as compared to their performance on the simpler problem settings available in existing benchmarks.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item2">
          [2]
         </a>
         <a href="/abs/2409.03937" id="2409.03937" title="Abstract">
          arXiv:2409.03937
         </a>
         [
         <a aria-labelledby="pdf-2409.03937" href="/pdf/2409.03937" id="pdf-2409.03937" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03937" href="https://arxiv.org/html/2409.03937v1" id="html-2409.03937" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03937" href="/format/2409.03937" id="oth-2409.03937" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Harnessing LLMs for Cross-City OD Flow Prediction
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+C">
            Chenyang Yu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+X">
            Xinpeng Xie
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+Y">
            Yan Huang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qiu,+C">
            Chenxi Qiu
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           12 pages, 18 figures
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Artificial Intelligence (cs.AI)
           </span>
          </div>
          <p class="mathjax">
           Understanding and predicting Origin-Destination (OD) flows is crucial for urban planning and transportation management. Traditional OD prediction models, while effective within single cities, often face limitations when applied across different cities due to varied traffic conditions, urban layouts, and socio-economic factors. In this paper, by employing Large Language Models (LLMs), we introduce a new method for cross-city OD flow prediction. Our approach leverages the advanced semantic understanding and contextual learning capabilities of LLMs to bridge the gap between cities with different characteristics, providing a robust and adaptable solution for accurate OD flow prediction that can be transferred from one city to another. Our novel framework involves four major components: collecting OD training datasets from a source city, instruction-tuning the LLMs, predicting destination POIs in a target city, and identifying the locations that best match the predicted destination POIs. We introduce a new loss function that integrates POI semantics and trip distance during training. By extracting high-quality semantic features from human mobility and POI data, the model understands spatial and functional relationships within urban spaces and captures interactions between individuals and various POIs. Extensive experimental results demonstrate the superiority of our approach over the state-of-the-art learning-based methods in cross-city OD flow prediction.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item3">
          [3]
         </a>
         <a href="/abs/2409.04056" id="2409.04056" title="Abstract">
          arXiv:2409.04056
         </a>
         [
         <a aria-labelledby="pdf-2409.04056" href="/pdf/2409.04056" id="pdf-2409.04056" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="oth-2409.04056" href="/format/2409.04056" id="oth-2409.04056" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Refining Wikidata Taxonomy using Large Language Models
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Peng,+Y">
            Yiwen Peng
           </a>
           (IP Paris),
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bonald,+T">
            Thomas Bonald
           </a>
           (IP Paris),
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Alam,+M">
            Mehwish Alam
           </a>
           (IP Paris)
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           ACM International Conference on Information and Knowledge Management, Oct 2024, Boise, Idaho, United States
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Artificial Intelligence (cs.AI)
           </span>
           ; Computation and Language (cs.CL); Information Retrieval (cs.IR)
          </div>
          <p class="mathjax">
           Due to its collaborative nature, Wikidata is known to have a complex taxonomy, with recurrent issues like the ambiguity between instances and classes, the inaccuracy of some taxonomic paths, the presence of cycles, and the high level of redundancy across classes. Manual efforts to clean up this taxonomy are time-consuming and prone to errors or subjective decisions. We present WiKC, a new version of Wikidata taxonomy cleaned automatically using a combination of Large Language Models (LLMs) and graph mining techniques. Operations on the taxonomy, such as cutting links or merging classes, are performed with the help of zero-shot prompting on an open-source LLM. The quality of the refined taxonomy is evaluated from both intrinsic and extrinsic perspectives, on a task of entity typing for the latter, showing the practical interest of WiKC.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item4">
          [4]
         </a>
         <a href="/abs/2409.04065" id="2409.04065" title="Abstract">
          arXiv:2409.04065
         </a>
         [
         <a aria-labelledby="pdf-2409.04065" href="/pdf/2409.04065" id="pdf-2409.04065" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04065" href="https://arxiv.org/html/2409.04065v1" id="html-2409.04065" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04065" href="/format/2409.04065" id="oth-2409.04065" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           An Argumentative Approach for Explaining Preemption in Soft-Constraint Based Norms
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fungwacharakorn,+W">
            Wachara Fungwacharakorn
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tsushima,+K">
            Kanae Tsushima
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hosobe,+H">
            Hiroshi Hosobe
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Takeda,+H">
            Hideaki Takeda
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Satoh,+K">
            Ken Satoh
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           submitted to VECOMP/AICOM 2024 associated with 27th European Conference on Artificial Intelligence (ECAI2024)
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Artificial Intelligence (cs.AI)
           </span>
          </div>
          <p class="mathjax">
           Although various aspects of soft-constraint based norms have been explored, it is still challenging to understand preemption. Preemption is a situation where higher-level norms override lower-level norms when new information emerges. To address this, we propose a derivation state argumentation framework (DSA-framework). DSA-framework incorporates derivation states to explain how preemption arises based on evolving situational knowledge. Based on DSA-framework, we present an argumentative approach for explaining preemption. We formally prove that, under local optimality, DSA-framework can provide explanations why one consequence is obligatory or forbidden by soft-constraint based norms represented as logical constraint hierarchies.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item5">
          [5]
         </a>
         <a href="/abs/2409.04102" id="2409.04102" title="Abstract">
          arXiv:2409.04102
         </a>
         [
         <a aria-labelledby="pdf-2409.04102" href="/pdf/2409.04102" id="pdf-2409.04102" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04102" href="https://arxiv.org/html/2409.04102v1" id="html-2409.04102" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04102" href="/format/2409.04102" id="oth-2409.04102" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Intelligent tutoring systems by Bayesian networks with noisy gates
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Antonucci,+A">
            Alessandro Antonucci
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mangili,+F">
            Francesca Mangili
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bonesana,+C">
            Claudio Bonesana
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Adorni,+G">
            Giorgia Adorni
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Artificial Intelligence (cs.AI)
           </span>
          </div>
          <p class="mathjax">
           Directed graphical models such as Bayesian nets are often used to implement intelligent tutoring systems able to interact in real-time with learners in a purely automatic way. When coping with such models, keeping a bound on the number of parameters might be important for multiple reasons. First, as these models are typically based on expert knowledge, a huge number of parameters to elicit might discourage practitioners from adopting them. Moreover, the number of model parameters affects the complexity of the inferences, while a fast computation of the queries is needed for real-time feedback. We advocate logical gates with uncertainty for a compact parametrization of the conditional probability tables in the underlying Bayesian net used by tutoring systems. We discuss the semantics of the model parameters to elicit and the assumptions required to apply such approach in this domain. We also derive a dedicated inference scheme to speed up computations.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item6">
          [6]
         </a>
         <a href="/abs/2409.04194" id="2409.04194" title="Abstract">
          arXiv:2409.04194
         </a>
         [
         <a aria-labelledby="pdf-2409.04194" href="/pdf/2409.04194" id="pdf-2409.04194" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04194" href="https://arxiv.org/html/2409.04194v1" id="html-2409.04194" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04194" href="/format/2409.04194" id="oth-2409.04194" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Towards Privacy-Preserving Relational Data Synthesis via Probabilistic Relational Models
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luttermann,+M">
            Malte Luttermann
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=M%C3%B6ller,+R">
            Ralf MÃ¶ller
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hartwig,+M">
            Mattis Hartwig
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Accepted to the Proceedings of the 47th German Conference on Artificial Intelligence (KI 2024)
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Artificial Intelligence (cs.AI)
           </span>
           ; Databases (cs.DB); Machine Learning (cs.LG)
          </div>
          <p class="mathjax">
           Probabilistic relational models provide a well-established formalism to combine first-order logic and probabilistic models, thereby allowing to represent relationships between objects in a relational domain. At the same time, the field of artificial intelligence requires increasingly large amounts of relational training data for various machine learning tasks. Collecting real-world data, however, is often challenging due to privacy concerns, data protection regulations, high costs, and so on. To mitigate these challenges, the generation of synthetic data is a promising approach. In this paper, we solve the problem of generating synthetic relational data via probabilistic relational models. In particular, we propose a fully-fledged pipeline to go from relational database to probabilistic relational model, which can then be used to sample new synthetic relational data points from its underlying probability distribution. As part of our proposed pipeline, we introduce a learning algorithm to construct a probabilistic relational model from a given relational database.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item7">
          [7]
         </a>
         <a href="/abs/2409.04224" id="2409.04224" title="Abstract">
          arXiv:2409.04224
         </a>
         [
         <a aria-labelledby="pdf-2409.04224" href="/pdf/2409.04224" id="pdf-2409.04224" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04224" href="https://arxiv.org/html/2409.04224v1" id="html-2409.04224" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04224" href="/format/2409.04224" id="oth-2409.04224" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tan,+D+J">
            Daniel J. Tan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Q">
            Qianyi Xu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=See,+K+C">
            Kay Choong See
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Perera,+D">
            Dilruk Perera
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Feng,+M">
            Mengling Feng
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Artificial Intelligence (cs.AI)
           </span>
           ; Machine Learning (cs.LG)
          </div>
          <p class="mathjax">
           Multi-organ diseases present significant challenges due to their simultaneous impact on multiple organ systems, necessitating complex and adaptive treatment strategies. Despite recent advancements in AI-powered healthcare decision support systems, existing solutions are limited to individual organ systems. They often ignore the intricate dependencies between organ system and thereby fails to provide holistic treatment recommendations that are useful in practice. We propose a novel hierarchical multi-agent reinforcement learning (HMARL) framework to address these challenges. This framework uses dedicated agents for each organ system, and model dynamic through explicit inter-agent communication channels, enabling coordinated treatment strategies across organs. Furthermore, we introduce a dual-layer state representation technique to contextualize patient conditions at various hierarchical levels, enhancing the treatment accuracy and relevance. Through extensive qualitative and quantitative evaluations in managing sepsis (a complex multi-organ disease), our approach demonstrates its ability to learn effective treatment policies that significantly improve patient survival rates. This framework marks a substantial advancement in clinical decision support systems, pioneering a comprehensive approach for multi-organ treatment recommendations.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item8">
          [8]
         </a>
         <a href="/abs/2409.04267" id="2409.04267" title="Abstract">
          arXiv:2409.04267
         </a>
         [
         <a aria-labelledby="pdf-2409.04267" href="/pdf/2409.04267" id="pdf-2409.04267" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04267" href="https://arxiv.org/html/2409.04267v1" id="html-2409.04267" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04267" href="/format/2409.04267" id="oth-2409.04267" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           An overview of domain-specific foundation model: key technologies, applications and challenges
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+H">
            Haolong Chen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+H">
            Hanzhi Chen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+Z">
            Zijian Zhao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Han,+K">
            Kaifeng Han
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+G">
            Guangxu Zhu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+Y">
            Yichen Zhao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Du,+Y">
            Ying Du
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+W">
            Wei Xu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shi,+Q">
            Qingjiang Shi
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Artificial Intelligence (cs.AI)
           </span>
           ; Computation and Language (cs.CL)
          </div>
          <p class="mathjax">
           The impressive performance of ChatGPT and other foundation-model-based products in human language understanding has prompted both academia and industry to explore how these models can be tailored for specific industries and application scenarios. This process, known as the customization of domain-specific foundation models, addresses the limitations of general-purpose models, which may not fully capture the unique patterns and requirements of domain-specific data. Despite its importance, there is a notable lack of comprehensive overview papers on building domain-specific foundation models, while numerous resources exist for general-purpose models. To bridge this gap, this article provides a timely and thorough overview of the methodology for customizing domain-specific foundation models. It introduces basic concepts, outlines the general architecture, and surveys key methods for constructing domain-specific models. Furthermore, the article discusses various domains that can benefit from these specialized models and highlights the challenges ahead. Through this overview, we aim to offer valuable guidance and reference for researchers and practitioners from diverse fields to develop their own customized foundation models.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item9">
          [9]
         </a>
         <a href="/abs/2409.04286" id="2409.04286" title="Abstract">
          arXiv:2409.04286
         </a>
         [
         <a aria-labelledby="pdf-2409.04286" href="/pdf/2409.04286" id="pdf-2409.04286" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04286" href="https://arxiv.org/html/2409.04286v1" id="html-2409.04286" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04286" href="/format/2409.04286" id="oth-2409.04286" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Heim,+D">
            Desiree Heim
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jilek,+C">
            Christian Jilek
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ulges,+A">
            Adrian Ulges
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dengel,+A">
            Andreas Dengel
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Accepted and in press (INFORMATIK Festival, Wiesbaden, 2024)
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Artificial Intelligence (cs.AI)
           </span>
           ; Computation and Language (cs.CL)
          </div>
          <p class="mathjax">
           Current publicly available knowledge work data collections lack diversity, extensive annotations, and contextual information about the users and their documents. These issues hinder objective and comparable data-driven evaluations and optimizations of knowledge work assistance systems. Due to the considerable resources needed to collect such data in real-life settings and the necessity of data censorship, collecting such a dataset appears nearly impossible. For this reason, we propose a configurable, multi-agent knowledge work dataset generator. This system simulates collaborative knowledge work among agents producing Large Language Model-generated documents and accompanying data traces. Additionally, the generator captures all background information, given in its configuration or created during the simulation process, in a knowledge graph. Finally, the resulting dataset can be utilized and shared without privacy or confidentiality concerns.
           <br/>
           This paper introduces our approach's design and vision and focuses on generating authentic knowledge work documents using Large Language Models. Our study involving human raters who assessed 53% of the generated and 74% of the real documents as realistic demonstrates the potential of our approach. Furthermore, we analyze the authenticity criteria mentioned in the participants' comments and elaborate on potential improvements for identified common issues.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item10">
          [10]
         </a>
         <a href="/abs/2409.04415" id="2409.04415" title="Abstract">
          arXiv:2409.04415
         </a>
         [
         <a aria-labelledby="pdf-2409.04415" href="/pdf/2409.04415" id="pdf-2409.04415" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04415" href="https://arxiv.org/html/2409.04415v1" id="html-2409.04415" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04415" href="/format/2409.04415" id="oth-2409.04415" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Improved Parallel Algorithm for Non-Monotone Submodular Maximization under Knapsack Constraint
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tran,+T+D">
            Tan D. Tran
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pham,+C+V">
            Canh V. Pham
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ha,+D+T+K">
            Dung T. K. Ha
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pham,+P+N">
            Phuong N.H. Pham
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI), Main Track
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Artificial Intelligence (cs.AI)
           </span>
          </div>
          <p class="mathjax">
           This work proposes an efficient parallel algorithm for non-monotone submodular maximization under a knapsack constraint problem over the ground set of size $n$. Our algorithm improves the best approximation factor of the existing parallel one from $8+\epsilon$ to $7+\epsilon$ with $O(\log n)$ adaptive complexity.
           <br/>
           The key idea of our approach is to create a new alternate threshold algorithmic framework. This strategy alternately constructs two disjoint candidate solutions within a constant number of sequence rounds. Then, the algorithm boosts solution quality without sacrificing the adaptive complexity. Extensive experimental studies on three applications, Revenue Maximization, Image Summarization, and Maximum Weighted Cut, show that our algorithm not only significantly increases solution quality but also requires comparative adaptivity to state-of-the-art algorithms.
          </p>
         </div>
        </dd>
       </dl>
       <dl id="articles">
        <h3>
         Cross submissions for Monday, 9 September 2024 (showing 55 of 55 entries )
        </h3>
        <dt>
         <a name="item11">
          [11]
         </a>
         <a href="/abs/2409.03759" id="2409.03759" title="Abstract">
          arXiv:2409.03759
         </a>
         (cross-list from cs.IR)

        [
         <a aria-labelledby="pdf-2409.03759" href="/pdf/2409.03759" id="pdf-2409.03759" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03759" href="https://arxiv.org/html/2409.03759v1" id="html-2409.03759" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03759" href="/format/2409.03759" id="oth-2409.03759" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           VERA: Validation and Evaluation of Retrieval-Augmented Systems
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ding,+T">
            Tianyu Ding
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Banerjee,+A">
            Adi Banerjee
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mombaerts,+L">
            Laurent Mombaerts
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y">
            Yunhong Li
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Borogovac,+T">
            Tarik Borogovac
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=De+la+Cruz+Weinstein,+J+P">
            Juan Pablo De la Cruz Weinstein
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Accepted in Workshop on Evaluation and Trustworthiness of Generative AI Models, KDD 2024
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Information Retrieval (cs.IR)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           The increasing use of Retrieval-Augmented Generation (RAG) systems in various applications necessitates stringent protocols to ensure RAG systems accuracy, safety, and alignment with user intentions. In this paper, we introduce VERA (Validation and Evaluation of Retrieval-Augmented Systems), a framework designed to enhance the transparency and reliability of outputs from large language models (LLMs) that utilize retrieved information. VERA improves the way we evaluate RAG systems in two important ways: (1) it introduces a cross-encoder based mechanism that encompasses a set of multidimensional metrics into a single comprehensive ranking score, addressing the challenge of prioritizing individual metrics, and (2) it employs Bootstrap statistics on LLM-based metrics across the document repository to establish confidence bounds, ensuring the repositorys topical coverage and improving the overall reliability of retrieval systems. Through several use cases, we demonstrate how VERA can strengthen decision-making processes and trust in AI applications. Our findings not only contribute to the theoretical understanding of LLM-based RAG evaluation metric but also promote the practical implementation of responsible AI systems, marking a significant advancement in the development of reliable and transparent generative AI technologies.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item12">
          [12]
         </a>
         <a href="/abs/2409.03788" id="2409.03788" title="Abstract">
          arXiv:2409.03788
         </a>
         (cross-list from cs.CR)

        [
         <a aria-labelledby="pdf-2409.03788" href="/pdf/2409.03788" id="pdf-2409.03788" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03788" href="https://arxiv.org/html/2409.03788v1" id="html-2409.03788" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03788" href="/format/2409.03788" id="oth-2409.03788" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           HSF: Defending against Jailbreak Attacks with Hidden State Filtering
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qian,+C">
            Cheng Qian
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+H">
            Hainan Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sha,+L">
            Lei Sha
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+Z">
            Zhiming Zheng
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           13 pages
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Cryptography and Security (cs.CR)
           </span>
           ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)
          </div>
          <p class="mathjax">
           With the growing deployment of LLMs in daily applications like chatbots and content generation, efforts to ensure outputs align with human values and avoid harmful content have intensified. However, increasingly sophisticated jailbreak attacks threaten this alignment, aiming to induce unsafe outputs. Current defense efforts either focus on prompt rewriting or detection, which are limited in effectiveness due to the various design of jailbreak prompts, or on output control and detection, which are computationally expensive as they require LLM inference. Therefore, designing a pre-inference defense method that resists diverse jailbreak prompts is crucial for preventing LLM jailbreak attacks. We observe that jailbreak attacks, safe queries, and harmful queries exhibit different clustering patterns within the LLM's hidden state representation space. This suggests that by leveraging the LLM's hidden state representational capabilities, we can analyze the LLM's forthcoming behavior and proactively intervene for defense. In this paper, we propose a jailbreak attack defense strategy based on a Hidden State Filter (HSF), a lossless architectural defense mechanism that enables the model to preemptively identify and reject adversarial inputs before the inference process begins. We activate its defensive potential through an additional plugin module, effectively framing the defense task as a classification problem. Experimental results on two benchmark datasets, utilizing three different LLMs, show that HSF significantly enhances resilience against six cutting-edge jailbreak attacks. It significantly reduces the success rate of jailbreak attacks while minimally impacting responses to benign user queries, with negligible inference overhead, and outperforming defense baselines.Our code and data are available at https://anonymous.4open.science/r/Hidden-State-Filtering-8652/
          </p>
         </div>
        </dd>
        <dt>
         <a name="item13">
          [13]
         </a>
         <a href="/abs/2409.03789" id="2409.03789" title="Abstract">
          arXiv:2409.03789
         </a>
         (cross-list from cs.CR)

        [
         <a aria-labelledby="pdf-2409.03789" href="/pdf/2409.03789" id="pdf-2409.03789" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03789" href="https://arxiv.org/html/2409.03789v1" id="html-2409.03789" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03789" href="/format/2409.03789" id="oth-2409.03789" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           BreachSeek: A Multi-Agent Automated Penetration Tester
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Alshehri,+I">
            Ibrahim Alshehri
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Alshehri,+A">
            Adnan Alshehri
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Almalki,+A">
            Abdulrahman Almalki
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bamardouf,+M">
            Majed Bamardouf
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Akbar,+A">
            Alaqsa Akbar
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           7 pages, 6 figures
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Cryptography and Security (cs.CR)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           The increasing complexity and scale of modern digital environments have exposed significant gaps in traditional cybersecurity penetration testing methods, which are often time-consuming, labor-intensive, and unable to rapidly adapt to emerging threats. There is a critical need for an automated solution that can efficiently identify and exploit vulnerabilities across diverse systems without extensive human intervention. BreachSeek addresses this challenge by providing an AI-driven multi-agent software platform that leverages Large Language Models (LLMs) integrated through LangChain and LangGraph in Python. This system enables autonomous agents to conduct thorough penetration testing by identifying vulnerabilities, simulating a variety of cyberattacks, executing exploits, and generating comprehensive security reports. In preliminary evaluations, BreachSeek successfully exploited vulnerabilities in exploitable machines within local networks, demonstrating its practical effectiveness. Future developments aim to expand its capabilities, positioning it as an indispensable tool for cybersecurity professionals.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item14">
          [14]
         </a>
         <a href="/abs/2409.03793" id="2409.03793" title="Abstract">
          arXiv:2409.03793
         </a>
         (cross-list from cs.CR)

        [
         <a aria-labelledby="pdf-2409.03793" href="/pdf/2409.03793" id="pdf-2409.03793" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="oth-2409.03793" href="/format/2409.03793" id="oth-2409.03793" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Safeguarding AI Agents: Developing and Analyzing Safety Architectures
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Domkundwar,+I">
            Ishaan Domkundwar
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=S,+M+N">
            Mukunda N S
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Cryptography and Security (cs.CR)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           AI agents, specifically powered by large language models, have demonstrated exceptional capabilities in various applications where precision and efficacy are necessary. However, these agents come with inherent risks, including the potential for unsafe or biased actions, vulnerability to adversarial attacks, lack of transparency, and tendency to generate hallucinations. As AI agents become more prevalent in critical sectors of the industry, the implementation of effective safety protocols becomes increasingly important. This paper addresses the critical need for safety measures in AI systems, especially ones that collaborate with human teams. We propose and evaluate three frameworks to enhance safety protocols in AI agent systems: an LLM-powered input-output filter, a safety agent integrated within the system, and a hierarchical delegation-based system with embedded safety checks. Our methodology involves implementing these frameworks and testing them against a set of unsafe agentic use cases, providing a comprehensive evaluation of their effectiveness in mitigating risks associated with AI agent deployment. We conclude that these frameworks can significantly strengthen the safety and security of AI agent systems, minimizing potential harmful actions or outputs. Our work contributes to the ongoing effort to create safe and reliable AI applications, particularly in automated operations, and provides a foundation for developing robust guardrails to ensure the responsible use of AI agents in real-world applications.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item15">
          [15]
         </a>
         <a href="/abs/2409.03795" id="2409.03795" title="Abstract">
          arXiv:2409.03795
         </a>
         (cross-list from cs.CR)

        [
         <a aria-labelledby="pdf-2409.03795" href="/pdf/2409.03795" id="pdf-2409.03795" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03795" href="https://arxiv.org/html/2409.03795v1" id="html-2409.03795" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03795" href="/format/2409.03795" id="oth-2409.03795" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Security Implications and Mitigation Strategies in MPLS Networks
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Thakur,+A">
            Ayush Thakur
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Cryptography and Security (cs.CR)
           </span>
           ; Artificial Intelligence (cs.AI); Networking and Internet Architecture (cs.NI)
          </div>
          <p class="mathjax">
           Multiprotocol Label Switching (MPLS) is a high-performance telecommunications technology that directs data from one network node to another based on short path labels rather than long network addresses. Its efficiency and scalability have made it a popular choice for large-scale and enterprise networks. However, as MPLS networks grow and evolve, they encounter various security challenges. This paper explores the security implications associated with MPLS networks, including risks such as label spoofing, traffic interception, and denial of service attacks. Additionally, it evaluates advanced mitigation strategies to address these vulnerabilities, leveraging mathematical models and security protocols to enhance MPLS network resilience. By integrating theoretical analysis with practical solutions, this paper aims to provide a comprehensive understanding of MPLS security and propose effective methods for safeguarding network infrastructure.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item16">
          [16]
         </a>
         <a href="/abs/2409.03796" id="2409.03796" title="Abstract">
          arXiv:2409.03796
         </a>
         (cross-list from cs.CR)

        [
         <a aria-labelledby="pdf-2409.03796" href="/pdf/2409.03796" id="pdf-2409.03796" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03796" href="https://arxiv.org/html/2409.03796v1" id="html-2409.03796" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03796" href="/format/2409.03796" id="oth-2409.03796" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Protecting Activity Sensing Data Privacy Using Hierarchical Information Dissociation
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+G">
            Guangjing Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+H">
            Hanqing Guo
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y">
            Yuanda Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+B">
            Bocheng Chen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+C">
            Ce Zhou
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yan,+Q">
            Qiben Yan
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Cryptography and Security (cs.CR)
           </span>
           ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
          </div>
          <p class="mathjax">
           Smartphones and wearable devices have been integrated into our daily lives, offering personalized services. However, many apps become overprivileged as their collected sensing data contains unnecessary sensitive information. For example, mobile sensing data could reveal private attributes (e.g., gender and age) and unintended sensitive features (e.g., hand gestures when entering passwords). To prevent sensitive information leakage, existing methods must obtain private labels and users need to specify privacy policies. However, they only achieve limited control over information disclosure. In this work, we present Hippo to dissociate hierarchical information including private metadata and multi-grained activity information from the sensing data. Hippo achieves fine-grained control over the disclosure of sensitive information without requiring private labels. Specifically, we design a latent guidance-based diffusion model, which generates multi-grained versions of raw sensor data conditioned on hierarchical latent activity features. Hippo enables users to control the disclosure of sensitive information in sensing data, ensuring their privacy while preserving the necessary features to meet the utility requirements of applications. Hippo is the first unified model that achieves two goals: perturbing the sensitive attributes and controlling the disclosure of sensitive information in mobile sensing data. Extensive experiments show that Hippo can anonymize personal attributes and transform activity information at various resolutions across different types of sensing data.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item17">
          [17]
         </a>
         <a href="/abs/2409.03805" id="2409.03805" title="Abstract">
          arXiv:2409.03805
         </a>
         (cross-list from stat.ME)

        [
         <a aria-labelledby="pdf-2409.03805" href="/pdf/2409.03805" id="pdf-2409.03805" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03805" href="https://arxiv.org/html/2409.03805v1" id="html-2409.03805" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03805" href="/format/2409.03805" id="oth-2409.03805" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Exploratory Visual Analysis for Increasing Data Readiness in Artificial Intelligence Projects
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Tiger,+M">
            Mattias Tiger
           </a>
           ,
           <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Jakobsson,+D">
            Daniel Jakobsson
           </a>
           ,
           <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Ynnerman,+A">
            Anders Ynnerman
           </a>
           ,
           <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Heintz,+F">
            Fredrik Heintz
           </a>
           ,
           <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=J%C3%B6nsson,+D">
            Daniel JÃ¶nsson
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Methodology (stat.ME)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           We present experiences and lessons learned from increasing data readiness of heterogeneous data for artificial intelligence projects using visual analysis methods. Increasing the data readiness level involves understanding both the data as well as the context in which it is used, which are challenges well suitable to visual analysis. For this purpose, we contribute a mapping between data readiness aspects and visual analysis techniques suitable for different data types. We use the defined mapping to increase data readiness levels in use cases involving time-varying data, including numerical, categorical, and text. In addition to the mapping, we extend the data readiness concept to better take aspects of the task and solution into account and explicitly address distribution shifts during data collection time. We report on our experiences in using the presented visual analysis techniques to aid future artificial intelligence projects in raising the data readiness level.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item18">
          [18]
         </a>
         <a href="/abs/2409.03806" id="2409.03806" title="Abstract">
          arXiv:2409.03806
         </a>
         (cross-list from eess.IV)

        [
         <a aria-labelledby="pdf-2409.03806" href="/pdf/2409.03806" id="pdf-2409.03806" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="oth-2409.03806" href="/format/2409.03806" id="oth-2409.03806" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Mpox Screen Lite: AI-Driven On-Device Offline Mpox Screening for Low-Resource African Mpox Emergency Response
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Kularathne,+Y">
            Yudara Kularathne
           </a>
           ,
           <a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Janitha,+P">
            Prathapa Janitha
           </a>
           ,
           <a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Ambepitiya,+S">
            Sithira Ambepitiya
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           11 Pages, 2 Figures, 3 Tables
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Image and Video Processing (eess.IV)
           </span>
           ; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)
          </div>
          <p class="mathjax">
           Background: The 2024 Mpox outbreak, particularly severe in Africa with clade 1b emergence, has highlighted critical gaps in diagnostic capabilities in resource-limited settings. This study aimed to develop and validate an artificial intelligence (AI)-driven, on-device screening tool for Mpox, designed to function offline in low-resource environments.
           <br/>
           Methods: We developed a YOLOv8n-based deep learning model trained on 2,700 images (900 each of Mpox, other skin conditions, and normal skin), including synthetic data. The model was validated on 360 images and tested on 540 images. A larger external validation was conducted using 1,500 independent images. Performance metrics included accuracy, precision, recall, F1-score, sensitivity, and specificity.
           <br/>
           Findings: The model demonstrated high accuracy (96%) in the final test set. For Mpox detection, it achieved 93% precision, 97% recall, and an F1-score of 95%. Sensitivity and specificity for Mpox detection were 97% and 96%, respectively. Performance remained consistent in the larger external validation, confirming the model's robustness and generalizability.
           <br/>
           Interpretation: This AI-driven screening tool offers a rapid, accurate, and scalable solution for Mpox detection in resource-constrained settings. Its offline functionality and high performance across diverse datasets suggest significant potential for improving Mpox surveillance and management, particularly in areas lacking traditional diagnostic infrastructure.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item19">
          [19]
         </a>
         <a href="/abs/2409.03810" id="2409.03810" title="Abstract">
          arXiv:2409.03810
         </a>
         (cross-list from cs.SE)

        [
         <a aria-labelledby="pdf-2409.03810" href="/pdf/2409.03810" id="pdf-2409.03810" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03810" href="https://arxiv.org/html/2409.03810v1" id="html-2409.03810" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03810" href="/format/2409.03810" id="oth-2409.03810" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y">
            Yejie Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+K">
            Keqing He
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fu,+D">
            Dayuan Fu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gongque,+Z">
            Zhuoma Gongque
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+H">
            Heyang Xu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Y">
            Yanxu Chen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">
            Zhexu Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fu,+Y">
            Yujia Fu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dong,+G">
            Guanting Dong
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Diao,+M">
            Muxi Diao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+J">
            Jingang Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+M">
            Mengdi Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cai,+X">
            Xunliang Cai
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+W">
            Weiran Xu
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Working in progress
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Software Engineering (cs.SE)
           </span>
           ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)
          </div>
          <p class="mathjax">
           Recently, there has been a growing interest in studying how to construct better code instruction tuning data. However, we observe Code models trained with these datasets exhibit high performance on HumanEval but perform worse on other benchmarks such as LiveCodeBench. Upon further investigation, we find that many datasets suffer from severe data leakage. After cleaning up most of the leaked data, some well-known high-quality datasets perform poorly. This discovery reveals a new challenge: identifying which dataset genuinely qualify as high-quality code instruction data. To address this, we propose an efficient code data pruning strategy for selecting good samples. Our approach is based on three dimensions: instruction complexity, response quality, and instruction diversity. Based on our selected data, we present XCoder, a family of models finetuned from LLaMA3. Our experiments show XCoder achieves new state-of-the-art performance using fewer training data, which verify the effectiveness of our data strategy. Moreover, we perform a comprehensive analysis on the data composition and find existing code datasets have different characteristics according to their construction methods, which provide new insights for future code LLMs. Our models and dataset are released in
           <a class="link-external link-https" href="https://github.com/banksy23/XCoder" rel="external noopener nofollow">
            this https URL
           </a>
          </p>
         </div>
        </dd>
        <dt>
         <a name="item20">
          [20]
         </a>
         <a href="/abs/2409.03811" id="2409.03811" title="Abstract">
          arXiv:2409.03811
         </a>
         (cross-list from cs.MA)

        [
         <a aria-labelledby="pdf-2409.03811" href="/pdf/2409.03811" id="pdf-2409.03811" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03811" href="https://arxiv.org/html/2409.03811v1" id="html-2409.03811" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03811" href="/format/2409.03811" id="oth-2409.03811" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           PARCO: Learning Parallel Autoregressive Policies for Efficient Multi-Agent Combinatorial Optimization
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Berto,+F">
            Federico Berto
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hua,+C">
            Chuanbo Hua
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luttmann,+L">
            Laurin Luttmann
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Son,+J">
            Jiwoo Son
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Park,+J">
            Junyoung Park
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ahn,+K">
            Kyuree Ahn
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kwon,+C">
            Changhyun Kwon
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+L">
            Lin Xie
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Park,+J">
            Jinkyoo Park
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Multiagent Systems (cs.MA)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Multi-agent combinatorial optimization problems such as routing and scheduling have great practical relevance but present challenges due to their NP-hard combinatorial nature, hard constraints on the number of possible agents, and hard-to-optimize objective functions. This paper introduces PARCO (Parallel AutoRegressive Combinatorial Optimization), a novel approach that learns fast surrogate solvers for multi-agent combinatorial problems with reinforcement learning by employing parallel autoregressive decoding. We propose a model with a Multiple Pointer Mechanism to efficiently decode multiple decisions simultaneously by different agents, enhanced by a Priority-based Conflict Handling scheme. Moreover, we design specialized Communication Layers that enable effective agent collaboration, thus enriching decision-making. We evaluate PARCO in representative multi-agent combinatorial problems in routing and scheduling and demonstrate that our learned solvers offer competitive results against both classical and neural baselines in terms of both solution quality and speed. We make our code openly available at
           <a class="link-external link-https" href="https://github.com/ai4co/parco" rel="external noopener nofollow">
            this https URL
           </a>
           .
          </p>
         </div>
        </dd>
        <dt>
         <a name="item21">
          [21]
         </a>
         <a href="/abs/2409.03833" id="2409.03833" title="Abstract">
          arXiv:2409.03833
         </a>
         (cross-list from gr-qc)

        [
         <a aria-labelledby="pdf-2409.03833" href="/pdf/2409.03833" id="pdf-2409.03833" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03833" href="https://arxiv.org/html/2409.03833v1" id="html-2409.03833" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03833" href="/format/2409.03833" id="oth-2409.03833" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           AI forecasting of higher-order wave modes of spinning binary black hole mergers
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/gr-qc?searchtype=author&amp;query=Tiki,+V">
            Victoria Tiki
           </a>
           ,
           <a href="https://arxiv.org/search/gr-qc?searchtype=author&amp;query=Pham,+K">
            Kiet Pham
           </a>
           ,
           <a href="https://arxiv.org/search/gr-qc?searchtype=author&amp;query=Huerta,+E">
            Eliu Huerta
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           27 pages, 1 appendix, 10 figures
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            General Relativity and Quantum Cosmology (gr-qc)
           </span>
           ; Instrumentation and Methods for Astrophysics (astro-ph.IM); Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           We present a physics-inspired transformer model that predicts the non-linear dynamics of higher-order wave modes emitted by quasi-circular, spinning, non-precessing binary black hole mergers. The model forecasts the waveform evolution from the pre-merger phase through the ringdown, starting with an input time-series spanning $ t \in [-5000\textrm{M}, -100\textrm{M}) $. The merger event, defined as the peak amplitude of waveforms that include the $l = |m| = 2$ modes, occurs at $ t = 0\textrm{M} $. The transformer then generates predictions over the time range $ t \in [-100\textrm{M}, 130\textrm{M}] $. We produced training, evaluation and test sets using the NRHybSur3dq8 model, considering a signal manifold defined by mass ratios $ q \in [1, 8] $; spin components $ s^z_{\{1,2\}} \in [-0.8, 0.8] $; modes up to $l \leq 4$, including the $(5,5)$ mode but excluding the $(4,0)$ and $(4,1)$ modes; and inclination angles $\theta \in [0, \pi]$. We trained the model on 14,440,761 waveforms, completing the training in 15 hours using 16 NVIDIA A100 GPUs in the Delta supercomputer. We used 4 H100 GPUs in the DeltaAI supercomputer to compute, within 7 hours, the overlap between ground truth and predicted waveforms using a test set of 840,000 waveforms, finding that the mean and median overlaps over the test set are 0.996 and 0.997, respectively. Additionally, we conducted interpretability studies to elucidate the waveform features utilized by our transformer model to produce accurate predictions. The scientific software used for this work is released with this manuscript.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item22">
          [22]
         </a>
         <a href="/abs/2409.03844" id="2409.03844" title="Abstract">
          arXiv:2409.03844
         </a>
         (cross-list from cs.SD)

        [
         <a aria-labelledby="pdf-2409.03844" href="/pdf/2409.03844" id="pdf-2409.03844" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03844" href="https://arxiv.org/html/2409.03844v1" id="html-2409.03844" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03844" href="/format/2409.03844" id="oth-2409.03844" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           MetaBGM: Dynamic Soundtrack Transformation For Continuous Multi-Scene Experiences With Ambient Awareness And Personalization
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+H">
            Haoxuan Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">
            Zihao Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hong,+H">
            Haorong Hong
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Feng,+Y">
            Youwei Feng
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+J">
            Jiaxin Yu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Diao,+H">
            Han Diao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Y">
            Yunfei Xu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+K">
            Kejun Zhang
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Sound (cs.SD)
           </span>
           ; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)
          </div>
          <p class="mathjax">
           This paper introduces MetaBGM, a groundbreaking framework for generating background music that adapts to dynamic scenes and real-time user interactions. We define multi-scene as variations in environmental contexts, such as transitions in game settings or movie scenes. To tackle the challenge of converting backend data into music description texts for audio generation models, MetaBGM employs a novel two-stage generation approach that transforms continuous scene and user state data into these texts, which are then fed into an audio generation model for real-time soundtrack creation. Experimental results demonstrate that MetaBGM effectively generates contextually relevant and dynamic background music for interactive applications.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item23">
          [23]
         </a>
         <a href="/abs/2409.03874" id="2409.03874" title="Abstract">
          arXiv:2409.03874
         </a>
         (cross-list from cs.GT)

        [
         <a aria-labelledby="pdf-2409.03874" href="/pdf/2409.03874" id="pdf-2409.03874" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="oth-2409.03874" href="/format/2409.03874" id="oth-2409.03874" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Cost-Control in Display Advertising: Theory vs Practice
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Katti,+A+R">
            Anoop R Katti
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gon%C3%A7alves,+R+C">
            Rui C. GonÃ§alves
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Iakovlev,+R">
            Rinchin Iakovlev
           </a>
          </div>
          <div class="list-journal-ref">
           <span class="descriptor">
            Journal-ref:
           </span>
           Presented at AdKDD 2024
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Science and Game Theory (cs.GT)
           </span>
           ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
          </div>
          <p class="mathjax">
           In display advertising, advertisers want to achieve a marketing objective with constraints on budget and cost-per-outcome. This is usually formulated as an optimization problem that maximizes the total utility under constraints. The optimization is carried out in an online fashion in the dual space - for an incoming Ad auction, a bid is placed using an optimal bidding formula, assuming optimal values for the dual variables; based on the outcome of the previous auctions, the dual variables are updated in an online fashion. While this approach is theoretically sound, in practice, the dual variables are not optimal from the beginning, but rather converge over time. Specifically, for the cost-constraint, the convergence is asymptotic. As a result, we find that cost-control is ineffective. In this work, we analyse the shortcomings of the optimal bidding formula and propose a modification that deviates from the theoretical derivation. We simulate various practical scenarios and study the cost-control behaviors of the two algorithms. Through a large-scale evaluation on the real-word data, we show that the proposed modification reduces the cost violations by 50%, thereby achieving a better cost-control than the theoretical bidding formula.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item24">
          [24]
         </a>
         <a href="/abs/2409.03881" id="2409.03881" title="Abstract">
          arXiv:2409.03881
         </a>
         (cross-list from cs.RO)

        [
         <a aria-labelledby="pdf-2409.03881" href="/pdf/2409.03881" id="pdf-2409.03881" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03881" href="https://arxiv.org/html/2409.03881v1" id="html-2409.03881" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03881" href="/format/2409.03881" id="oth-2409.03881" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Multi-agent Path Finding for Mixed Autonomy Traffic Coordination
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+H">
            Han Zheng
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yan,+Z">
            Zhongxia Yan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+C">
            Cathy Wu
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Robotics (cs.RO)
           </span>
           ; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)
          </div>
          <p class="mathjax">
           In the evolving landscape of urban mobility, the prospective integration of Connected and Automated Vehicles (CAVs) with Human-Driven Vehicles (HDVs) presents a complex array of challenges and opportunities for autonomous driving systems. While recent advancements in robotics have yielded Multi-Agent Path Finding (MAPF) algorithms tailored for agent coordination task characterized by simplified kinematics and complete control over agent behaviors, these solutions are inapplicable in mixed-traffic environments where uncontrollable HDVs must coexist and interact with CAVs. Addressing this gap, we propose the Behavior Prediction Kinematic Priority Based Search (BK-PBS), which leverages an offline-trained conditional prediction model to forecast HDV responses to CAV maneuvers, integrating these insights into a Priority Based Search (PBS) where the A* search proceeds over motion primitives to accommodate kinematic constraints. We compare BK-PBS with CAV planning algorithms derived by rule-based car-following models, and reinforcement learning. Through comprehensive simulation on a highway merging scenario across diverse scenarios of CAV penetration rate and traffic density, BK-PBS outperforms these baselines in reducing collision rates and enhancing system-level travel delay. Our work is directly applicable to many scenarios of multi-human multi-robot coordination.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item25">
          [25]
         </a>
         <a href="/abs/2409.03911" id="2409.03911" title="Abstract">
          arXiv:2409.03911
         </a>
         (cross-list from cs.CV)

        [
         <a aria-labelledby="pdf-2409.03911" href="/pdf/2409.03911" id="pdf-2409.03911" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03911" href="https://arxiv.org/html/2409.03911v1" id="html-2409.03911" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03911" href="/format/2409.03911" id="oth-2409.03911" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           The Role of Generative Systems in Historical Photography Management: A Case Study on Catalan Archives
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=%C5%9Aanchez,+%C3%88">
            Ãric Åanchez
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Molina,+A">
            AdriÃ  Molina
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Terrades,+O+R">
            Oriol Ramos Terrades
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Accepted at ECCV workshop AI4DH
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           The use of image analysis in automated photography management is an increasing trend in heritage institutions. Such tools alleviate the human cost associated with the manual and expensive annotation of new data sources while facilitating fast access to the citizenship through online indexes and search engines. However, available tagging and description tools are usually designed around modern photographs in English, neglecting historical corpora in minoritized languages, each of which exhibits intrinsic particularities. The primary objective of this research is to study the quantitative contribution of generative systems in the description of historical sources. This is done by contextualizing the task of captioning historical photographs from the Catalan archives as a case study. Our findings provide practitioners with tools and directions on transfer learning for captioning models based on visual adaptation and linguistic proximity.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item26">
          [26]
         </a>
         <a href="/abs/2409.03933" id="2409.03933" title="Abstract">
          arXiv:2409.03933
         </a>
         (cross-list from physics.flu-dyn)

        [
         <a aria-labelledby="pdf-2409.03933" href="/pdf/2409.03933" id="pdf-2409.03933" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03933" href="https://arxiv.org/html/2409.03933v1" id="html-2409.03933" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03933" href="/format/2409.03933" id="oth-2409.03933" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Lagemann,+E">
            Esther Lagemann
           </a>
           ,
           <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Roeb,+J">
            Julia Roeb
           </a>
           ,
           <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Brunton,+S+L">
            Steven L. Brunton
           </a>
           ,
           <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Lagemann,+C">
            Christian Lagemann
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Fluid Dynamics (physics.flu-dyn)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           The accurate quantification of wall-shear stress dynamics is of substantial importance for various applications in fundamental and applied research, spanning areas from human health to aircraft design and optimization. Despite significant progress in experimental measurement techniques and post-processing algorithms, temporally resolved wall-shear stress dynamics with adequate spatial resolution and within a suitable spatial domain remain an elusive goal. To address this gap, we introduce a deep learning architecture that ingests wall-parallel velocity fields from the logarithmic layer of turbulent wall-bounded flows and outputs the corresponding 2D wall-shear stress fields with identical spatial resolution and domain size. From a physical perspective, our framework acts as a surrogate model encapsulating the various mechanisms through which highly energetic outer-layer flow structures influence the governing wall-shear stress dynamics. The network is trained in a supervised fashion on a unified dataset comprising direct numerical simulations of statistically 1D turbulent channel and spatially developing turbulent boundary layer flows at friction Reynolds numbers ranging from 390 to 1,500. We demonstrate a zero-shot applicability to experimental velocity fields obtained from Particle-Image Velocimetry measurements and verify the physical accuracy of the wall-shear stress estimates with synchronized wall-shear stress measurements using the Micro-Pillar Shear-Stress Sensor for Reynolds numbers up to 2,000. In summary, the presented framework lays the groundwork for extracting inaccessible experimental wall-shear stress information from readily available velocity measurements and thus, facilitates advancements in a variety of experimental applications.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item27">
          [27]
         </a>
         <a href="/abs/2409.03944" id="2409.03944" title="Abstract">
          arXiv:2409.03944
         </a>
         (cross-list from cs.CV)

        [
         <a aria-labelledby="pdf-2409.03944" href="/pdf/2409.03944" id="pdf-2409.03944" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03944" href="https://arxiv.org/html/2409.03944v1" id="html-2409.03944" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03944" href="/format/2409.03944" id="oth-2409.03944" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           HUMOS: Human Motion Model Conditioned on Body Shape
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tripathi,+S">
            Shashank Tripathi
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Taheri,+O">
            Omid Taheri
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lassner,+C">
            Christoph Lassner
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Black,+M+J">
            Michael J. Black
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Holden,+D">
            Daniel Holden
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Stoll,+C">
            Carsten Stoll
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Accepted in ECCV'24. Project page:
           <a class="link-external link-https" href="https://CarstenEpic.github.io/humos/" rel="external noopener nofollow">
            this https URL
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Generating realistic human motion is essential for many computer vision and graphics applications. The wide variety of human body shapes and sizes greatly impacts how people move. However, most existing motion models ignore these differences, relying on a standardized, average body. This leads to uniform motion across different body types, where movements don't match their physical characteristics, limiting diversity. To solve this, we introduce a new approach to develop a generative motion model based on body shape. We show that it's possible to train this model using unpaired data by applying cycle consistency, intuitive physics, and stability constraints, which capture the relationship between identity and movement. The resulting model generates diverse, physically plausible, and dynamically stable human motions that are both quantitatively and qualitatively more realistic than current state-of-the-art methods. More details are available on our project page
           <a class="link-external link-https" href="https://CarstenEpic.github.io/humos/" rel="external noopener nofollow">
            this https URL
           </a>
           .
          </p>
         </div>
        </dd>
        <dt>
         <a name="item28">
          [28]
         </a>
         <a href="/abs/2409.03947" id="2409.03947" title="Abstract">
          arXiv:2409.03947
         </a>
         (cross-list from cs.CV)

        [
         <a aria-labelledby="pdf-2409.03947" href="/pdf/2409.03947" id="pdf-2409.03947" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03947" href="https://arxiv.org/html/2409.03947v1" id="html-2409.03947" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03947" href="/format/2409.03947" id="oth-2409.03947" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shu,+K">
            Kai Shu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jia,+Y">
            Yuzhuo Jia
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z">
            Ziyang Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+J">
            Jiechao Gao
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Automatic Medical Imaging Narrative generation aims to alleviate the workload of radiologists by producing accurate clinical descriptions directly from radiological images. However, the subtle visual nuances and domain-specific terminology in medical images pose significant challenges compared to generic image captioning tasks. Existing approaches often neglect the vital distinction between normal and abnormal findings, leading to suboptimal performance. In this work, we propose FODA-PG, a novel Fine-grained Organ-Disease Adaptive Partitioning Graph framework that addresses these limitations through domain-adaptive learning. FODA-PG constructs a granular graphical representation of radiological findings by separating disease-related attributes into distinct "disease-specific" and "disease-free" categories based on their clinical significance and location. This adaptive partitioning enables our model to capture the nuanced differences between normal and pathological states, mitigating the impact of data biases. By integrating this fine-grained semantic knowledge into a powerful transformer-based architecture and providing rigorous mathematical justifications for its effectiveness, FODA-PG generates precise and clinically coherent reports with enhanced generalization capabilities. Extensive experiments on the IU-Xray and MIMIC-CXR benchmarks demonstrate the superiority of our approach over state-of-the-art methods, highlighting the importance of domain adaptation in medical report generation.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item29">
          [29]
         </a>
         <a href="/abs/2409.03992" id="2409.03992" title="Abstract">
          arXiv:2409.03992
         </a>
         (cross-list from cs.DC)

        [
         <a aria-labelledby="pdf-2409.03992" href="/pdf/2409.03992" id="pdf-2409.03992" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03992" href="https://arxiv.org/html/2409.03992v1" id="html-2409.03992" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03992" href="/format/2409.03992" id="oth-2409.03992" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Confidential Computing on nVIDIA H100 GPU: A Performance Benchmark Study
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+J">
            Jianwei Zhu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yin,+H">
            Hang Yin
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+S">
            Shunfan Zhou
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Distributed, Parallel, and Cluster Computing (cs.DC)
           </span>
           ; Artificial Intelligence (cs.AI); Performance (cs.PF)
          </div>
          <p class="mathjax">
           This report evaluates the performance impact of enabling Trusted Execution Environments (TEE) on NVIDIA H100 GPUs for large language model (LLM) inference tasks. We benchmark the overhead introduced by TEE mode across various models and token lengths, focusing on the bottleneck caused by CPU-GPU data transfers via PCIe. Our results show that while there is minimal computational overhead within the GPU, the overall performance penalty is primarily due to data transfer. For most typical LLM queries, the overhead remains below 5%, with larger models and longer sequences experiencing near-zero overhead.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item30">
          [30]
         </a>
         <a href="/abs/2409.04007" id="2409.04007" title="Abstract">
          arXiv:2409.04007
         </a>
         (cross-list from cs.SD)

        [
         <a aria-labelledby="pdf-2409.04007" href="/pdf/2409.04007" id="pdf-2409.04007" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04007" href="https://arxiv.org/html/2409.04007v1" id="html-2409.04007" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04007" href="/format/2409.04007" id="oth-2409.04007" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kim,+B">
            Byunggun Kim
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kwon,+Y">
            Younghun Kwon
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Sound (cs.SD)
           </span>
           ; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)
          </div>
          <p class="mathjax">
           Speech emotion recognition (SER) classifies human emotions in speech with a computer model. Recently, performance in SER has steadily increased as deep learning techniques have adapted. However, unlike many domains that use speech data, data for training in the SER model is insufficient. This causes overfitting of training of the neural network, resulting in performance degradation. In fact, successful emotion recognition requires an effective preprocessing method and a model structure that efficiently uses the number of weight parameters. In this study, we propose using eight dataset versions with different frequency-time resolutions to search for an effective emotional speech preprocessing method. We propose a 6-layer convolutional neural network (CNN) model with efficient channel attention (ECA) to pursue an efficient model structure. In particular, the well-positioned ECA blocks can improve channel feature representation with only a few parameters. With the interactive emotional dyadic motion capture (IEMOCAP) dataset, increasing the frequency resolution in preprocessing emotional speech can improve emotion recognition performance. Also, ECA after the deep convolution layer can effectively increase channel feature representation. Consequently, the best result (79.37UA 79.68WA) can be obtained, exceeding the performance of previous SER models. Furthermore, to compensate for the lack of emotional speech data, we experiment with multiple preprocessing data methods that augment trainable data preprocessed with all different settings from one sample. In the experiment, we can achieve the highest result (80.28UA 80.46WA).
          </p>
         </div>
        </dd>
        <dt>
         <a name="item31">
          [31]
         </a>
         <a href="/abs/2409.04025" id="2409.04025" title="Abstract">
          arXiv:2409.04025
         </a>
         (cross-list from cs.CV)

        [
         <a aria-labelledby="pdf-2409.04025" href="/pdf/2409.04025" id="pdf-2409.04025" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04025" href="https://arxiv.org/html/2409.04025v1" id="html-2409.04025" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04025" href="/format/2409.04025" id="oth-2409.04025" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           BFA-YOLO: Balanced multiscale object detection network for multi-view building facade attachments detection
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Y">
            Yangguang Chen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+T">
            Tong Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+G">
            Guanzhou Chen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+K">
            Kun Zhu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tan,+X">
            Xiaoliang Tan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+J">
            Jiaqi Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+H">
            Hong Xie
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+W">
            Wenlin Zhou
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+J">
            Jingyi Zhao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Q">
            Qing Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luo,+X">
            Xiaolong Luo
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X">
            Xiaodong Zhang
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           22 pages
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Detection of building facade attachments such as doors, windows, balconies, air conditioner units, billboards, and glass curtain walls plays a pivotal role in numerous applications. Building facade attachments detection aids in vbuilding information modeling (BIM) construction and meeting Level of Detail 3 (LOD3) standards. Yet, it faces challenges like uneven object distribution, small object detection difficulty, and background interference. To counter these, we propose BFA-YOLO, a model for detecting facade attachments in multi-view images. BFA-YOLO incorporates three novel innovations: the Feature Balanced Spindle Module (FBSM) for addressing uneven distribution, the Target Dynamic Alignment Task Detection Head (TDATH) aimed at improving small object detection, and the Position Memory Enhanced Self-Attention Mechanism (PMESA) to combat background interference, with each component specifically designed to solve its corresponding challenge. Detection efficacy of deep network models deeply depends on the dataset's characteristics. Existing open source datasets related to building facades are limited by their single perspective, small image pool, and incomplete category coverage. We propose a novel method for building facade attachments detection dataset construction and construct the BFA-3D dataset for facade attachments detection. The BFA-3D dataset features multi-view, accurate labels, diverse categories, and detailed classification. BFA-YOLO surpasses YOLOv8 by 1.8% and 2.9% in mAP@0.5 on the multi-view BFA-3D and street-view Facade-WHU datasets, respectively. These results underscore BFA-YOLO's superior performance in detecting facade attachments.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item32">
          [32]
         </a>
         <a href="/abs/2409.04040" id="2409.04040" title="Abstract">
          arXiv:2409.04040
         </a>
         (cross-list from cs.CR)

        [
         <a aria-labelledby="pdf-2409.04040" href="/pdf/2409.04040" id="pdf-2409.04040" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04040" href="https://arxiv.org/html/2409.04040v1" id="html-2409.04040" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04040" href="/format/2409.04040" id="oth-2409.04040" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           A First Look At Efficient And Secure On-Device LLM Inference Against KV Leakage
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+H">
            Huan Yang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+D">
            Deyu Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+Y">
            Yudong Zhao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y">
            Yuanchun Li
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Y">
            Yunxin Liu
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Cryptography and Security (cs.CR)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Running LLMs on end devices has garnered significant attention recently due to their advantages in privacy preservation. With the advent of lightweight LLM models and specially designed GPUs, on-device LLM inference has achieved the necessary accuracy and performance metrics. However, we have identified that LLM inference on GPUs can leak privacy-sensitive intermediate information, specifically the KV pairs. An attacker could exploit these KV pairs to reconstruct the entire user conversation, leading to significant vulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE) and Trusted Execution Environments (TEE), are either too computation-intensive or resource-limited. To address these issues, we designed KV-Shield, which operates in two phases. In the initialization phase, it permutes the weight matrices so that all KV pairs are correspondingly permuted. During the runtime phase, the attention vector is inversely permuted to ensure the correctness of the layer output. All permutation-related operations are executed within the TEE, ensuring that insecure GPUs cannot access the original KV pairs, thus preventing conversation reconstruction. Finally, we theoretically analyze the correctness of KV-Shield, along with its advantages and overhead.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item33">
          [33]
         </a>
         <a href="/abs/2409.04060" id="2409.04060" title="Abstract">
          arXiv:2409.04060
         </a>
         (cross-list from cs.CV)

        [
         <a aria-labelledby="pdf-2409.04060" href="/pdf/2409.04060" id="pdf-2409.04060" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04060" href="https://arxiv.org/html/2409.04060v1" id="html-2409.04060" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04060" href="/format/2409.04060" id="oth-2409.04060" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           D4: Text-guided diffusion model-based domain adaptive data augmentation for vineyard shoot detection
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hirahara,+K">
            Kentaro Hirahara
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nakane,+C">
            Chikahito Nakane
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ebisawa,+H">
            Hajime Ebisawa
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kuroda,+T">
            Tsuyoshi Kuroda
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Iwaki,+Y">
            Yohei Iwaki
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Utsumi,+T">
            Tomoyoshi Utsumi
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nomura,+Y">
            Yuichiro Nomura
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Koike,+M">
            Makoto Koike
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mineno,+H">
            Hiroshi Mineno
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
          </div>
          <p class="mathjax">
           In an agricultural field, plant phenotyping using object detection models is gaining attention. However, collecting the training data necessary to create generic and high-precision models is extremely challenging due to the difficulty of annotation and the diversity of domains. Furthermore, it is difficult to transfer training data across different crops, and although machine learning models effective for specific environments, conditions, or crops have been developed, they cannot be widely applied in actual fields. In this study, we propose a generative data augmentation method (D4) for vineyard shoot detection. D4 uses a pre-trained text-guided diffusion model based on a large number of original images culled from video data collected by unmanned ground vehicles or other means, and a small number of annotated datasets. The proposed method generates new annotated images with background information adapted to the target domain while retaining annotation information necessary for object detection. In addition, D4 overcomes the lack of training data in agriculture, including the difficulty of annotation and diversity of domains. We confirmed that this generative data augmentation method improved the mean average precision by up to 28.65% for the BBox detection task and the average precision by up to 13.73% for the keypoint detection task for vineyard shoot detection. Our generative data augmentation method D4 is expected to simultaneously solve the cost and domain diversity issues of training data generation in agriculture and improve the generalization performance of detection models.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item34">
          [34]
         </a>
         <a href="/abs/2409.04073" id="2409.04073" title="Abstract">
          arXiv:2409.04073
         </a>
         (cross-list from cs.CL)

        [
         <a aria-labelledby="pdf-2409.04073" href="/pdf/2409.04073" id="pdf-2409.04073" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04073" href="https://arxiv.org/html/2409.04073v1" id="html-2409.04073" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04073" href="/format/2409.04073" id="oth-2409.04073" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language Model
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z">
            Zeyu Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Groth,+P">
            Paul Groth
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Calixto,+I">
            Iacer Calixto
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schelter,+S">
            Sebastian Schelter
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           12 pages excluding references, 3 figures, and 5 tables
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI); Databases (cs.DB)
          </div>
          <p class="mathjax">
           Entity matching (EM) is the problem of determining whether two records refer to same real-world entity, which is crucial in data integration, e.g., for product catalogs or address databases. A major drawback of many EM approaches is their dependence on labelled examples. We thus focus on the challenging setting of zero-shot entity matching where no labelled examples are available for an unseen target dataset. Recently, large language models (LLMs) have shown promising results for zero-shot EM, but their low throughput and high deployment cost limit their applicability and scalability.
           <br/>
           We revisit the zero-shot EM problem with AnyMatch, a small language model fine-tuned in a transfer learning setup. We propose several novel data selection techniques to generate fine-tuning data for our model, e.g., by selecting difficult pairs to match via an AutoML filter, by generating additional attribute-level examples, and by controlling label imbalance in the data.
           <br/>
           We conduct an extensive evaluation of the prediction quality and deployment cost of our model, in a comparison to thirteen baselines on nine benchmark datasets. We find that AnyMatch provides competitive prediction quality despite its small parameter size: it achieves the second-highest F1 score overall, and outperforms several other approaches that employ models with hundreds of billions of parameters. Furthermore, our approach exhibits major cost benefits: the average prediction quality of AnyMatch is within 4.4% of the state-of-the-art method MatchGPT with the proprietary trillion-parameter model GPT-4, yet AnyMatch requires four orders of magnitude less parameters and incurs a 3,899 times lower inference cost (in dollars per 1,000 tokens).
          </p>
         </div>
        </dd>
        <dt>
         <a name="item35">
          [35]
         </a>
         <a href="/abs/2409.04081" id="2409.04081" title="Abstract">
          arXiv:2409.04081
         </a>
         (cross-list from cs.CL)

        [
         <a aria-labelledby="pdf-2409.04081" href="/pdf/2409.04081" id="pdf-2409.04081" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04081" href="https://arxiv.org/html/2409.04081v1" id="html-2409.04081" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04081" href="/format/2409.04081" id="oth-2409.04081" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           UI-JEPA: Towards Active Perception of User Intent through Onscreen User Activity
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fu,+Y">
            Yicheng Fu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Anantha,+R">
            Raviteja Anantha
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vashisht,+P">
            Prabal Vashisht
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng,+J">
            Jianpeng Cheng
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Littwin,+E">
            Etai Littwin
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)
          </div>
          <p class="mathjax">
           Generating user intent from a sequence of user interface (UI) actions is a core challenge in comprehensive UI understanding. Recent advancements in multimodal large language models (MLLMs) have led to substantial progress in this area, but their demands for extensive model parameters, computing power, and high latency makes them impractical for scenarios requiring lightweight, on-device solutions with low latency or heightened privacy. Additionally, the lack of high-quality datasets has hindered the development of such lightweight models. To address these challenges, we propose UI-JEPA, a novel framework that employs masking strategies to learn abstract UI embeddings from unlabeled data through self-supervised learning, combined with an LLM decoder fine-tuned for user intent prediction. We also introduce two new UI-grounded multimodal datasets, "Intent in the Wild" (IIW) and "Intent in the Tame" (IIT), designed for few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos across 219 intent categories, while IIT contains 914 videos across 10 categories. We establish the first baselines for these datasets, showing that representations learned using a JEPA-style objective, combined with an LLM decoder, can achieve user intent predictions that match the performance of state-of-the-art large MLLMs, but with significantly reduced annotation and deployment resources. Measured by intent similarity scores, UI-JEPA outperforms GPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged across two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x reduction in computational cost and a 6.6x improvement in latency in the IIW dataset. These results underscore the effectiveness of UI-JEPA, highlighting its potential for lightweight, high-performance UI understanding.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item36">
          [36]
         </a>
         <a href="/abs/2409.04082" id="2409.04082" title="Abstract">
          arXiv:2409.04082
         </a>
         (cross-list from cs.CV)

        [
         <a aria-labelledby="pdf-2409.04082" href="/pdf/2409.04082" id="pdf-2409.04082" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04082" href="https://arxiv.org/html/2409.04082v1" id="html-2409.04082" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04082" href="/format/2409.04082" id="oth-2409.04082" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           SDformerFlow: Spatiotemporal swin spikeformer for event-based optical flow estimation
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tian,+Y">
            Yi Tian
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Andrade-Cetto,+J">
            Juan Andrade-Cetto
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Under Review
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Event cameras generate asynchronous and sparse event streams capturing changes in light intensity. They offer significant advantages over conventional frame-based cameras, such as a higher dynamic range and an extremely faster data rate, making them particularly useful in scenarios involving fast motion or challenging lighting conditions. Spiking neural networks (SNNs) share similar asynchronous and sparse characteristics and are well-suited for processing data from event cameras. Inspired by the potential of transformers and spike-driven transformers (spikeformers) in other computer vision tasks, we propose two solutions for fast and robust optical flow estimation for event cameras: STTFlowNet and SDformerFlow. STTFlowNet adopts a U-shaped artificial neural network (ANN) architecture with spatiotemporal shifted window self-attention (swin) transformer encoders, while SDformerFlow presents its fully spiking counterpart, incorporating swin spikeformer encoders. Furthermore, we present two variants of the spiking version with different neuron models. Our work is the first to make use of spikeformers for dense optical flow estimation. We conduct end-to-end training for all models using supervised learning. Our results yield state-of-the-art performance among SNN-based event optical flow methods on both the DSEC and MVSEC datasets, and show significant reduction in power consumption compared to the equivalent ANNs.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item37">
          [37]
         </a>
         <a href="/abs/2409.04103" id="2409.04103" title="Abstract">
          arXiv:2409.04103
         </a>
         (cross-list from cs.LG)

        [
         <a aria-labelledby="pdf-2409.04103" href="/pdf/2409.04103" id="pdf-2409.04103" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04103" href="https://arxiv.org/html/2409.04103v1" id="html-2409.04103" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04103" href="/format/2409.04103" id="oth-2409.04103" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           The Role of Graph Topology in the Performance of Biomedical Knowledge Graph Completion Models
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cattaneo,+A">
            Alberto Cattaneo
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bonner,+S">
            Stephen Bonner
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Martynec,+T">
            Thomas Martynec
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luschi,+C">
            Carlo Luschi
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Barrett,+I+P">
            Ian P Barrett
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Justus,+D">
            Daniel Justus
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)
          </div>
          <p class="mathjax">
           Knowledge Graph Completion has been increasingly adopted as a useful method for several tasks in biomedical research, like drug repurposing or drug-target identification. To that end, a variety of datasets and Knowledge Graph Embedding models has been proposed over the years. However, little is known about the properties that render a dataset useful for a given task and, even though theoretical properties of Knowledge Graph Embedding models are well understood, their practical utility in this field remains controversial. We conduct a comprehensive investigation into the topological properties of publicly available biomedical Knowledge Graphs and establish links to the accuracy observed in real-world applications. By releasing all model predictions and a new suite of analysis tools we invite the community to build upon our work and continue improving the understanding of these crucial applications.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item38">
          [38]
         </a>
         <a href="/abs/2409.04104" id="2409.04104" title="Abstract">
          arXiv:2409.04104
         </a>
         (cross-list from cs.LG)

        [
         <a aria-labelledby="pdf-2409.04104" href="/pdf/2409.04104" id="pdf-2409.04104" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04104" href="https://arxiv.org/html/2409.04104v1" id="html-2409.04104" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04104" href="/format/2409.04104" id="oth-2409.04104" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           MixNet: Joining Force of Classical and Modern Approaches Toward the Comprehensive Pipeline in Motor Imagery EEG Classification
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Autthasan,+P">
            Phairot Autthasan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chaisaen,+R">
            Rattanaphon Chaisaen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Phan,+H">
            Huy Phan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=De+Vos,+M">
            Maarten De Vos
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wilaiprasitporn,+T">
            Theerawit Wilaiprasitporn
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Supplementary materials and source codes are available on-line at
           <a class="link-external link-https" href="https://github.com/Max-Phairot-A/MixNet" rel="external noopener nofollow">
            this https URL
           </a>
          </div>
          <div class="list-journal-ref">
           <span class="descriptor">
            Journal-ref:
           </span>
           IEEE Internet of Things Journal 2024
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Signal Processing (eess.SP)
          </div>
          <p class="mathjax">
           Recent advances in deep learning (DL) have significantly impacted motor imagery (MI)-based brain-computer interface (BCI) systems, enhancing the decoding of electroencephalography (EEG) signals. However, most studies struggle to identify discriminative patterns across subjects during MI tasks, limiting MI classification performance. In this article, we propose MixNet, a novel classification framework designed to overcome this limitation by utilizing spectral-spatial signals from MI data, along with a multitask learning architecture named MIN2Net, for classification. Here, the spectral-spatial signals are generated using the filter-bank common spatial patterns (FBCSPs) method on MI data. Since the multitask learning architecture is used for the classification task, the learning in each task may exhibit different generalization rates and potential overfitting across tasks. To address this issue, we implement adaptive gradient blending, simultaneously regulating multiple loss weights and adjusting the learning pace for each task based on its generalization/overfitting tendencies. Experimental results on six benchmark data sets of different data sizes demonstrate that MixNet consistently outperforms all state-of-the-art algorithms in subject-dependent and -independent settings. Finally, the low-density EEG MI classification results show that MixNet outperforms all state-of-the-art algorithms, offering promising implications for Internet of Thing (IoT) applications, such as lightweight and portable EEG wearable devices based on low-density montages.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item39">
          [39]
         </a>
         <a href="/abs/2409.04109" id="2409.04109" title="Abstract">
          arXiv:2409.04109
         </a>
         (cross-list from cs.CL)

        [
         <a aria-labelledby="pdf-2409.04109" href="/pdf/2409.04109" id="pdf-2409.04109" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04109" href="https://arxiv.org/html/2409.04109v1" id="html-2409.04109" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04109" href="/format/2409.04109" id="oth-2409.04109" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Si,+C">
            Chenglei Si
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+D">
            Diyi Yang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hashimoto,+T">
            Tatsunori Hashimoto
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           main paper is 20 pages
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)
          </div>
          <p class="mathjax">
           Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p &lt; 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item40">
          [40]
         </a>
         <a href="/abs/2409.04114" id="2409.04114" title="Abstract">
          arXiv:2409.04114
         </a>
         (cross-list from cs.CL)

        [
         <a aria-labelledby="pdf-2409.04114" href="/pdf/2409.04114" id="pdf-2409.04114" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04114" href="https://arxiv.org/html/2409.04114v1" id="html-2409.04114" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04114" href="/format/2409.04114" id="oth-2409.04114" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Multi-Programming Language Ensemble for Code Generation in Large Language Model
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xue,+T">
            Tengfei Xue
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+X">
            Xuefeng Li
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Azim,+T">
            Tahir Azim
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Smirnov,+R">
            Roman Smirnov
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+J">
            Jianhui Yu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sadrieh,+A">
            Arash Sadrieh
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pahlavan,+B">
            Babak Pahlavan
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Code available at
           <a class="link-external link-https" href="https://github.com/NinjaTech-AI/MPLE" rel="external noopener nofollow">
            this https URL
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Large language models (LLMs) have significantly improved code generation, particularly in one-pass code generation. However, most existing approaches focus solely on generating code in a single programming language, overlooking the potential of leveraging the multi-language capabilities of LLMs. LLMs have varying patterns of errors across different languages, suggesting that a more robust approach could be developed by leveraging these multi-language outputs. In this study, we propose Multi-Programming Language Ensemble (MPLE), a novel ensemble-based method that utilizes code generation across multiple programming languages to enhance overall performance. By treating each language-specific code generation process as an individual "weak expert" and effectively integrating their outputs, our method mitigates language-specific errors and biases. This multi-language ensemble strategy leverages the complementary strengths of different programming languages, enabling the model to produce more accurate and robust code. Our approach can be seamlessly integrated with commonly used techniques such as the reflection algorithm and Monte Carlo tree search to improve code generation quality further. Experimental results show that our framework consistently enhances baseline performance by up to 17.92% on existing benchmarks (HumanEval and HumanEval-plus), with a standout result of 96.25% accuracy on the HumanEval benchmark, achieving new state-of-the-art results across various LLM models. The code will be released at
           <a class="link-external link-https" href="https://github.com/NinjaTech-AI/MPLE" rel="external noopener nofollow">
            this https URL
           </a>
          </p>
         </div>
        </dd>
        <dt>
         <a name="item41">
          [41]
         </a>
         <a href="/abs/2409.04117" id="2409.04117" title="Abstract">
          arXiv:2409.04117
         </a>
         (cross-list from cs.CV)

        [
         <a aria-labelledby="pdf-2409.04117" href="/pdf/2409.04117" id="pdf-2409.04117" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04117" href="https://arxiv.org/html/2409.04117v1" id="html-2409.04117" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04117" href="/format/2409.04117" id="oth-2409.04117" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Confidence-Aware Document OCR Error Detection
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hemmer,+A">
            Arthur Hemmer
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Coustaty,+M">
            MickaÃ«l Coustaty
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bartolo,+N">
            Nicola Bartolo
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ogier,+J">
            Jean-Marc Ogier
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)
          </div>
          <p class="mathjax">
           Optical Character Recognition (OCR) continues to face accuracy challenges that impact subsequent applications. To address these errors, we explore the utility of OCR confidence scores for enhancing post-OCR error detection. Our study involves analyzing the correlation between confidence scores and error rates across different OCR systems. We develop ConfBERT, a BERT-based model that incorporates OCR confidence scores into token embeddings and offers an optional pre-training phase for noise adjustment. Our experimental results demonstrate that integrating OCR confidence scores can enhance error detection capabilities. This work underscores the importance of OCR confidence scores in improving detection accuracy and reveals substantial disparities in performance between commercial and open-source OCR technologies.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item42">
          [42]
         </a>
         <a href="/abs/2409.04142" id="2409.04142" title="Abstract">
          arXiv:2409.04142
         </a>
         (cross-list from cs.CR)

        [
         <a aria-labelledby="pdf-2409.04142" href="/pdf/2409.04142" id="pdf-2409.04142" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04142" href="https://arxiv.org/html/2409.04142v1" id="html-2409.04142" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04142" href="/format/2409.04142" id="oth-2409.04142" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Context is the Key: Backdoor Attacks for In-Context Learning with Vision Transformers
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Abad,+G">
            Gorka Abad
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Picek,+S">
            Stjepan Picek
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cavallaro,+L">
            Lorenzo Cavallaro
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Urbieta,+A">
            Aitor Urbieta
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Cryptography and Security (cs.CR)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Due to the high cost of training, large model (LM) practitioners commonly use pretrained models downloaded from untrusted sources, which could lead to owning compromised models. In-context learning is the ability of LMs to perform multiple tasks depending on the prompt or context. This can enable new attacks, such as backdoor attacks with dynamic behavior depending on how models are prompted.
           <br/>
           In this paper, we leverage the ability of vision transformers (ViTs) to perform different tasks depending on the prompts. Then, through data poisoning, we investigate two new threats: i) task-specific backdoors where the attacker chooses a target task to attack, and only the selected task is compromised at test time under the presence of the trigger. At the same time, any other task is not affected, even if prompted with the trigger. We succeeded in attacking every tested model, achieving up to 89.90\% degradation on the target task. ii) We generalize the attack, allowing the backdoor to affect \emph{any} task, even tasks unseen during the training phase. Our attack was successful on every tested model, achieving a maximum of $13\times$ degradation. Finally, we investigate the robustness of prompts and fine-tuning as techniques for removing the backdoors from the model. We found that these methods fall short and, in the best case, reduce the degradation from 89.90\% to 73.46\%.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item43">
          [43]
         </a>
         <a href="/abs/2409.04168" id="2409.04168" title="Abstract">
          arXiv:2409.04168
         </a>
         (cross-list from cs.CL)

        [
         <a aria-labelledby="pdf-2409.04168" href="/pdf/2409.04168" id="pdf-2409.04168" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04168" href="https://arxiv.org/html/2409.04168v1" id="html-2409.04168" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04168" href="/format/2409.04168" id="oth-2409.04168" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Stephan,+A">
            Andreas Stephan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+D">
            Dawei Zhu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=A%C3%9Fenmacher,+M">
            Matthias AÃenmacher
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+X">
            Xiaoyu Shen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Roth,+B">
            Benjamin Roth
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           To reduce the need for human annotations, large language models (LLMs) have been proposed as judges of the quality of other candidate models. LLM judges are typically evaluated by measuring the correlation with human judgments on generation tasks such as summarization or machine translation. In contrast, we study LLM judges on mathematical reasoning tasks. These tasks require multi-step reasoning, and the correctness of their solutions is verifiable, enabling a more objective evaluation. We perform a detailed performance analysis and find that the used judges are mostly unable to improve task performance but are able to pick the better model. Our analysis uncovers a strong correlation between judgment performance and the candidate model task performance. We observe that judges tend to choose the model of higher quality even if its answer is incorrect. Further, we show that it is possible to use statistics, such as the task performances of the individual models, to predict judgment performance. In an ablation, we either swap or mask the candidate answers and observe that judges often keep the original judgment, providing evidence that judges incorporate writing style in their judgments. In summary, we find that regularities in the judgments are quantifiable using statistical measures and provide various angles on exploiting them.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item44">
          [44]
         </a>
         <a href="/abs/2409.04180" id="2409.04180" title="Abstract">
          arXiv:2409.04180
         </a>
         (cross-list from cs.LG)

        [
         <a aria-labelledby="pdf-2409.04180" href="/pdf/2409.04180" id="pdf-2409.04180" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04180" href="https://arxiv.org/html/2409.04180v1" id="html-2409.04180" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04180" href="/format/2409.04180" id="oth-2409.04180" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           The Prevalence of Neural Collapse in Neural Multivariate Regression
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Andriopoulos,+G">
            George Andriopoulos
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dong,+Z">
            Zixuan Dong
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+L">
            Li Guo
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+Z">
            Zifan Zhao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ross,+K">
            Keith Ross
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Recently it has been observed that neural networks exhibit Neural Collapse (NC) during the final stage of training for the classification problem. We empirically show that multivariate regression, as employed in imitation learning and other applications, exhibits Neural Regression Collapse (NRC), a new form of neural collapse: (NRC1) The last-layer feature vectors collapse to the subspace spanned by the $n$ principal components of the feature vectors, where $n$ is the dimension of the targets (for univariate regression, $n=1$); (NRC2) The last-layer feature vectors also collapse to the subspace spanned by the last-layer weight vectors; (NRC3) The Gram matrix for the weight vectors converges to a specific functional form that depends on the covariance matrix of the targets. After empirically establishing the prevalence of (NRC1)-(NRC3) for a variety of datasets and network architectures, we provide an explanation of these phenomena by modeling the regression task in the context of the Unconstrained Feature Model (UFM), in which the last layer feature vectors are treated as free variables when minimizing the loss function. We show that when the regularization parameters in the UFM model are strictly positive, then (NRC1)-(NRC3) also emerge as solutions in the UFM optimization problem. We also show that if the regularization parameters are equal to zero, then there is no collapse. To our knowledge, this is the first empirical and theoretical study of neural collapse in the context of regression. This extension is significant not only because it broadens the applicability of neural collapse to a new category of problems but also because it suggests that the phenomena of neural collapse could be a universal behavior in deep learning.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item45">
          [45]
         </a>
         <a href="/abs/2409.04183" id="2409.04183" title="Abstract">
          arXiv:2409.04183
         </a>
         (cross-list from cs.CL)

        [
         <a aria-labelledby="pdf-2409.04183" href="/pdf/2409.04183" id="pdf-2409.04183" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04183" href="https://arxiv.org/html/2409.04183v1" id="html-2409.04183" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04183" href="/format/2409.04183" id="oth-2409.04183" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z">
            Ziyin Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+H">
            Hang Yu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+S">
            Shijie Li
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Di,+P">
            Peng Di
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J">
            Jianguo Li
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+R">
            Rui Wang
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Programming languages possess rich semantic information such as data flow that is represented by graphs and not available from the surface form of source code. Recent code language models have scaled to billions of parameters, but model source code solely as text tokens while ignoring any other structural information. Conversely, models that do encode structural information of code make modifications to the Transformer architecture, limiting their scale and compatibility with pretrained LLMs. In this work, we take the best of both worlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph neural networks and cross-modal alignment technologies to inject the structural information of code into LLMs as an auxiliary task during finetuning. This framework is both model-agnostic and task-agnostic, as it can be applied to any code LLM for any code downstream task, and requires the structural graph data only at training time from a corpus unrelated to the finetuning data, while incurring no cost at inference time over the baseline LLM. Experiments on five code tasks with four different baseline LLMs ranging in size from 350M to 8B validate the effectiveness of GALLa, demonstrating consistent improvement over the baseline, even for powerful models such as LLaMA3.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item46">
          [46]
         </a>
         <a href="/abs/2409.04196" id="2409.04196" title="Abstract">
          arXiv:2409.04196
         </a>
         (cross-list from cs.CV)

        [
         <a aria-labelledby="pdf-2409.04196" href="/pdf/2409.04196" id="pdf-2409.04196" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04196" href="https://arxiv.org/html/2409.04196v1" id="html-2409.04196" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04196" href="/format/2409.04196" id="oth-2409.04196" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           GST: Precise 3D Human Body from a Single Image with Gaussian Splatting Transformers
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Prospero,+L">
            Lorenza Prospero
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hamdi,+A">
            Abdullah Hamdi
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Henriques,+J+F">
            Joao F. Henriques
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rupprecht,+C">
            Christian Rupprecht
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           preprint
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Reconstructing realistic 3D human models from monocular images has significant applications in creative industries, human-computer interfaces, and healthcare. We base our work on 3D Gaussian Splatting (3DGS), a scene representation composed of a mixture of Gaussians. Predicting such mixtures for a human from a single input image is challenging, as it is a non-uniform density (with a many-to-one relationship with input pixels) with strict physical constraints. At the same time, it needs to be flexible to accommodate a variety of clothes and poses. Our key observation is that the vertices of standardized human meshes (such as SMPL) can provide an adequate density and approximate initial position for Gaussians. We can then train a transformer model to jointly predict comparatively small adjustments to these positions, as well as the other Gaussians' attributes and the SMPL parameters. We show empirically that this combination (using only multi-view supervision) can achieve fast inference of 3D human models from a single image without test-time optimization, expensive diffusion models, or 3D points supervision. We also show that it can improve 3D pose estimation by better fitting human models that account for clothes and other variations. The code is available on the project website
           <a class="link-external link-https" href="https://abdullahamdi.com/gst/" rel="external noopener nofollow">
            this https URL
           </a>
           .
          </p>
         </div>
        </dd>
        <dt>
         <a name="item47">
          [47]
         </a>
         <a href="/abs/2409.04230" id="2409.04230" title="Abstract">
          arXiv:2409.04230
         </a>
         (cross-list from cs.RO)

        [
         <a aria-labelledby="pdf-2409.04230" href="/pdf/2409.04230" id="pdf-2409.04230" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04230" href="https://arxiv.org/html/2409.04230v1" id="html-2409.04230" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04230" href="/format/2409.04230" id="oth-2409.04230" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           SPACE: A Python-based Simulator for Evaluating Decentralized Multi-Robot Task Allocation Algorithms
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jang,+I">
            Inmo Jang
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Robotics (cs.RO)
           </span>
           ; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)
          </div>
          <p class="mathjax">
           Swarm robotics explores the coordination of multiple robots to achieve collective goals, with collective decision-making being a central focus. This process involves decentralized robots autonomously making local decisions and communicating them, which influences the overall emergent behavior. Testing such decentralized algorithms in real-world scenarios with hundreds or more robots is often impractical, underscoring the need for effective simulation tools. We propose SPACE (Swarm Planning and Control Evaluation), a Python-based simulator designed to support the research, evaluation, and comparison of decentralized Multi-Robot Task Allocation (MRTA) algorithms. SPACE streamlines core algorithmic development by allowing users to implement decision-making algorithms as Python plug-ins, easily construct agent behavior trees via an intuitive GUI, and leverage built-in support for inter-agent communication and local task awareness. To demonstrate its practical utility, we implement and evaluate CBBA and GRAPE within the simulator, comparing their performance across different metrics, particularly in scenarios with dynamically introduced tasks. This evaluation shows the usefulness of SPACE in conducting rigorous and standardized comparisons of MRTA algorithms, helping to support future research in the field.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item48">
          [48]
         </a>
         <a href="/abs/2409.04244" id="2409.04244" title="Abstract">
          arXiv:2409.04244
         </a>
         (cross-list from cs.LG)

        [
         <a aria-labelledby="pdf-2409.04244" href="/pdf/2409.04244" id="pdf-2409.04244" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="oth-2409.04244" href="/format/2409.04244" id="oth-2409.04244" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           WarpAdam: A new Adam optimizer based on Meta-Learning approach
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pan,+C">
            Chengxi Pan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+J">
            Junshang Chen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ye,+J">
            Jingrui Ye
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)
          </div>
          <p class="mathjax">
           Optimal selection of optimization algorithms is crucial for training deep learning models. The Adam optimizer has gained significant attention due to its efficiency and wide applicability. However, to enhance the adaptability of optimizers across diverse datasets, we propose an innovative optimization strategy by integrating the 'warped gradient descend'concept from Meta Learning into the Adam optimizer. In the conventional Adam optimizer, gradients are utilized to compute estimates of gradient mean and variance, subsequently updating model parameters. Our approach introduces a learnable distortion matrix, denoted as P, which is employed for linearly transforming gradients. This transformation slightly adjusts gradients during each iteration, enabling the optimizer to better adapt to distinct dataset characteristics. By learning an appropriate distortion matrix P, our method aims to adaptively adjust gradient information across different data distributions, thereby enhancing optimization performance. Our research showcases the potential of this novel approach through theoretical insights and empirical evaluations. Experimental results across various tasks and datasets validate the superiority of our optimizer that integrates the 'warped gradient descend' concept in terms of adaptability. Furthermore, we explore effective strategies for training the adaptation matrix P and identify scenarios where this method can yield optimal results. In summary, this study introduces an innovative approach that merges the 'warped gradient descend' concept from Meta Learning with the Adam optimizer. By introducing a learnable distortion matrix P within the optimizer, we aim to enhance the model's generalization capability across diverse data distributions, thus opening up new possibilities in the field of deep learning optimization.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item49">
          [49]
         </a>
         <a href="/abs/2409.04249" id="2409.04249" title="Abstract">
          arXiv:2409.04249
         </a>
         (cross-list from cs.DC)

        [
         <a aria-labelledby="pdf-2409.04249" href="/pdf/2409.04249" id="pdf-2409.04249" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04249" href="https://arxiv.org/html/2409.04249v1" id="html-2409.04249" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04249" href="/format/2409.04249" id="oth-2409.04249" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Hermes: Memory-Efficient Pipeline Inference for Large Models on Edge Devices
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Han,+X">
            Xueyuan Han
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cai,+Z">
            Zinuo Cai
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Y">
            Yichu Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fan,+C">
            Chongxin Fan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J">
            Junhan Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+R">
            Ruhui Ma
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Buyya,+R">
            Rajkumar Buyya
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Accepted by the 42nd IEEE International Conference on Computer Design (ICCD 2024)
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Distributed, Parallel, and Cluster Computing (cs.DC)
           </span>
           ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
          </div>
          <p class="mathjax">
           The application of Transformer-based large models has achieved numerous success in recent years. However, the exponential growth in the parameters of large models introduces formidable memory challenge for edge deployment. Prior works to address this challenge mainly focus on optimizing the model structure and adopting memory swapping methods. However, the former reduces the inference accuracy, and the latter raises the inference latency. This paper introduces PIPELOAD, a novel memory-efficient pipeline execution mechanism. It reduces memory usage by incorporating dynamic memory management and minimizes inference latency by employing parallel model loading. Based on PIPELOAD mechanism, we present Hermes, a framework optimized for large model inference on edge devices. We evaluate Hermes on Transformer-based models of different sizes. Our experiments illustrate that Hermes achieves up to 4.24 X increase in inference speed and 86.7% lower memory consumption than the state-of-the-art pipeline mechanism for BERT and ViT models, 2.58 X increase in inference speed and 90.3% lower memory consumption for GPT-style models.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item50">
          [50]
         </a>
         <a href="/abs/2409.04272" id="2409.04272" title="Abstract">
          arXiv:2409.04272
         </a>
         (cross-list from cs.CV)

        [
         <a aria-labelledby="pdf-2409.04272" href="/pdf/2409.04272" id="pdf-2409.04272" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04272" href="https://arxiv.org/html/2409.04272v1" id="html-2409.04272" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04272" href="/format/2409.04272" id="oth-2409.04272" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Cycle Pixel Difference Network for Crisp Edge Detection
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+C">
            Changsong Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+W">
            Wei Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Y">
            Yanyan Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+M">
            Mingyang Li
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+W">
            Wenlin Li
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fan,+Y">
            Yimeng Fan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bai,+X">
            Xiangnan Bai
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhangd,+L">
            Liang Zhangd
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Edge detection, as a fundamental task in computer vision, has garnered increasing attention. The advent of deep learning has significantly advanced this field. However, recent deep learning-based methods which rely on large-scale pre-trained weights cannot be trained from scratch, with very limited research addressing this issue. This paper proposes a novel cycle pixel difference convolution (CPDC), which effectively integrates image gradient information with modern convolution operations. Based on the CPDC, we develop a U-shape encoder-decoder model named CPD-Net, which is a purely end-to-end network. Additionally, to address the issue of edge thickness produced by most existing methods, we construct a multi-scale information enhancement module (MSEM) to enhance the discriminative ability of the model, thereby generating crisp and clean contour maps. Comprehensive experiments conducted on three standard benchmarks demonstrate that our method achieves competitive performance on the BSDS500 dataset (ODS=0.813), NYUD-V2 (ODS=0.760), and BIPED dataset (ODS=0.898). Our approach provides a novel perspective for addressing these challenges in edge detection.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item51">
          [51]
         </a>
         <a href="/abs/2409.04290" id="2409.04290" title="Abstract">
          arXiv:2409.04290
         </a>
         (cross-list from cs.LG)

        [
         <a aria-labelledby="pdf-2409.04290" href="/pdf/2409.04290" id="pdf-2409.04290" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04290" href="https://arxiv.org/html/2409.04290v1" id="html-2409.04290" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04290" href="/format/2409.04290" id="oth-2409.04290" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Knottenbelt,+W">
            William Knottenbelt
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+Z">
            Zeyu Gao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wray,+R">
            Rebecca Wray
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+W+Z">
            Woody Zhidong Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J">
            Jiashuai Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Crispin-Ortuzar,+M">
            Mireia Crispin-Ortuzar
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Survival analysis is a branch of statistics used for modeling the time until a specific event occurs and is widely used in medicine, engineering, finance, and many other fields. When choosing survival models, there is typically a trade-off between performance and interpretability, where the highest performance is achieved by black-box models based on deep learning. This is a major problem in fields such as medicine where practitioners are reluctant to blindly trust black-box models to make important patient decisions. Kolmogorov-Arnold Networks (KANs) were recently proposed as an interpretable and accurate alternative to multi-layer perceptrons (MLPs). We introduce CoxKAN, a Cox proportional hazards Kolmogorov-Arnold Network for interpretable, high-performance survival analysis. We evaluate the proposed CoxKAN on 4 synthetic datasets and 9 real medical datasets. The synthetic experiments demonstrate that CoxKAN accurately recovers interpretable symbolic formulae for the hazard function, and effectively performs automatic feature selection. Evaluation on the 9 real datasets show that CoxKAN consistently outperforms the Cox proportional hazards model and achieves performance that is superior or comparable to that of tuned MLPs. Furthermore, we find that CoxKAN identifies complex interactions between predictor variables that would be extremely difficult to recognise using existing survival methods, and automatically finds symbolic formulae which uncover the precise effect of important biomarkers on patient risk.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item52">
          [52]
         </a>
         <a href="/abs/2409.04306" id="2409.04306" title="Abstract">
          arXiv:2409.04306
         </a>
         (cross-list from cs.RO)

        [
         <a aria-labelledby="pdf-2409.04306" href="/pdf/2409.04306" id="pdf-2409.04306" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="oth-2409.04306" href="/format/2409.04306" id="oth-2409.04306" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Safe and Efficient Path Planning under Uncertainty via Deep Collision Probability Fields
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Herrmann,+F">
            Felix Herrmann
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zach,+S">
            Sebastian Zach
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Banfi,+J">
            Jacopo Banfi
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Peters,+J">
            Jan Peters
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chalvatzaki,+G">
            Georgia Chalvatzaki
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tateo,+D">
            Davide Tateo
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Preprint version of a paper accepted to the IEEE Robotics and Automation Letters
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Robotics (cs.RO)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Estimating collision probabilities between robots and environmental obstacles or other moving agents is crucial to ensure safety during path planning. This is an important building block of modern planning algorithms in many application scenarios such as autonomous driving, where noisy sensors perceive obstacles. While many approaches exist, they either provide too conservative estimates of the collision probabilities or are computationally intensive due to their sampling-based nature. To deal with these issues, we introduce Deep Collision Probability Fields, a neural-based approach for computing collision probabilities of arbitrary objects with arbitrary unimodal uncertainty distributions. Our approach relegates the computationally intensive estimation of collision probabilities via sampling at the training step, allowing for fast neural network inference of the constraints during planning. In extensive experiments, we show that Deep Collision Probability Fields can produce reasonably accurate collision probabilities (up to 10^{-3}) for planning and that our approach can be easily plugged into standard path planning approaches to plan safe paths on 2-D maps containing uncertain static and dynamic obstacles. Additional material, code, and videos are available at
           <a class="link-external link-https" href="https://sites.google.com/view/ral-dcpf" rel="external noopener nofollow">
            this https URL
           </a>
           .
          </p>
         </div>
        </dd>
        <dt>
         <a name="item53">
          [53]
         </a>
         <a href="/abs/2409.04318" id="2409.04318" title="Abstract">
          arXiv:2409.04318
         </a>
         (cross-list from cs.CL)

        [
         <a aria-labelledby="pdf-2409.04318" href="/pdf/2409.04318" id="pdf-2409.04318" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04318" href="https://arxiv.org/html/2409.04318v1" id="html-2409.04318" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04318" href="/format/2409.04318" id="oth-2409.04318" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Learning vs Retrieval: The Role of In-Context Examples in Regression with LLMs
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nafar,+A">
            Aliakbar Nafar
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Venable,+K+B">
            Kristen Brent Venable
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kordjamshidi,+P">
            Parisa Kordjamshidi
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Generative Large Language Models (LLMs) are capable of being in-context learners. However, the underlying mechanism of in-context learning (ICL) is still a major research question, and experimental research results about how models exploit ICL are not always consistent. In this work, we propose a framework for evaluating in-context learning mechanisms, which we claim are a combination of retrieving internal knowledge and learning from in-context examples by focusing on regression tasks. First, we show that LLMs can perform regression on real-world datasets and then design experiments to measure the extent to which the LLM retrieves its internal knowledge versus learning from in-context examples. We argue that this process lies on a spectrum between these two extremes. We provide an in-depth analysis of the degrees to which these mechanisms are triggered depending on various factors, such as prior knowledge about the tasks and the type and richness of the information provided by the in-context examples. We employ three LLMs and utilize multiple datasets to corroborate the robustness of our findings. Our results shed light on how to engineer prompts to leverage meta-learning from in-context examples and foster knowledge retrieval depending on the problem being addressed.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item54">
          [54]
         </a>
         <a href="/abs/2409.04340" id="2409.04340" title="Abstract">
          arXiv:2409.04340
         </a>
         (cross-list from cs.LG)

        [
         <a aria-labelledby="pdf-2409.04340" href="/pdf/2409.04340" id="pdf-2409.04340" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04340" href="https://arxiv.org/html/2409.04340v1" id="html-2409.04340" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04340" href="/format/2409.04340" id="oth-2409.04340" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           AGR: Age Group fairness Reward for Bias Mitigation in LLMs
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cao,+S">
            Shuirong Cao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng,+R">
            Ruoxi Cheng
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">
            Zhiqiang Wang
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           The first two authors contributed equally to this work. Corresponding to Zhiqiang Wang. ACKNOWLEDGMENT: we would like to thank the computing resources support from the State Key Laboratory of New Computer Software Technologies at Nanjing University
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)
          </div>
          <p class="mathjax">
           LLMs can exhibit age biases, resulting in unequal treatment of individuals across age groups. While much research has addressed racial and gender biases, age bias remains little explored. The scarcity of instruction-tuning and preference datasets for age bias hampers its detection and measurement, and existing fine-tuning methods seldom address age-related fairness. In this paper, we construct age bias preference datasets and instruction-tuning datasets for RLHF. We introduce ARG, an age fairness reward to reduce differences in the response quality of LLMs across different age groups. Extensive experiments demonstrate that this reward significantly improves response accuracy and reduces performance disparities across age groups. Our source code and datasets are available at the anonymous \href{https://anonymous.4open.science/r/FairRLHF-D445/readme.md}{link}.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item55">
          [55]
         </a>
         <a href="/abs/2409.04341" id="2409.04341" title="Abstract">
          arXiv:2409.04341
         </a>
         (cross-list from cs.CR)

        [
         <a aria-labelledby="pdf-2409.04341" href="/pdf/2409.04341" id="pdf-2409.04341" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04341" href="https://arxiv.org/html/2409.04341v1" id="html-2409.04341" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04341" href="/format/2409.04341" id="oth-2409.04341" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Towards Fine-Grained Webpage Fingerprinting at Scale
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+X">
            Xiyuan Zhao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+X">
            Xinhao Deng
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Q">
            Qi Li
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Y">
            Yunpeng Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Z">
            Zhuotao Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+K">
            Kun Sun
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+K">
            Ke Xu
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           To appear in the Proceedings of The ACM Conference on Computer and Communications Security (CCS), 2024
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Cryptography and Security (cs.CR)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Website Fingerprinting (WF) attacks can effectively identify the websites visited by Tor clients via analyzing encrypted traffic patterns. Existing attacks focus on identifying different websites, but their accuracy dramatically decreases when applied to identify fine-grained webpages, especially when distinguishing among different subpages of the same website. WebPage Fingerprinting (WPF) attacks face the challenges of highly similar traffic patterns and a much larger scale of webpages. Furthermore, clients often visit multiple webpages concurrently, increasing the difficulty of extracting the traffic patterns of each webpage from the obfuscated traffic. In this paper, we propose Oscar, a WPF attack based on multi-label metric learning that identifies different webpages from obfuscated traffic by transforming the feature space. Oscar can extract the subtle differences among various webpages, even those with similar traffic patterns. In particular, Oscar combines proxy-based and sample-based metric learning losses to extract webpage features from obfuscated traffic and identify multiple webpages. We prototype Oscar and evaluate its performance using traffic collected from 1,000 monitored webpages and over 9,000 unmonitored webpages in the real world. Oscar demonstrates an 88.6% improvement in the multi-label metric Recall@5 compared to the state-of-the-art attacks.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item56">
          [56]
         </a>
         <a href="/abs/2409.04360" id="2409.04360" title="Abstract">
          arXiv:2409.04360
         </a>
         (cross-list from cs.CV)

        [
         <a aria-labelledby="pdf-2409.04360" href="/pdf/2409.04360" id="pdf-2409.04360" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04360" href="https://arxiv.org/html/2409.04360v1" id="html-2409.04360" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04360" href="/format/2409.04360" id="oth-2409.04360" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Connectivity-Inspired Network for Context-Aware Recognition
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Carloni,+G">
            Gianluca Carloni
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Colantonio,+S">
            Sara Colantonio
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           ECCV 2024 - HCV Workshop, Accepted for presentation, Submitted Manuscript Version (adapted to include author names, Acknowledgements, and reference DOIs): the version of the manuscript improved after peer review will appear in the Proceedings later
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)
          </div>
          <p class="mathjax">
           The aim of this paper is threefold. We inform the AI practitioner about the human visual system with an extensive literature review; we propose a novel biologically motivated neural network for image classification; and, finally, we present a new plug-and-play module to model context awareness. We focus on the effect of incorporating circuit motifs found in biological brains to address visual recognition. Our convolutional architecture is inspired by the connectivity of human cortical and subcortical streams, and we implement bottom-up and top-down modulations that mimic the extensive afferent and efferent connections between visual and cognitive areas. Our Contextual Attention Block is simple and effective and can be integrated with any feed-forward neural network. It infers weights that multiply the feature maps according to their causal influence on the scene, modeling the co-occurrence of different objects in the image. We place our module at different bottlenecks to infuse a hierarchical context awareness into the model. We validated our proposals through image classification experiments on benchmark data and found a consistent improvement in performance and the robustness of the produced explanations via class activation. Our code is available at
           <a class="link-external link-https" href="https://github.com/gianlucarloni/CoCoReco" rel="external noopener nofollow">
            this https URL
           </a>
           .
          </p>
         </div>
        </dd>
        <dt>
         <a name="item57">
          [57]
         </a>
         <a href="/abs/2409.04367" id="2409.04367" title="Abstract">
          arXiv:2409.04367
         </a>
         (cross-list from cs.LG)

        [
         <a aria-labelledby="pdf-2409.04367" href="/pdf/2409.04367" id="pdf-2409.04367" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04367" href="https://arxiv.org/html/2409.04367v1" id="html-2409.04367" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04367" href="/format/2409.04367" id="oth-2409.04367" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Provable Hyperparameter Tuning for Structured Pfaffian Settings
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Balcan,+M">
            Maria-Florina Balcan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nguyen,+A+T">
            Anh Tuan Nguyen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sharma,+D">
            Dravyansh Sharma
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
          </div>
          <p class="mathjax">
           Data-driven algorithm design automatically adapts algorithms to specific application domains, achieving better performance. In the context of parameterized algorithms, this approach involves tuning the algorithm parameters using problem instances drawn from the problem distribution of the target application domain. While empirical evidence supports the effectiveness of data-driven algorithm design, providing theoretical guarantees for several parameterized families remains challenging. This is due to the intricate behaviors of their corresponding utility functions, which typically admit piece-wise and discontinuity structures. In this work, we present refined frameworks for providing learning guarantees for parameterized data-driven algorithm design problems in both distributional and online learning settings. For the distributional learning setting, we introduce the Pfaffian GJ framework, an extension of the classical GJ framework, capable of providing learning guarantees for function classes for which the computation involves Pfaffian functions. Unlike the GJ framework, which is limited to function classes with computation characterized by rational functions, our proposed framework can deal with function classes involving Pfaffian functions, which are much more general and widely applicable. We then show that for many parameterized algorithms of interest, their utility function possesses a refined piece-wise structure, which automatically translates to learning guarantees using our proposed framework. For the online learning setting, we provide a new tool for verifying dispersion property of a sequence of loss functions. This sufficient condition allows no-regret learning for sequences of piece-wise structured loss functions where the piece-wise structure involves Pfaffian transition boundaries.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item58">
          [58]
         </a>
         <a href="/abs/2409.04368" id="2409.04368" title="Abstract">
          arXiv:2409.04368
         </a>
         (cross-list from eess.IV)

        [
         <a aria-labelledby="pdf-2409.04368" href="/pdf/2409.04368" id="pdf-2409.04368" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04368" href="https://arxiv.org/html/2409.04368v1" id="html-2409.04368" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04368" href="/format/2409.04368" id="oth-2409.04368" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Szumel,+G">
            Gregory Szumel
           </a>
           ,
           <a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Guo,+B">
            Brian Guo
           </a>
           ,
           <a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Lu,+D">
            Darui Lu
           </a>
           ,
           <a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Gui,+R">
            Rongze Gui
           </a>
           ,
           <a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Wang,+T">
            Tingyu Wang
           </a>
           ,
           <a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Konz,+N">
            Nicholas Konz
           </a>
           ,
           <a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Mazurowski,+M+A">
            Maciej A. Mazurowski
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Image and Video Processing (eess.IV)
           </span>
           ; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)
          </div>
          <p class="mathjax">
           Purpose: Medical images acquired using different scanners and protocols can differ substantially in their appearance. This phenomenon, scanner domain shift, can result in a drop in the performance of deep neural networks which are trained on data acquired by one scanner and tested on another. This significant practical issue is well-acknowledged, however, no systematic study of the issue is available across different modalities and diagnostic tasks. Materials and Methods: In this paper, we present a broad experimental study evaluating the impact of scanner domain shift on convolutional neural network performance for different automated diagnostic tasks. We evaluate this phenomenon in common radiological modalities, including X-ray, CT, and MRI. Results: We find that network performance on data from a different scanner is almost always worse than on same-scanner data, and we quantify the degree of performance drop across different datasets. Notably, we find that this drop is most severe for MRI, moderate for X-ray, and quite small for CT, on average, which we attribute to the standardized nature of CT acquisition systems which is not present in MRI or X-ray. We also study how injecting varying amounts of target domain data into the training set, as well as adding noise to the training data, helps with generalization. Conclusion: Our results provide extensive experimental evidence and quantification of the extent of performance drop caused by scanner domain shift in deep learning across different modalities, with the goal of guiding the future development of robust deep learning models for medical image analysis.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item59">
          [59]
         </a>
         <a href="/abs/2409.04388" id="2409.04388" title="Abstract">
          arXiv:2409.04388
         </a>
         (cross-list from cs.CV)

        [
         <a aria-labelledby="pdf-2409.04388" href="/pdf/2409.04388" id="pdf-2409.04388" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04388" href="https://arxiv.org/html/2409.04388v1" id="html-2409.04388" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04388" href="/format/2409.04388" id="oth-2409.04388" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Question-Answering Dense Video Events
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qin,+H">
            Hangyu Qin
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao,+J">
            Junbin Xiao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yao,+A">
            Angela Yao
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI); Multimedia (cs.MM)
          </div>
          <p class="mathjax">
           Multimodal Large Language Models (MLLMs) have shown excellent performance in question-answering of single-event videos. In this paper, we present question-answering dense video events, a novel task that requires answering and grounding the dense-event questions in long videos, thus challenging MLLMs to faithfully comprehend and reason about multiple events occurring over extended time periods. To facilitate the study, we construct DeVE-QA - a dataset featuring 78K questions about 26K events on 10.6K long videos. We then benchmark and show that existing MLLMs excelling at single-event QA struggle to perform well in DeVE-QA. For improvement, we propose DeVi, a novel training-free MLLM approach that highlights a hierarchical captioning module, a temporal event memory module, and a self-consistency checking module to respectively detect, contextualize and memorize, and ground dense-events in long videos for question answering. Extensive experiments show that DeVi is superior at answering dense-event questions and grounding relevant video moments. Compared with existing MLLMs, it achieves a remarkable increase of 4.1 percent and 3.7 percent for G(round)QA accuracy on DeVE-QA and NExT-GQA respectively.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item60">
          [60]
         </a>
         <a href="/abs/2409.04398" id="2409.04398" title="Abstract">
          arXiv:2409.04398
         </a>
         (cross-list from cs.CV)

        [
         <a aria-labelledby="pdf-2409.04398" href="/pdf/2409.04398" id="pdf-2409.04398" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04398" href="https://arxiv.org/html/2409.04398v1" id="html-2409.04398" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04398" href="/format/2409.04398" id="oth-2409.04398" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           HiSC4D: Human-centered interaction and 4D Scene Capture in Large-scale Space Using Wearable IMUs and LiDAR
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dai,+Y">
            Yudi Dai
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">
            Zhiyong Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+X">
            Xiping Lin
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wen,+C">
            Chenglu Wen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+L">
            Lan Xu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+S">
            Siqi Shen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+Y">
            Yuexin Ma
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+C">
            Cheng Wang
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           17 pages, 10 figures, Jornal
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI); Graphics (cs.GR); Multimedia (cs.MM)
          </div>
          <p class="mathjax">
           We introduce HiSC4D, a novel Human-centered interaction and 4D Scene Capture method, aimed at accurately and efficiently creating a dynamic digital world, containing large-scale indoor-outdoor scenes, diverse human motions, rich human-human interactions, and human-environment interactions. By utilizing body-mounted IMUs and a head-mounted LiDAR, HiSC4D can capture egocentric human motions in unconstrained space without the need for external devices and pre-built maps. This affords great flexibility and accessibility for human-centered interaction and 4D scene capturing in various environments. Taking into account that IMUs can capture human spatially unrestricted poses but are prone to drifting for long-period using, and while LiDAR is stable for global localization but rough for local positions and orientations, HiSC4D employs a joint optimization method, harmonizing all sensors and utilizing environment cues, yielding promising results for long-term capture in large scenes. To promote research of egocentric human interaction in large scenes and facilitate downstream tasks, we also present a dataset, containing 8 sequences in 4 large scenes (200 to 5,000 $m^2$), providing 36k frames of accurate 4D human motions with SMPL annotations and dynamic scenes, 31k frames of cropped human point clouds, and scene mesh of the environment. A variety of scenarios, such as the basketball gym and commercial street, alongside challenging human motions, such as daily greeting, one-on-one basketball playing, and tour guiding, demonstrate the effectiveness and the generalization ability of HiSC4D. The dataset and code will be publicated on
           <a class="link-external link-http" href="http://www.lidarhumanmotion.net/hisc4d" rel="external noopener nofollow">
            this http URL
           </a>
           available for research purposes.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item61">
          [61]
         </a>
         <a href="/abs/2409.04410" id="2409.04410" title="Abstract">
          arXiv:2409.04410
         </a>
         (cross-list from cs.CV)

        [
         <a aria-labelledby="pdf-2409.04410" href="/pdf/2409.04410" id="pdf-2409.04410" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04410" href="https://arxiv.org/html/2409.04410v1" id="html-2409.04410" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04410" href="/format/2409.04410" id="oth-2409.04410" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Open-MAGVIT2: An Open-Source Project Toward Democratizing Auto-regressive Visual Generation
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luo,+Z">
            Zhuoyan Luo
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shi,+F">
            Fengyuan Shi
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ge,+Y">
            Yixiao Ge
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+Y">
            Yujiu Yang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+L">
            Limin Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shan,+Y">
            Ying Shan
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           We present Open-MAGVIT2, a family of auto-regressive image generation models ranging from 300M to 1.5B. The Open-MAGVIT2 project produces an open-source replication of Google's MAGVIT-v2 tokenizer, a tokenizer with a super-large codebook (i.e., $2^{18}$ codes), and achieves the state-of-the-art reconstruction performance (1.17 rFID) on ImageNet $256 \times 256$. Furthermore, we explore its application in plain auto-regressive models and validate scalability properties. To assist auto-regressive models in predicting with a super-large vocabulary, we factorize it into two sub-vocabulary of different sizes by asymmetric token factorization, and further introduce "next sub-token prediction" to enhance sub-token interaction for better generation quality. We release all models and codes to foster innovation and creativity in the field of auto-regressive visual generation.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item62">
          [62]
         </a>
         <a href="/abs/2409.04421" id="2409.04421" title="Abstract">
          arXiv:2409.04421
         </a>
         (cross-list from cs.CL)

        [
         <a aria-labelledby="pdf-2409.04421" href="/pdf/2409.04421" id="pdf-2409.04421" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04421" href="https://arxiv.org/html/2409.04421v1" id="html-2409.04421" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04421" href="/format/2409.04421" id="oth-2409.04421" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+J">
            Jiaxing Wu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ning,+L">
            Lin Ning
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+L">
            Luyang Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+H">
            Harrison Lee
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+N">
            Neo Wu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+C">
            Chao Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Prakash,+S">
            Sushant Prakash
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=O'Banion,+S">
            Shawn O'Banion
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Green,+B">
            Bradley Green
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+J">
            Jun Xie
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
          </div>
          <p class="mathjax">
           LLM-powered personalization agent systems employ Large Language Models (LLMs) to predict users' behavior from their past activities. However, their effectiveness often hinges on the ability to effectively leverage extensive, long user historical data due to its inherent noise and length of such data. Existing pretrained LLMs may generate summaries that are concise but lack the necessary context for downstream tasks, hindering their utility in personalization systems. To address these challenges, we introduce Reinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to generate concise, human-readable user summaries that are optimized for downstream task performance. By maximizing the usefulness of the generated summaries, RLPF effectively distills extensive user history data while preserving essential information for downstream tasks. Our empirical evaluation demonstrates significant improvements in both extrinsic downstream task utility and intrinsic summary quality, surpassing baseline methods by up to 22% on downstream task performance and achieving an up to 84.59% win rate on Factuality, Abstractiveness, and Readability. RLPF also achieves a remarkable 74% reduction in context length while improving performance on 16 out of 19 unseen tasks and/or datasets, showcasing its generalizability. This approach offers a promising solution for enhancing LLM personalization by effectively transforming long, noisy user histories into informative and human-readable representations.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item63">
          [63]
         </a>
         <a href="/abs/2409.04428" id="2409.04428" title="Abstract">
          arXiv:2409.04428
         </a>
         (cross-list from cs.LG)

        [
         <a aria-labelledby="pdf-2409.04428" href="/pdf/2409.04428" id="pdf-2409.04428" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04428" href="https://arxiv.org/html/2409.04428v1" id="html-2409.04428" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04428" href="/format/2409.04428" id="oth-2409.04428" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Hybrid Spiking Neural Networks for Low-Power Intra-Cortical Brain-Machine Interfaces
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vasilache,+A">
            Alexandru Vasilache
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Krausse,+J">
            Jann Krausse
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Knobloch,+K">
            Klaus Knobloch
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Becker,+J">
            Juergen Becker
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           This work has been accepted at the 2024 IEEE Biomedical Circuits and Systems Conference
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI); Neurons and Cognition (q-bio.NC)
          </div>
          <p class="mathjax">
           Intra-cortical brain-machine interfaces (iBMIs) have the potential to dramatically improve the lives of people with paraplegia by restoring their ability to perform daily activities. However, current iBMIs suffer from scalability and mobility limitations due to bulky hardware and wiring. Wireless iBMIs offer a solution but are constrained by a limited data rate. To overcome this challenge, we are investigating hybrid spiking neural networks for embedded neural decoding in wireless iBMIs. The networks consist of a temporal convolution-based compression followed by recurrent processing and a final interpolation back to the original sequence length. As recurrent units, we explore gated recurrent units (GRUs), leaky integrate-and-fire (LIF) neurons, and a combination of both - spiking GRUs (sGRUs) and analyze their differences in terms of accuracy, footprint, and activation sparsity. To that end, we train decoders on the "Nonhuman Primate Reaching with Multichannel Sensorimotor Cortex Electrophysiology" dataset and evaluate it using the NeuroBench framework, targeting both tracks of the IEEE BioCAS Grand Challenge on Neural Decoding. Our approach achieves high accuracy in predicting velocities of primate reaching movements from multichannel primary motor cortex recordings while maintaining a low number of synaptic operations, surpassing the current baseline models in the NeuroBench framework. This work highlights the potential of hybrid neural networks to facilitate wireless iBMIs with high decoding precision and a substantial increase in the number of monitored neurons, paving the way toward more advanced neuroprosthetic technologies.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item64">
          [64]
         </a>
         <a href="/abs/2409.04432" id="2409.04432" title="Abstract">
          arXiv:2409.04432
         </a>
         (cross-list from cs.DL)

        [
         <a aria-labelledby="pdf-2409.04432" href="/pdf/2409.04432" id="pdf-2409.04432" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04432" href="https://arxiv.org/html/2409.04432v1" id="html-2409.04432" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04432" href="/format/2409.04432" id="oth-2409.04432" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           A Survey on Knowledge Organization Systems of Research Fields: Resources and Challenges
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Salatino,+A">
            Angelo Salatino
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Aggarwal,+T">
            Tanay Aggarwal
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mannocci,+A">
            Andrea Mannocci
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Osborne,+F">
            Francesco Osborne
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Motta,+E">
            Enrico Motta
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Digital Libraries (cs.DL)
           </span>
           ; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)
          </div>
          <p class="mathjax">
           Knowledge Organization Systems (KOSs), such as term lists, thesauri, taxonomies, and ontologies, play a fundamental role in categorising, managing, and retrieving information. In the academic domain, KOSs are often adopted for representing research areas and their relationships, primarily aiming to classify research articles, academic courses, patents, books, scientific venues, domain experts, grants, software, experiment materials, and several other relevant products and agents. These structured representations of research areas, widely embraced by many academic fields, have proven effective in empowering AI-based systems to i) enhance retrievability of relevant documents, ii) enable advanced analytic solutions to quantify the impact of academic research, and iii) analyse and forecast research dynamics. This paper aims to present a comprehensive survey of the current KOS for academic disciplines. We analysed and compared 45 KOSs according to five main dimensions: scope, structure, curation, usage, and links to other KOSs. Our results reveal a very heterogeneous scenario in terms of scope, scale, quality, and usage, highlighting the need for more integrated solutions for representing research knowledge across academic fields. We conclude by discussing the main challenges and the most promising future directions.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item65">
          [65]
         </a>
         <a href="/abs/2409.04434" id="2409.04434" title="Abstract">
          arXiv:2409.04434
         </a>
         (cross-list from cs.LG)

        [
         <a aria-labelledby="pdf-2409.04434" href="/pdf/2409.04434" id="pdf-2409.04434" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.04434" href="https://arxiv.org/html/2409.04434v1" id="html-2409.04434" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.04434" href="/format/2409.04434" id="oth-2409.04434" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Accelerating Training with Neuron Interaction and Nowcasting Networks
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Knyazev,+B">
            Boris Knyazev
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Moudgil,+A">
            Abhinav Moudgil
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lajoie,+G">
            Guillaume Lajoie
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Belilovsky,+E">
            Eugene Belilovsky
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lacoste-Julien,+S">
            Simon Lacoste-Julien
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           code
           <a class="link-external link-https" href="https://github.com/SamsungSAILMontreal/nino" rel="external noopener nofollow">
            this https URL
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
          </div>
          <p class="mathjax">
           Neural network training can be accelerated when a learnable update rule is used in lieu of classic adaptive optimizers (e.g. Adam). However, learnable update rules can be costly and unstable to train and use. A simpler recently proposed approach to accelerate training is to use Adam for most of the optimization steps and periodically, only every few steps, nowcast (predict future) parameters. We improve this approach by Neuron interaction and Nowcasting (NiNo) networks. NiNo leverages neuron connectivity and graph neural networks to more accurately nowcast parameters by learning in a supervised way from a set of training trajectories over multiple tasks. We show that in some networks, such as Transformers, neuron connectivity is non-trivial. By accurately modeling neuron connectivity, we allow NiNo to accelerate Adam training by up to 50\% in vision and language tasks.
          </p>
         </div>
        </dd>
       </dl>
       <dl id="articles">
        <h3>
         Replacement submissions for Monday, 9 September 2024 (showing 51 of 51 entries )
        </h3>
        <dt>
         <a name="item66">
          [66]
         </a>
         <a href="/abs/2304.12653" id="2304.12653" title="Abstract">
          arXiv:2304.12653
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2304.12653" href="/pdf/2304.12653" id="pdf-2304.12653" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2304.12653" href="https://arxiv.org/html/2304.12653v3" id="html-2304.12653" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2304.12653" href="/format/2304.12653" id="oth-2304.12653" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+M">
            Min Yang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+G">
            Guanjun Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+Z">
            Ziyuan Zhou
           </a>
          </div>
          <div class="list-journal-ref">
           <span class="descriptor">
            Journal-ref:
           </span>
           Drones 2023, 7(7), 476
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Artificial Intelligence (cs.AI)
           </span>
          </div>
          <p class="mathjax">
           Traditional multi-agent reinforcement learning algorithms are difficultly applied in a large-scale multi-agent environment. The introduction of mean field theory has enhanced the scalability of multi-agent reinforcement learning in recent years. This paper considers partially observable multi-agent reinforcement learning (MARL), where each agent can only observe other agents within a fixed range. This partial observability affects the agent's ability to assess the quality of the actions of surrounding agents. This paper focuses on developing a method to capture more effective information from local observations in order to select more effective actions. Previous work in this field employs probability distributions or weighted mean field to update the average actions of neighborhood agents, but it does not fully consider the feature information of surrounding neighbors and leads to a local optimum. In this paper, we propose a novel multi-agent reinforcement learning algorithm, Partially Observable Mean Field Multi-Agent Reinforcement Learning based on Graph--Attention (GAMFQ) to remedy this flaw. GAMFQ uses a graph attention module and a mean field module to describe how an agent is influenced by the actions of other agents at each time step. This graph attention module consists of a graph attention encoder and a differentiable attention mechanism, and this mechanism outputs a dynamic graph to represent the effectiveness of neighborhood agents against central agents. The mean--field module approximates the effect of a neighborhood agent on a central agent as the average effect of effective neighborhood agents. We evaluate GAMFQ on three challenging tasks in the MAgents framework. Experiments show that GAMFQ outperforms baselines including the state-of-the-art partially observable mean-field reinforcement learning algorithms.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item67">
          [67]
         </a>
         <a href="/abs/2404.04298" id="2404.04298" title="Abstract">
          arXiv:2404.04298
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2404.04298" href="/pdf/2404.04298" id="pdf-2404.04298" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2404.04298" href="https://arxiv.org/html/2404.04298v3" id="html-2404.04298" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2404.04298" href="/format/2404.04298" id="oth-2404.04298" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           SELF-[IN]CORRECT: LLMs Struggle with Discriminating Self-Generated Responses
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+D">
            Dongwei Jiang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+J">
            Jingyu Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Weller,+O">
            Orion Weller
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Weir,+N">
            Nathaniel Weir
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Van+Durme,+B">
            Benjamin Van Durme
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Khashabi,+D">
            Daniel Khashabi
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Artificial Intelligence (cs.AI)
           </span>
           ; Computation and Language (cs.CL); Machine Learning (cs.LG)
          </div>
          <p class="mathjax">
           Can LLMs consistently improve their previous outputs for better results? For this to be true, LLMs would need to be better at discriminating among previously-generated alternatives, than generating initial responses. We explore the validity of this hypothesis in practice. We first formulate a unified framework that allows us to compare the generative and discriminative capability of any model on any task. In our resulting experimental analysis of several open-source and industrial LLMs, we observe that models are not reliably better at discriminating among previously-generated alternatives than generating initial responses. This finding challenges the notion that LLMs may be able to enhance their performance only through their own judgment.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item68">
          [68]
         </a>
         <a href="/abs/2405.02664" id="2405.02664" title="Abstract">
          arXiv:2405.02664
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2405.02664" href="/pdf/2405.02664" id="pdf-2405.02664" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2405.02664" href="https://arxiv.org/html/2405.02664v3" id="html-2405.02664" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2405.02664" href="/format/2405.02664" id="oth-2405.02664" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           MedPromptExtract (Medical Data Extraction Tool): Anonymization and Hi-fidelity Automated data extraction using NLP and prompt engineering
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Roomani">
            Roomani Srivastava
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Prasad,+S">
            Suraj Prasad
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bhat,+L">
            Lipika Bhat
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Deshpande,+S">
            Sarvesh Deshpande
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Das,+B">
            Barnali Das
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jadhav,+K">
            Kshitij Jadhav
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Artificial Intelligence (cs.AI)
           </span>
           ; Information Retrieval (cs.IR)
          </div>
          <p class="mathjax">
           Introduction: The labour-intensive nature of data extraction from sources like discharge summaries (DS) poses significant obstacles to the digitisation of medical records particularly for low- and middle-income countries (LMICs). In this paper we present a completely automated method MedPromptExtract to efficiently extract data from DS while maintaining confidentiality. Methods: The source of data was Discharge Summaries (DS) from Kokilaben Dhirubhai Ambani Hospital (KDAH) of patients having Acute Kidney Injury (AKI). A pre-existing tool EIGEN which leverages semi-supervised learning techniques for high-fidelity information extraction was used to anonymize the DS, Natural Language Processing (NLP) was used to extract data from regular fields. We used Prompt Engineering and Large Language Model(LLM) to extract custom clinical information from free flowing text describing the patients stay in the hospital. Twelve features associated with occurrence of AKI were extracted. The LLM responses were validated against clinicians annotations. Results: The MedPromptExtracttool first subjected DS to the anonymization pipeline which took three seconds per summary. Successful anonymization was verified by clinicians, thereafter NLP pipeline extracted structured text from the anonymized pdfs at the rate of 0.2 seconds per summary with 100% accuracy.Finally DS were analysed by the LLM pipeline using Gemini Pro for the twelve features. Accuracy metrics were calculated by comparing model responses to clinicians annotations with seven features achieving AUCs above 0.9, indicating high fidelity of the extraction process. Conclusion: MedPromptExtract serves as an automated adaptable tool for efficient data extraction from medical records with a dynamic user interface. Keywords: Digitizing Medical Records, Automated Anonymisation, Information Retrieval, Large Language Models, Prompt Engineering
          </p>
         </div>
        </dd>
        <dt>
         <a name="item69">
          [69]
         </a>
         <a href="/abs/2405.20234" id="2405.20234" title="Abstract">
          arXiv:2405.20234
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2405.20234" href="/pdf/2405.20234" id="pdf-2405.20234" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2405.20234" href="https://arxiv.org/html/2405.20234v3" id="html-2405.20234" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2405.20234" href="/format/2405.20234" id="oth-2405.20234" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Hidden in Plain Sight: Exploring Chat History Tampering in Interactive Language Models
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei,+C">
            Cheng'an Wei
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+Y">
            Yue Zhao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gong,+Y">
            Yujia Gong
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+K">
            Kai Chen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiang,+L">
            Lu Xiang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+S">
            Shenchen Zhu
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Artificial Intelligence (cs.AI)
           </span>
          </div>
          <p class="mathjax">
           Large Language Models (LLMs) such as ChatGPT and Llama have become prevalent in real-world applications, exhibiting impressive text generation performance. LLMs are fundamentally developed from a scenario where the input data remains static and unstructured. To behave interactively, LLM-based chat systems must integrate prior chat history as context into their inputs, following a pre-defined structure. However, LLMs cannot separate user inputs from context, enabling chat history tampering. This paper introduces a systematic methodology to inject user-supplied history into LLM conversations without any prior knowledge of the target model. The key is to utilize prompt templates that can well organize the messages to be injected, leading the target LLM to interpret them as genuine chat history. To automatically search for effective templates in a WebUI black-box setting, we propose the LLM-Guided Genetic Algorithm (LLMGA) that leverages an LLM to generate and iteratively optimize the templates. We apply the proposed method to popular real-world LLMs including ChatGPT and Llama-2/3. The results show that chat history tampering can enhance the malleability of the model's behavior over time and greatly influence the model output. For example, it can improve the success rate of disallowed response elicitation up to 97% on ChatGPT. Our findings provide insights into the challenges associated with the real-world deployment of interactive LLMs.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item70">
          [70]
         </a>
         <a href="/abs/2211.03933" id="2211.03933" title="Abstract">
          arXiv:2211.03933
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2211.03933" href="/pdf/2211.03933" id="pdf-2211.03933" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2211.03933" href="https://arxiv.org/html/2211.03933v3" id="html-2211.03933" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2211.03933" href="/format/2211.03933" id="oth-2211.03933" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           A Hypergraph-Based Machine Learning Ensemble Network Intrusion Detection System
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+Z">
            Zong-Zhi Lin
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pike,+T+D">
            Thomas D. Pike
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bailey,+M+M">
            Mark M. Bailey
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bastian,+N+D">
            Nathaniel D. Bastian
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           in IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2024. An updated version of this work has been accepted for publication in an IEEE journal available here:
           <a class="link-external link-https" href="https://ieeexplore.ieee.org/document/10666746" rel="external noopener nofollow">
            this https URL
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Cryptography and Security (cs.CR)
           </span>
           ; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Methodology (stat.ME); Machine Learning (stat.ML)
          </div>
          <p class="mathjax">
           Network intrusion detection systems (NIDS) to detect malicious attacks continue to meet challenges. NIDS are often developed offline while they face auto-generated port scan infiltration attempts, resulting in a significant time lag from adversarial adaption to NIDS response. To address these challenges, we use hypergraphs focused on internet protocol addresses and destination ports to capture evolving patterns of port scan attacks. The derived set of hypergraph-based metrics are then used to train an ensemble machine learning (ML) based NIDS that allows for real-time adaption in monitoring and detecting port scanning activities, other types of attacks, and adversarial intrusions at high accuracy, precision and recall performances. This ML adapting NIDS was developed through the combination of (1) intrusion examples, (2) NIDS update rules, (3) attack threshold choices to trigger NIDS retraining requests, and (4) a production environment with no prior knowledge of the nature of network traffic. 40 scenarios were auto-generated to evaluate the ML ensemble NIDS comprising three tree-based models. The resulting ML Ensemble NIDS was extended and evaluated with the CIC-IDS2017 dataset. Results show that under the model settings of an Update-ALL-NIDS rule (specifically retrain and update all the three models upon the same NIDS retraining request) the proposed ML ensemble NIDS evolved intelligently and produced the best results with nearly 100% detection performance throughout the simulation.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item71">
          [71]
         </a>
         <a href="/abs/2212.12741" id="2212.12741" title="Abstract">
          arXiv:2212.12741
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2212.12741" href="/pdf/2212.12741" id="pdf-2212.12741" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2212.12741" href="https://arxiv.org/html/2212.12741v2" id="html-2212.12741" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2212.12741" href="/format/2212.12741" id="oth-2212.12741" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           LMFLOSS: A Hybrid Loss For Imbalanced Medical Image Classification
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sadi,+A+A">
            Abu Adnan Sadi
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chowdhury,+L">
            Labib Chowdhury
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jahan,+N">
            Nusrat Jahan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rafi,+M+N+S">
            Mohammad Newaz Sharif Rafi
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chowdhury,+R">
            Radeya Chowdhury
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Khan,+F+A">
            Faisal Ahamed Khan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mohammed,+N">
            Nabeel Mohammed
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           21 pages, 4 figures, a detailed version of our previous submission with additional findings
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           With advances in digital technology, the classification of medical images has become a crucial step for image-based clinical decision support systems. Automatic medical image classification represents a pivotal domain where the use of AI holds the potential to create a significant social impact. However, several challenges act as obstacles to the development of practical and effective solutions. One of these challenges is the prevalent class imbalance problem in most medical imaging datasets. As a result, existing AI techniques, particularly deep-learning-based methodologies, often underperform in such scenarios. In this study, we propose a novel framework called Large Margin aware Focal (LMF) loss to mitigate the class imbalance problem in medical imaging. The LMF loss represents a linear combination of two loss functions optimized by two hyperparameters. This framework harnesses the distinct characteristics of both loss functions by enforcing wider margins for minority classes while simultaneously emphasizing challenging samples found in the datasets. We perform rigorous experiments on three neural network architectures and with four medical imaging datasets. We provide empirical evidence that our proposed framework consistently outperforms other baseline methods, showing an improvement of 2%-9% in macro-f1 scores. Through class-wise analysis of f1 scores, we also demonstrate how the proposed framework can significantly improve performance for minority classes. The results of our experiments show that our proposed framework can perform consistently well across different architectures and datasets. Overall, our study demonstrates a simple and effective approach to addressing the class imbalance problem in medical imaging datasets. We hope our work will inspire new research toward a more generalized approach to medical image classification.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item72">
          [72]
         </a>
         <a href="/abs/2304.08275" id="2304.08275" title="Abstract">
          arXiv:2304.08275
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2304.08275" href="/pdf/2304.08275" id="pdf-2304.08275" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2304.08275" href="https://arxiv.org/html/2304.08275v4" id="html-2304.08275" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2304.08275" href="/format/2304.08275" id="oth-2304.08275" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Implementing Responsible AI: Tensions and Trade-Offs Between Ethics Aspects
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sanderson,+C">
            Conrad Sanderson
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Douglas,+D">
            David Douglas
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+Q">
            Qinghua Lu
           </a>
          </div>
          <div class="list-journal-ref">
           <span class="descriptor">
            Journal-ref:
           </span>
           International Joint Conference on Neural Networks (IJCNN), 2023
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computers and Society (cs.CY)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Many sets of ethics principles for responsible AI have been proposed to allay concerns about misuse and abuse of AI/ML systems. The underlying aspects of such sets of principles include privacy, accuracy, fairness, robustness, explainability, and transparency. However, there are potential tensions between these aspects that pose difficulties for AI/ML developers seeking to follow these principles. For example, increasing the accuracy of an AI/ML system may reduce its explainability. As part of the ongoing effort to operationalise the principles into practice, in this work we compile and discuss a catalogue of 10 notable tensions, trade-offs and other interactions between the underlying aspects. We primarily focus on two-sided interactions, drawing on support spread across a diverse literature. This catalogue can be helpful in raising awareness of the possible interactions between aspects of ethics principles, as well as facilitating well-supported judgements by the designers and developers of AI/ML systems.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item73">
          [73]
         </a>
         <a href="/abs/2304.09639" id="2304.09639" title="Abstract">
          arXiv:2304.09639
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2304.09639" href="/pdf/2304.09639" id="pdf-2304.09639" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2304.09639" href="https://arxiv.org/html/2304.09639v3" id="html-2304.09639" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2304.09639" href="/format/2304.09639" id="oth-2304.09639" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           The Transformation Logics
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ronca,+A">
            Alessandro Ronca
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Extended version with appendix of a paper with the same title that appears in the proceedings of IJCAI 2024
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Logic in Computer Science (cs.LO)
           </span>
           ; Artificial Intelligence (cs.AI); Formal Languages and Automata Theory (cs.FL)
          </div>
          <p class="mathjax">
           We introduce a new family of temporal logics designed to finely balance the trade-off between expressivity and complexity. Their key feature is the possibility of defining operators of a new kind that we call transformation operators. Some of them subsume existing temporal operators, while others are entirely novel. Of particular interest are transformation operators based on semigroups. They enable logics to harness the richness of semigroup theory, and we show them to yield logics capable of creating hierarchies of increasing expressivity and complexity which are non-trivial to characterise in existing logics. The result is a genuinely novel and yet unexplored landscape of temporal logics, each of them with the potential of matching the trade-off between expressivity and complexity required by specific applications.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item74">
          [74]
         </a>
         <a href="/abs/2306.13004" id="2306.13004" title="Abstract">
          arXiv:2306.13004
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2306.13004" href="/pdf/2306.13004" id="pdf-2306.13004" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2306.13004" href="https://arxiv.org/html/2306.13004v5" id="html-2306.13004" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2306.13004" href="/format/2306.13004" id="oth-2306.13004" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Can Differentiable Decision Trees Enable Interpretable Reward Learning from Human Feedback?
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kalra,+A">
            Akansha Kalra
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Brown,+D+S">
            Daniel S. Brown
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Accepted at RLC 2024
          </div>
          <div class="list-journal-ref">
           <span class="descriptor">
            Journal-ref:
           </span>
           Reinforcement Learning Journal, vol. 1, no. 1, 2024, pp. TBD
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for capturing human intent to alleviate the challenges of hand-crafting the reward values. Despite the increasing interest in RLHF, most works learn black box reward functions that while expressive are difficult to interpret and often require running the whole costly process of RL before we can even decipher if these frameworks are actually aligned with human preferences. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs). Our experiments across several domains, including CartPole, Visual Gridworld environments and Atari games, provide evidence that the tree structure of our learned reward function is useful in determining the extent to which the reward function is aligned with human preferences. We also provide experimental evidence that not only shows that reward DDTs can often achieve competitive RL performance when compared with larger capacity deep neural network reward functions but also demonstrates the diagnostic utility of our framework in checking alignment of learned reward functions. We also observe that the choice between soft and hard (argmax) output of reward DDT reveals a tension between wanting highly shaped rewards to ensure good RL performance, while also wanting simpler, more interpretable rewards. Videos and code, are available at:
           <a class="link-external link-https" href="https://sites.google.com/view/ddt-rlhf" rel="external noopener nofollow">
            this https URL
           </a>
          </p>
         </div>
        </dd>
        <dt>
         <a name="item75">
          [75]
         </a>
         <a href="/abs/2307.11133" id="2307.11133" title="Abstract">
          arXiv:2307.11133
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2307.11133" href="/pdf/2307.11133" id="pdf-2307.11133" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2307.11133" href="https://arxiv.org/html/2307.11133v3" id="html-2307.11133" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2307.11133" href="/format/2307.11133" id="oth-2307.11133" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Contrastive Graph Pooling for Explainable Classification of Brain Networks
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/q-bio?searchtype=author&amp;query=Xu,+J">
            Jiaxing Xu
           </a>
           ,
           <a href="https://arxiv.org/search/q-bio?searchtype=author&amp;query=Bian,+Q">
            Qingtian Bian
           </a>
           ,
           <a href="https://arxiv.org/search/q-bio?searchtype=author&amp;query=Li,+X">
            Xinhang Li
           </a>
           ,
           <a href="https://arxiv.org/search/q-bio?searchtype=author&amp;query=Zhang,+A">
            Aihu Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/q-bio?searchtype=author&amp;query=Ke,+Y">
            Yiping Ke
           </a>
           ,
           <a href="https://arxiv.org/search/q-bio?searchtype=author&amp;query=Qiao,+M">
            Miao Qiao
           </a>
           ,
           <a href="https://arxiv.org/search/q-bio?searchtype=author&amp;query=Zhang,+W">
            Wei Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/q-bio?searchtype=author&amp;query=Sim,+W+K+J">
            Wei Khang Jeremy Sim
           </a>
           ,
           <a href="https://arxiv.org/search/q-bio?searchtype=author&amp;query=Guly%C3%A1s,+B">
            BalÃ¡zs GulyÃ¡s
           </a>
          </div>
          <div class="list-journal-ref">
           <span class="descriptor">
            Journal-ref:
           </span>
           IEEE Transactions on Medical Imaging, vol. 43, no. 9, pp. 3292-3305, Sept. 2024
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Neurons and Cognition (q-bio.NC)
           </span>
           ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
          </div>
          <p class="mathjax">
           Functional magnetic resonance imaging (fMRI) is a commonly used technique to measure neural activation. Its application has been particularly important in identifying underlying neurodegenerative conditions such as Parkinson's, Alzheimer's, and Autism. Recent analysis of fMRI data models the brain as a graph and extracts features by graph neural networks (GNNs). However, the unique characteristics of fMRI data require a special design of GNN. Tailoring GNN to generate effective and domain-explainable features remains challenging. In this paper, we propose a contrastive dual-attention block and a differentiable graph pooling method called ContrastPool to better utilize GNN for brain networks, meeting fMRI-specific requirements. We apply our method to 5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its superiority over state-of-the-art baselines. Our case study confirms that the patterns extracted by our method match the domain knowledge in neuroscience literature, and disclose direct and interesting insights. Our contributions underscore the potential of ContrastPool for advancing the understanding of brain networks and neurodegenerative conditions. The source code is available at
           <a class="link-external link-https" href="https://github.com/AngusMonroe/ContrastPool" rel="external noopener nofollow">
            this https URL
           </a>
           .
          </p>
         </div>
        </dd>
        <dt>
         <a name="item76">
          [76]
         </a>
         <a href="/abs/2308.06493" id="2308.06493" title="Abstract">
          arXiv:2308.06493
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2308.06493" href="/pdf/2308.06493" id="pdf-2308.06493" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2308.06493" href="https://arxiv.org/html/2308.06493v3" id="html-2308.06493" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2308.06493" href="/format/2308.06493" id="oth-2308.06493" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           EgoPoser: Robust Real-Time Egocentric Pose Estimation from Sparse and Intermittent Observations Everywhere
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+J">
            Jiaxi Jiang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Streli,+P">
            Paul Streli
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Meier,+M">
            Manuel Meier
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Holz,+C">
            Christian Holz
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Accepted by ECCV 2024, Code:
           <a class="link-external link-https" href="https://siplab.org/projects/EgoPoser" rel="external noopener nofollow">
            this https URL
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI); Graphics (cs.GR); Human-Computer Interaction (cs.HC)
          </div>
          <p class="mathjax">
           Full-body egocentric pose estimation from head and hand poses alone has become an active area of research to power articulate avatar representations on headset-based platforms. However, existing methods over-rely on the indoor motion-capture spaces in which datasets were recorded, while simultaneously assuming continuous joint motion capture and uniform body dimensions. We propose EgoPoser to overcome these limitations with four main contributions. 1) EgoPoser robustly models body pose from intermittent hand position and orientation tracking only when inside a headset's field of view. 2) We rethink input representations for headset-based ego-pose estimation and introduce a novel global motion decomposition method that predicts full-body pose independent of global positions. 3) We enhance pose estimation by capturing longer motion time series through an efficient SlowFast module design that maintains computational efficiency. 4) EgoPoser generalizes across various body shapes for different users. We experimentally evaluate our method and show that it outperforms state-of-the-art methods both qualitatively and quantitatively while maintaining a high inference speed of over 600fps. EgoPoser establishes a robust baseline for future work where full-body pose estimation no longer needs to rely on outside-in capture and can scale to large-scale and unseen environments.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item77">
          [77]
         </a>
         <a href="/abs/2310.00505" id="2310.00505" title="Abstract">
          arXiv:2310.00505
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2310.00505" href="/pdf/2310.00505" id="pdf-2310.00505" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="oth-2310.00505" href="/format/2310.00505" id="oth-2310.00505" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Unveiling the Unborn: Advancing Fetal Health Classification through Machine Learning
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mandala,+S+K">
            Sujith K Mandala
           </a>
          </div>
          <div class="list-journal-ref">
           <span class="descriptor">
            Journal-ref:
           </span>
           AIH 2024, 1(1), 57:67
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Fetal health classification is a critical task in obstetrics, enabling early identification and management of potential health problems. However, it remains challenging due to data complexity and limited labeled samples. This research paper presents a novel machine-learning approach for fetal health classification, leveraging a LightGBM classifier trained on a comprehensive dataset. The proposed model achieves an impressive accuracy of 98.31% on a test set. Our findings demonstrate the potential of machine learning in enhancing fetal health classification, offering a more objective and accurate assessment. Notably, our approach combines various features, such as fetal heart rate, uterine contractions, and maternal blood pressure, to provide a comprehensive evaluation. This methodology holds promise for improving early detection and treatment of fetal health issues, ensuring better outcomes for both mothers and babies. Beyond the high accuracy achieved, the novelty of our approach lies in its comprehensive feature selection and assessment methodology. By incorporating multiple data points, our model offers a more holistic and reliable evaluation compared to traditional methods. This research has significant implications in the field of obstetrics, paving the way for advancements in early detection and intervention of fetal health concerns. Future work involves validating the model on a larger dataset and developing a clinical application. Ultimately, we anticipate that our research will revolutionize the assessment and management of fetal health, contributing to improved healthcare outcomes for expectant mothers and their babies.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item78">
          [78]
         </a>
         <a href="/abs/2310.06585" id="2310.06585" title="Abstract">
          arXiv:2310.06585
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2310.06585" href="/pdf/2310.06585" id="pdf-2310.06585" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2310.06585" href="https://arxiv.org/html/2310.06585v2" id="html-2310.06585" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2310.06585" href="/format/2310.06585" id="oth-2310.06585" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           A Black-Box Physics-Informed Estimator based on Gaussian Process Regression for Robot Inverse Dynamics Identification
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Giacomuzzos,+G">
            Giulio Giacomuzzos
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Carli,+R">
            Ruggero Carli
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Romeres,+D">
            Diego Romeres
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Libera,+A+D">
            Alberto Dalla Libera
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Robotics (cs.RO)
           </span>
           ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY)
          </div>
          <p class="mathjax">
           Learning the inverse dynamics of robots directly from data, adopting a black-box approach, is interesting for several real-world scenarios where limited knowledge about the system is available. In this paper, we propose a black-box model based on Gaussian Process (GP) Regression for the identification of the inverse dynamics of robotic manipulators. The proposed model relies on a novel multidimensional kernel, called \textit{Lagrangian Inspired Polynomial} (\kernelInitials{}) kernel. The \kernelInitials{} kernel is based on two main ideas. First, instead of directly modeling the inverse dynamics components, we model as GPs the kinetic and potential energy of the system. The GP prior on the inverse dynamics components is derived from those on the energies by applying the properties of GPs under linear operators. Second, as regards the energy prior definition, we prove a polynomial structure of the kinetic and potential energy, and we derive a polynomial kernel that encodes this property. As a consequence, the proposed model allows also to estimate the kinetic and potential energy without requiring any label on these quantities. Results on simulation and on two real robotic manipulators, namely a 7 DOF Franka Emika Panda, and a 6 DOF MELFA RV4FL, show that the proposed model outperforms state-of-the-art black-box estimators based both on Gaussian Processes and Neural Networks in terms of accuracy, generality and data efficiency. The experiments on the MELFA robot also demonstrate that our approach achieves performance comparable to fine-tuned model-based estimators, despite requiring less prior information.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item79">
          [79]
         </a>
         <a href="/abs/2310.18365" id="2310.18365" title="Abstract">
          arXiv:2310.18365
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2310.18365" href="/pdf/2310.18365" id="pdf-2310.18365" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="oth-2310.18365" href="/format/2310.18365" id="oth-2310.18365" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Using GPT-4 to Augment Unbalanced Data for Automatic Scoring
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fang,+L">
            Luyang Fang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+G">
            Gyeong-Geon Lee
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhai,+X">
            Xiaoming Zhai
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)
          </div>
          <p class="mathjax">
           Machine learning-based automatic scoring faces challenges with unbalanced student responses across scoring categories. To address this, we introduce a novel text data augmentation framework leveraging GPT-4, a generative large language model, specifically tailored for unbalanced datasets in automatic scoring. Our experimental dataset comprised student written responses to four science items. We crafted prompts for GPT-4 to generate responses, especially for minority scoring classes, enhancing the data set. We then finetuned DistillBERT for automatic scoring based on the augmented and original datasets. Model performance was assessed using accuracy, precision, recall, and F1 metrics. Our findings revealed that incorporating GPT-4-augmented data remarkedly improved model performance, particularly for precision and F1 scores. Interestingly, the extent of improvement varied depending on the specific dataset and the proportion of augmented data used. Notably, we found that a varying amount of augmented data (20%-40%) was needed to obtain stable improvement for automatic scoring. Comparisons with models trained on additional student-written responses suggest that GPT-4 augmented models match those trained with student data. This research underscores the potential and effectiveness of data augmentation techniques utilizing generative large language models like GPT-4 in addressing unbalanced datasets within automated assessment.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item80">
          [80]
         </a>
         <a href="/abs/2310.19796" id="2310.19796" title="Abstract">
          arXiv:2310.19796
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2310.19796" href="/pdf/2310.19796" id="pdf-2310.19796" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2310.19796" href="https://arxiv.org/html/2310.19796v3" id="html-2310.19796" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2310.19796" href="/format/2310.19796" id="oth-2310.19796" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Re-evaluating Retrosynthesis Algorithms with Syntheseus
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Maziarz,+K">
            Krzysztof Maziarz
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tripp,+A">
            Austin Tripp
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+G">
            Guoqing Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Stanley,+M">
            Megan Stanley
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+S">
            Shufang Xie
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gai%C5%84ski,+P">
            Piotr GaiÅski
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Seidl,+P">
            Philipp Seidl
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Segler,+M">
            Marwin Segler
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Accepted for publication in Faraday Discussions
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)
          </div>
          <p class="mathjax">
           Automated Synthesis Planning has recently re-emerged as a research area at the intersection of chemistry and machine learning. Despite the appearance of steady progress, we argue that imperfect benchmarks and inconsistent comparisons mask systematic shortcomings of existing techniques, and unnecessarily hamper progress. To remedy this, we present a synthesis planning library with an extensive benchmarking framework, called syntheseus, which promotes best practice by default, enabling consistent meaningful evaluation of single-step models and multi-step planning algorithms. We demonstrate the capabilities of syntheseus by re-evaluating several previous retrosynthesis algorithms, and find that the ranking of state-of-the-art models changes in controlled evaluation experiments. We end with guidance for future works in this area, and call the community to engage in the discussion on how to improve benchmarks for synthesis planning.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item81">
          [81]
         </a>
         <a href="/abs/2311.04939" id="2311.04939" title="Abstract">
          arXiv:2311.04939
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2311.04939" href="/pdf/2311.04939" id="pdf-2311.04939" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2311.04939" href="https://arxiv.org/html/2311.04939v2" id="html-2311.04939" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2311.04939" href="/format/2311.04939" id="oth-2311.04939" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           LooGLE: Can Long-Context Language Models Understand Long Contexts?
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J">
            Jiaqi Li
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+M">
            Mengmeng Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+Z">
            Zilong Zheng
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+M">
            Muhan Zhang
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Large language models (LLMs), despite their impressive performance in various language tasks, are typically limited to processing texts within context-window size. This limitation has spurred significant research efforts to enhance LLMs' long-context understanding with high-quality long-sequence benchmarks. However, prior datasets in this regard suffer from shortcomings, such as short context length compared to the context window of modern LLMs; outdated documents that have data leakage problems; and an emphasis on short dependency tasks rather than long dependency tasks. In this paper, we present LooGLE, a Long Context Generic Language Evaluation benchmark for LLMs' long context understanding. LooGLE features relatively new documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning diverse domains. Human annotators meticulously crafted more than 1,100 high-quality question-answer pairs to meet the long dependency requirements. These pairs underwent thorough cross-validation, yielding the most precise assessment of LLMs' long dependency capabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed key findings: (i) commercial models outperformed open-sourced models; (ii) LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks; (iii) in-context learning and chaining thoughts offered only marginal improvements; (iv) retrieval-based techniques demonstrated substantial benefits for short question-answering, while strategies for extending context window length had limited impact on long context understanding. As such, LooGLE not only provides a systematic and comprehensive evaluation schema on long-context LLMs, but also sheds light on future development of enhanced models towards "true long-context understanding".
          </p>
         </div>
        </dd>
        <dt>
         <a name="item82">
          [82]
         </a>
         <a href="/abs/2401.04472" id="2401.04472" title="Abstract">
          arXiv:2401.04472
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2401.04472" href="/pdf/2401.04472" id="pdf-2401.04472" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2401.04472" href="https://arxiv.org/html/2401.04472v3" id="html-2401.04472" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2401.04472" href="/format/2401.04472" id="oth-2401.04472" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           A Survey on Efficient Federated Learning Methods for Foundation Model Training
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Woisetschl%C3%A4ger,+H">
            Herbert WoisetschlÃ¤ger
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Isenko,+A">
            Alexander Isenko
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+S">
            Shiqiang Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mayer,+R">
            Ruben Mayer
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jacobsen,+H">
            Hans-Arno Jacobsen
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Accepted for publication at IJCAI 2024. Please cite the published paper via
           <a class="link-external link-https" href="https://doi.org/10.24963/ijcai.2024/919" rel="external noopener nofollow">
            this https URL
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)
          </div>
          <p class="mathjax">
           Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training across a multitude of clients. However, new approaches to FL often discuss their contributions involving small deep-learning models only and focus on training full models on clients. In the wake of Foundation Models (FM), the reality is different for many deep learning applications. Typically, FMs have already been pre-trained across a wide variety of tasks and can be fine-tuned to specific downstream tasks over significantly smaller datasets than required for full model training. However, access to such datasets is often challenging. By its design, FL can help to open data silos. With this survey, we introduce a novel taxonomy focused on computational and communication efficiency, the vital elements to make use of FMs in FL systems. We discuss the benefits and drawbacks of parameter-efficient fine-tuning (PEFT) for FL applications, elaborate on the readiness of FL frameworks to work with FMs, and provide future research opportunities on how to evaluate generative models in FL as well as the interplay of privacy and PEFT.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item83">
          [83]
         </a>
         <a href="/abs/2402.05399" id="2402.05399" title="Abstract">
          arXiv:2402.05399
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2402.05399" href="/pdf/2402.05399" id="pdf-2402.05399" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2402.05399" href="https://arxiv.org/html/2402.05399v2" id="html-2402.05399" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2402.05399" href="/format/2402.05399" id="oth-2402.05399" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           CURE: Simulation-Augmented Auto-Tuning in Robotics
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hossen,+M+A">
            Md Abir Hossen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kharade,+S">
            Sonam Kharade
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=O'Kane,+J+M">
            Jason M. O'Kane
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schmerl,+B">
            Bradley Schmerl
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Garlan,+D">
            David Garlan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jamshidi,+P">
            Pooyan Jamshidi
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Revised submission in IEEE Transactions on Robotics (T-RO), 2024
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Robotics (cs.RO)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Robotic systems are typically composed of various subsystems, such as localization and navigation, each encompassing numerous configurable components (e.g., selecting different planning algorithms). Once an algorithm has been selected for a component, its associated configuration options must be set to the appropriate values. Configuration options across the system stack interact non-trivially. Finding optimal configurations for highly configurable robots to achieve desired performance poses a significant challenge due to the interactions between configuration options across software and hardware that result in an exponentially large and complex configuration space. These challenges are further compounded by the need for transferability between different environments and robotic platforms. Data efficient optimization algorithms (e.g., Bayesian optimization) have been increasingly employed to automate the tuning of configurable parameters in cyber-physical systems. However, such optimization algorithms converge at later stages, often after exhausting the allocated budget (e.g., optimization steps, allotted time) and lacking transferability. This paper proposes CURE -- a method that identifies causally relevant configuration options, enabling the optimization process to operate in a reduced search space, thereby enabling faster optimization of robot performance. CURE abstracts the causal relationships between various configuration options and robot performance objectives by learning a causal model in the source (a low-cost environment such as the Gazebo simulator) and applying the learned knowledge to perform optimization in the target (e.g., Turtlebot 3 physical robot). We demonstrate the effectiveness and transferability of CURE by conducting experiments that involve varying degrees of deployment changes in both physical robots and simulation.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item84">
          [84]
         </a>
         <a href="/abs/2402.09225" id="2402.09225" title="Abstract">
          arXiv:2402.09225
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2402.09225" href="/pdf/2402.09225" id="pdf-2402.09225" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2402.09225" href="https://arxiv.org/html/2402.09225v2" id="html-2402.09225" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2402.09225" href="/format/2402.09225" id="oth-2402.09225" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Is my Data in your AI Model? Membership Inference Test with Application to Face Images
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=DeAlcala,+D">
            Daniel DeAlcala
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Morales,+A">
            Aythami Morales
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fierrez,+J">
            Julian Fierrez
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mancera,+G">
            Gonzalo Mancera
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tolosana,+R">
            Ruben Tolosana
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ortega-Garcia,+J">
            Javier Ortega-Garcia
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           12 pages including references and authors
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           This article introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if given data was used during the training of AI/ML models. Specifically, we propose two MINT architectures designed to learn the distinct activation patterns that emerge when an Audited Model is exposed to data used during its training process. These architectures are based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs). The experimental framework focuses on the challenging task of Face Recognition, considering three state-of-the-art Face Recognition systems. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Different experimental scenarios are considered depending on the context of the AI model to test. Our proposed MINT approach achieves promising results, with up to 90% accuracy, indicating the potential to recognize if an AI model has been trained with specific data. The proposed MINT approach can serve to enforce privacy and fairness in several AI applications, e.g., revealing if sensitive or private data was used for training or tuning Large Language Models (LLMs).
          </p>
         </div>
        </dd>
        <dt>
         <a name="item85">
          [85]
         </a>
         <a href="/abs/2402.15761" id="2402.15761" title="Abstract">
          arXiv:2402.15761
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2402.15761" href="/pdf/2402.15761" id="pdf-2402.15761" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2402.15761" href="https://arxiv.org/html/2402.15761v3" id="html-2402.15761" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2402.15761" href="/format/2402.15761" id="oth-2402.15761" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Res-VMamba: Fine-Grained Food Category Visual Classification Using Selective State Space Models with Deep Residual Learning
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+C">
            Chi-Sheng Chen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+G">
            Guan-Ying Chen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+D">
            Dong Zhou
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+D">
            Di Jiang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+D">
            Dai-Shi Chen
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           14 pages, 3 figures
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Food classification is the foundation for developing food vision tasks and plays a key role in the burgeoning field of computational nutrition. Due to the complexity of food requiring fine-grained classification, recent academic research mainly modifies Convolutional Neural Networks (CNNs) and/or Vision Transformers (ViTs) to perform food category classification. However, to learn fine-grained features, the CNN backbone needs additional structural design, whereas ViT, containing the self-attention module, has increased computational complexity. In recent months, a new Sequence State Space (S4) model, through a Selection mechanism and computation with a Scan (S6), colloquially termed Mamba, has demonstrated superior performance and computation efficiency compared to the Transformer architecture. The VMamba model, which incorporates the Mamba mechanism into image tasks (such as classification), currently establishes the state-of-the-art (SOTA) on the ImageNet dataset. In this research, we introduce an academically underestimated food dataset CNFOOD-241, and pioneer the integration of a residual learning framework within the VMamba model to concurrently harness both global and local state features inherent in the original VMamba architectural design. The research results show that VMamba surpasses current SOTA models in fine-grained and food classification. The proposed Res-VMamba further improves the classification accuracy to 79.54\% without pretrained weight. Our findings elucidate that our proposed methodology establishes a new benchmark for SOTA performance in food recognition on the CNFOOD-241 dataset. The code can be obtained on GitHub:
           <a class="link-external link-https" href="https://github.com/ChiShengChen/ResVMamba" rel="external noopener nofollow">
            this https URL
           </a>
           .
          </p>
         </div>
        </dd>
        <dt>
         <a name="item86">
          [86]
         </a>
         <a href="/abs/2403.00884" id="2403.00884" title="Abstract">
          arXiv:2403.00884
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2403.00884" href="/pdf/2403.00884" id="pdf-2403.00884" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2403.00884" href="https://arxiv.org/html/2403.00884v3" id="html-2403.00884" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2403.00884" href="/format/2403.00884" id="oth-2403.00884" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Zero-Shot Topic Classification of Column Headers: Leveraging LLMs for Metadata Enrichment
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Martorana,+M">
            Margherita Martorana
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kuhn,+T">
            Tobias Kuhn
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Stork,+L">
            Lise Stork
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=van+Ossenbruggen,+J">
            Jacco van Ossenbruggen
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Databases (cs.DB)
           </span>
           ; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)
          </div>
          <p class="mathjax">
           Traditional dataset retrieval systems rely on metadata for indexing, rather than on the underlying data values. However, high-quality metadata creation and enrichment often require manual annotations, which is a labour-intensive and challenging process to automate. In this study, we propose a method to support metadata enrichment using topic annotations generated by three Large Language Models (LLMs): ChatGPT-3.5, GoogleBard, and GoogleGemini. Our analysis focuses on classifying column headers based on domain-specific topics from the Consortium of European Social Science Data Archives (CESSDA), a Linked Data controlled vocabulary. Our approach operates in a zero-shot setting, integrating the controlled topic vocabulary directly within the input prompt. This integration serves as a Large Context Windows approach, with the aim of improving the results of the topic classification task.
           <br/>
           We evaluated the performance of the LLMs in terms of internal consistency, inter-machine alignment, and agreement with human classification. Additionally, we investigate the impact of contextual information (i.e., dataset description) on the classification outcomes. Our findings suggest that ChatGPT and GoogleGemini outperform GoogleBard in terms of internal consistency as well as LLM-human-agreement. Interestingly, we found that contextual information had no significant impact on LLM performance.
           <br/>
           This work proposes a novel approach that leverages LLMs for topic classification of column headers using a controlled vocabulary, presenting a practical application of LLMs and Large Context Windows within the Semantic Web domain. This approach has the potential to facilitate automated metadata enrichment, thereby enhancing dataset retrieval and the Findability, Accessibility, Interoperability, and Reusability (FAIR) of research data on the Web.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item87">
          [87]
         </a>
         <a href="/abs/2403.01875" id="2403.01875" title="Abstract">
          arXiv:2403.01875
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2403.01875" href="/pdf/2403.01875" id="pdf-2403.01875" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2403.01875" href="https://arxiv.org/html/2403.01875v2" id="html-2403.01875" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2403.01875" href="/format/2403.01875" id="oth-2403.01875" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Locally Convex Global Loss Network for Decision-Focused Learning
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jeon,+H">
            Haeun Jeon
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bae,+H">
            Hyunglip Bae
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Park,+M">
            Minsu Park
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kim,+C">
            Chanyeong Kim
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kim,+W+C">
            Woo Chang Kim
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           In decision-making problem under uncertainty, predicting unknown parameters is often considered independent of the optimization part. Decision-focused Learning (DFL) is a task-oriented framework to integrate prediction and optimization by adapting predictive model to give better decision for the corresponding task. Here, an inevitable challenge arises when computing gradients of the optimal decision with respect to the parameters. Existing researches cope this issue by smoothly reforming surrogate optimization or construct surrogate loss function that mimic task loss. However, they are applied to restricted optimization domain. In this paper, we propose Locally Convex Global Loss Network (LCGLN), a global surrogate loss model which can be implemented in a general DFL paradigm. LCGLN learns task loss via partial input convex neural network which is guaranteed to be convex for chosen inputs, while keeping the non-convex global structure for the other inputs. This enables LCGLN to admit general DFL through only a single surrogate loss without any sense for choosing appropriate parametric forms. We confirm effectiveness and flexibility of LCGLN by evaluating our proposed model with three stochastic decision-making problems.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item88">
          [88]
         </a>
         <a href="/abs/2403.07747" id="2403.07747" title="Abstract">
          arXiv:2403.07747
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2403.07747" href="/pdf/2403.07747" id="pdf-2403.07747" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2403.07747" href="https://arxiv.org/html/2403.07747v2" id="html-2403.07747" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2403.07747" href="/format/2403.07747" id="oth-2403.07747" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Y">
            Yan Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jin,+R">
            Renren Jin
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shi,+L">
            Ling Shi
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yao,+Z">
            Zheng Yao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiong,+D">
            Deyi Xiong
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           To thoroughly assess the mathematical reasoning abilities of Large Language Models (LLMs), we need to carefully curate evaluation datasets covering diverse mathematical concepts and mathematical problems at different difficulty levels. In pursuit of this objective, we propose FineMath in this paper, a fine-grained mathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath is created to cover the major key mathematical concepts taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of mathematical reasoning abilities of LLMs. All the 17 categories of math word problems are manually annotated with their difficulty levels according to the number of reasoning steps required to solve these problems. We conduct extensive experiments on a wide range of LLMs on FineMath and find that there is still considerable room for improvements in terms of mathematical reasoning capability of Chinese LLMs. We also carry out an in-depth analysis on the evaluation process and methods that have been overlooked previously. These two factors significantly influence the model results and our understanding of their mathematical reasoning capabilities. The dataset will be publicly available soon.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item89">
          [89]
         </a>
         <a href="/abs/2403.09948" id="2403.09948" title="Abstract">
          arXiv:2403.09948
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2403.09948" href="/pdf/2403.09948" id="pdf-2403.09948" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2403.09948" href="https://arxiv.org/html/2403.09948v2" id="html-2403.09948" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2403.09948" href="/format/2403.09948" id="oth-2403.09948" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+Z">
            Zhixiu Lu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+H">
            Hailong Li
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Parikh,+N+A">
            Nehal A. Parikh
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dillman,+J+R">
            Jonathan R. Dillman
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+L">
            Lili He
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           The integration of artificial intelligence (AI) with radiology marks a transformative era in medicine. Vision foundation models have been adopted to enhance radiologic imaging analysis. However, the distinct complexities of radiologic 2D and 3D radiologic data pose unique challenges that existing models, pre-trained on general non-medical images, fail to address adequately. To bridge this gap and capitalize on the diagnostic precision required in radiologic imaging, we introduce Radiologic Contrastive Language-Image Pre-training (RadCLIP): a cross-modal vision-language foundational model that harnesses Vision Language Pre-training (VLP) framework to improve radiologic image analysis. Building upon Contrastive Language-Image Pre-training (CLIP), RadCLIP incorporates a slice pooling mechanism tailored for volumetric image analysis and is pre-trained using a large and diverse dataset of radiologic image-text pairs. The RadCLIP was pre-trained to effectively align radiologic images with their corresponding text annotations, creating a robust vision backbone for radiologic images. Extensive experiments demonstrate RadCLIP's superior performance in both uni-modal radiologic image classification and cross-modal image-text matching, highlighting its significant promise for improving diagnostic accuracy and efficiency in clinical settings. Our Key contributions include curating a large dataset with diverse radiologic 2D/3D radiologic image-text pairs, a slice pooling adapter using an attention mechanism for integrating 2D images, and comprehensive evaluations of RadCLIP on various radiologic downstream tasks.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item90">
          [90]
         </a>
         <a href="/abs/2404.09932" id="2404.09932" title="Abstract">
          arXiv:2404.09932
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2404.09932" href="/pdf/2404.09932" id="pdf-2404.09932" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="oth-2404.09932" href="/format/2404.09932" id="oth-2404.09932" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Foundational Challenges in Assuring Alignment and Safety of Large Language Models
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Anwar,+U">
            Usman Anwar
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Saparov,+A">
            Abulhair Saparov
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rando,+J">
            Javier Rando
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Paleka,+D">
            Daniel Paleka
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Turpin,+M">
            Miles Turpin
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hase,+P">
            Peter Hase
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lubana,+E+S">
            Ekdeep Singh Lubana
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jenner,+E">
            Erik Jenner
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Casper,+S">
            Stephen Casper
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sourbut,+O">
            Oliver Sourbut
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Edelman,+B+L">
            Benjamin L. Edelman
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z">
            Zhaowei Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=G%C3%BCnther,+M">
            Mario GÃ¼nther
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Korinek,+A">
            Anton Korinek
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hernandez-Orallo,+J">
            Jose Hernandez-Orallo
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hammond,+L">
            Lewis Hammond
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bigelow,+E">
            Eric Bigelow
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pan,+A">
            Alexander Pan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Langosco,+L">
            Lauro Langosco
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Korbak,+T">
            Tomasz Korbak
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+H">
            Heidi Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhong,+R">
            Ruiqi Zhong
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=h%C3%89igeartaigh,+S+%C3%93">
            SeÃ¡n Ã hÃigeartaigh
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Recchia,+G">
            Gabriel Recchia
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Corsi,+G">
            Giulio Corsi
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chan,+A">
            Alan Chan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Anderljung,+M">
            Markus Anderljung
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Edwards,+L">
            Lilian Edwards
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Petrov,+A">
            Aleksandar Petrov
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=de+Witt,+C+S">
            Christian Schroeder de Witt
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Motwan,+S+R">
            Sumeet Ramesh Motwan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bengio,+Y">
            Yoshua Bengio
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+D">
            Danqi Chen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Torr,+P+H">
            Philip H.S. Torr
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Albanie,+S">
            Samuel Albanie
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Maharaj,+T">
            Tegan Maharaj
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Foerster,+J">
            Jakob Foerster
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tramer,+F">
            Florian Tramer
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+H">
            He He
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kasirzadeh,+A">
            Atoosa Kasirzadeh
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Choi,+Y">
            Yejin Choi
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Krueger,+D">
            David Krueger
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY)
          </div>
          <p class="mathjax">
           This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose $200+$ concrete research questions.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item91">
          [91]
         </a>
         <a href="/abs/2404.17400" id="2404.17400" title="Abstract">
          arXiv:2404.17400
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2404.17400" href="/pdf/2404.17400" id="pdf-2404.17400" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2404.17400" href="https://arxiv.org/html/2404.17400v2" id="html-2404.17400" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2404.17400" href="/format/2404.17400" id="oth-2404.17400" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Spatial-frequency Dual-Domain Feature Fusion Network for Low-Light Remote Sensing Image Enhancement
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yao,+Z">
            Zishu Yao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fan,+G">
            Guodong Fan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fan,+J">
            Jinfu Fan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gan,+M">
            Min Gan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+C+P">
            C.L. Philip Chen
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           14 page
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)
          </div>
          <p class="mathjax">
           Low-light remote sensing images generally feature high resolution and high spatial complexity, with continuously distributed surface features in space. This continuity in scenes leads to extensive long-range correlations in spatial domains within remote sensing images. Convolutional Neural Networks, which rely on local correlations for long-distance modeling, struggle to establish long-range correlations in such images. On the other hand, transformer-based methods that focus on global information face high computational complexities when processing high-resolution remote sensing images. From another perspective, Fourier transform can compute global information without introducing a large number of parameters, enabling the network to more efficiently capture the overall image structure and establish long-range correlations. Therefore, we propose a Dual-Domain Feature Fusion Network (DFFN) for low-light remote sensing image enhancement. Specifically, this challenging task of low-light enhancement is divided into two more manageable sub-tasks: the first phase learns amplitude information to restore image brightness, and the second phase learns phase information to refine details. To facilitate information exchange between the two phases, we designed an information fusion affine block that combines data from different phases and scales. Additionally, we have constructed two dark light remote sensing datasets to address the current lack of datasets in dark light remote sensing image enhancement. Extensive evaluations show that our method outperforms existing state-of-the-art methods. The code is available at
           <a class="link-external link-https" href="https://github.com/iijjlk/DFFN" rel="external noopener nofollow">
            this https URL
           </a>
           .
          </p>
         </div>
        </dd>
        <dt>
         <a name="item92">
          [92]
         </a>
         <a href="/abs/2406.02630" id="2406.02630" title="Abstract">
          arXiv:2406.02630
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2406.02630" href="/pdf/2406.02630" id="pdf-2406.02630" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="oth-2406.02630" href="/format/2406.02630" id="oth-2406.02630" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           AI Agents Under Threat: A Survey of Key Security Challenges and Future Pathways
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+Z">
            Zehang Deng
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+Y">
            Yongjian Guo
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Han,+C">
            Changzhou Han
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+W">
            Wanlun Ma
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiong,+J">
            Junwu Xiong
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wen,+S">
            Sheng Wen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiang,+Y">
            Yang Xiang
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Submitted to ACM Computing Survey
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Cryptography and Security (cs.CR)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           An Artificial Intelligence (AI) agent is a software entity that autonomously performs tasks or makes decisions based on pre-defined objectives and data inputs. AI agents, capable of perceiving user inputs, reasoning and planning tasks, and executing actions, have seen remarkable advancements in algorithm development and task performance. However, the security challenges they pose remain under-explored and unresolved. This survey delves into the emerging security threats faced by AI agents, categorizing them into four critical knowledge gaps: unpredictability of multi-step user inputs, complexity in internal executions, variability of operational environments, and interactions with untrusted external entities. By systematically reviewing these threats, this paper highlights both the progress made and the existing limitations in safeguarding AI agents. The insights provided aim to inspire further research into addressing the security threats associated with AI agents, thereby fostering the development of more robust and secure AI agent applications.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item93">
          [93]
         </a>
         <a href="/abs/2406.05572" id="2406.05572" title="Abstract">
          arXiv:2406.05572
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2406.05572" href="/pdf/2406.05572" id="pdf-2406.05572" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="oth-2406.05572" href="/format/2406.05572" id="oth-2406.05572" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Trust the PRoC3S: Solving Long-Horizon Robotics Problems with LLMs and Constraint Satisfaction
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Curtis,+A">
            Aidan Curtis
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar,+N">
            Nishanth Kumar
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cao,+J">
            Jing Cao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lozano-P%C3%A9rez,+T">
            TomÃ¡s Lozano-PÃ©rez
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kaelbling,+L+P">
            Leslie Pack Kaelbling
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Robotics (cs.RO)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Recent developments in pretrained large language models (LLMs) applied to robotics have demonstrated their capacity for sequencing a set of discrete skills to achieve open-ended goals in simple robotic tasks. In this paper, we examine the topic of LLM planning for a set of continuously parameterized skills whose execution must avoid violations of a set of kinematic, geometric, and physical constraints. We prompt the LLM to output code for a function with open parameters, which, together with environmental constraints, can be viewed as a Continuous Constraint Satisfaction Problem (CCSP). This CCSP can be solved through sampling or optimization to find a skill sequence and continuous parameter settings that achieve the goal while avoiding constraint violations. Additionally, we consider cases where the LLM proposes unsatisfiable CCSPs, such as those that are kinematically infeasible, dynamically unstable, or lead to collisions, and re-prompt the LLM to form a new CCSP accordingly. Experiments across three different simulated 3D domains demonstrate that our proposed strategy, PRoC3S, is capable of solving a wide range of complex manipulation tasks with realistic constraints on continuous parameters much more efficiently and effectively than existing baselines.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item94">
          [94]
         </a>
         <a href="/abs/2407.09050" id="2407.09050" title="Abstract">
          arXiv:2407.09050
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2407.09050" href="/pdf/2407.09050" id="pdf-2407.09050" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2407.09050" href="https://arxiv.org/html/2407.09050v2" id="html-2407.09050" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2407.09050" href="/format/2407.09050" id="oth-2407.09050" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Refusing Safe Prompts for Multi-modal Large Language Models
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shao,+Z">
            Zedian Shao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+H">
            Hongbin Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+Y">
            Yuepeng Hu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gong,+N+Z">
            Neil Zhenqiang Gong
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Cryptography and Security (cs.CR)
           </span>
           ; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)
          </div>
          <p class="mathjax">
           Multimodal large language models (MLLMs) have become the cornerstone of today's generative AI ecosystem, sparking intense competition among tech giants and startups. In particular, an MLLM generates a text response given a prompt consisting of an image and a question. While state-of-the-art MLLMs use safety filters and alignment techniques to refuse unsafe prompts, in this work, we introduce MLLM-Refusal, the first method that induces refusals for safe prompts. In particular, our MLLM-Refusal optimizes a nearly-imperceptible refusal perturbation and adds it to an image, causing target MLLMs to likely refuse a safe prompt containing the perturbed image and a safe question. Specifically, we formulate MLLM-Refusal as a constrained optimization problem and propose an algorithm to solve it. Our method offers competitive advantages for MLLM model providers by potentially disrupting user experiences of competing MLLMs, since competing MLLM's users will receive unexpected refusals when they unwittingly use these perturbed images in their prompts. We evaluate MLLM-Refusal on four MLLMs across four datasets, demonstrating its effectiveness in causing competing MLLMs to refuse safe prompts while not affecting non-competing MLLMs. Furthermore, we explore three potential countermeasures-adding Gaussian noise, DiffPure, and adversarial training. Our results show that though they can mitigate MLLM-Refusal's effectiveness, they also sacrifice the accuracy and/or efficiency of the competing MLLM. The code is available at
           <a class="link-external link-https" href="https://github.com/Sadcardation/MLLM-Refusal" rel="external noopener nofollow">
            this https URL
           </a>
           .
          </p>
         </div>
        </dd>
        <dt>
         <a name="item95">
          [95]
         </a>
         <a href="/abs/2407.11041" id="2407.11041" title="Abstract">
          arXiv:2407.11041
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2407.11041" href="/pdf/2407.11041" id="pdf-2407.11041" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2407.11041" href="https://arxiv.org/html/2407.11041v2" id="html-2407.11041" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2407.11041" href="/format/2407.11041" id="oth-2407.11041" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ling,+T">
            Tianheng Ling
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qian,+C">
            Chao Qian
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schiele,+G">
            Gregor Schiele
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           7 pages, 3 figures, 4 tables. The paper was accepted by 2024 IEEE Annual Congress on Artificial Intelligence of Things (IEEE AIoT) and got best paper award
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           This paper presents the design of a hardware accelerator for Transformers, optimized for on-device time-series forecasting in AIoT systems. It integrates integer-only quantization and Quantization-Aware Training with optimized hardware designs to realize 6-bit and 4-bit quantized Transformer models, which achieved precision comparable to 8-bit quantized models from related research. Utilizing a complete implementation on an embedded FPGA (Xilinx Spartan-7 XC7S15), we examine the feasibility of deploying Transformer models on embedded IoT devices. This includes a thorough analysis of achievable precision, resource utilization, timing, power, and energy consumption for on-device inference. Our results indicate that while sufficient performance can be attained, the optimization process is not trivial. For instance, reducing the quantization bitwidth does not consistently result in decreased latency or energy consumption, underscoring the necessity of systematically exploring various optimization combinations. Compared to an 8-bit quantized Transformer model in related studies, our 4-bit quantized Transformer model increases test loss by only 0.63%, operates up to 132.33x faster, and consumes 48.19x less energy.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item96">
          [96]
         </a>
         <a href="/abs/2407.13492" id="2407.13492" title="Abstract">
          arXiv:2407.13492
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2407.13492" href="/pdf/2407.13492" id="pdf-2407.13492" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2407.13492" href="https://arxiv.org/html/2407.13492v2" id="html-2407.13492" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2407.13492" href="/format/2407.13492" id="oth-2407.13492" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Enhancing Biomedical Knowledge Discovery for Diseases: An Open-Source Framework Applied on Rett Syndrome and Alzheimer's Disease
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Theodoropoulos,+C">
            Christos Theodoropoulos
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Coman,+A+C">
            Andrei Catalin Coman
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Henderson,+J">
            James Henderson
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Moens,+M">
            Marie-Francine Moens
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Under Review
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           The ever-growing volume of biomedical publications creates a critical need for efficient knowledge discovery. In this context, we introduce an open-source end-to-end framework designed to construct knowledge around specific diseases directly from raw text. To facilitate research in disease-related knowledge discovery, we create two annotated datasets focused on Rett syndrome and Alzheimer's disease, enabling the identification of semantic relations between biomedical entities. Extensive benchmarking explores various ways to represent relations and entity representations, offering insights into optimal modeling strategies for semantic relation detection and highlighting language models' competence in knowledge discovery. We also conduct probing experiments using different layer representations and attention scores to explore transformers' ability to capture semantic relations.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item97">
          [97]
         </a>
         <a href="/abs/2407.17844" id="2407.17844" title="Abstract">
          arXiv:2407.17844
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2407.17844" href="/pdf/2407.17844" id="pdf-2407.17844" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="oth-2407.17844" href="/format/2407.17844" id="oth-2407.17844" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Innovative Speech-Based Deep Learning Approaches for Parkinson's Disease Classification: A Systematic Review
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=van+Gelderen,+L">
            Lisanne van Gelderen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tejedor-Garc%C3%ADa,+C">
            Cristian Tejedor-GarcÃ­a
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           van Gelderen, L., &amp; Tejedor-GarcÃ­a, C. (2024). Innovative Speech-Based Deep Learning Approaches for Parkinson's Disease Classification: A Systematic Review. Applied Sciences, 14(17). doi:
           <a class="link-https link-external" data-doi="10.3390/app14177873" href="https://doi.org/10.3390/app14177873" rel="external noopener nofollow">
            https://doi.org/10.3390/app14177873
           </a>
           This research was funded by the NWO research programme NGF AiNed Fellowship Grants under the project Responsible AI for Voice Diagnostics (RAIVD) - grant number NGF.1607.22.013
          </div>
          <div class="list-journal-ref">
           <span class="descriptor">
            Journal-ref:
           </span>
           Appl. Sci. 2024, 14(17), 7873
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Sound (cs.SD)
           </span>
           ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)
          </div>
          <p class="mathjax">
           Parkinson's disease (PD), the second most prevalent neurodegenerative disorder worldwide, frequently presents with early-stage speech impairments. Recent advancements in Artificial Intelligence (AI), particularly deep learning (DL), have significantly enhanced PD diagnosis through the analysis of speech data. Nevertheless, the progress of research is restricted by the limited availability of publicly accessible speech-based PD datasets, primarily due to privacy concerns. The goal of this systematic review is to explore the current landscape of speech-based DL approaches for PD classification, based on 33 scientific works published between January 2020 and March 2024. We discuss their available resources, capabilities, and potential limitations, and issues related to bias, explainability, and privacy. Furthermore, this review provides an overview of publicly accessible speech-based datasets and open-source material for PD. The DL approaches identified are categorized into end-to-end (E2E) learning, transfer learning (TL), and deep acoustic feature extraction (DAFE). Among E2E approaches, Convolutional Neural Networks (CNNs) are prevalent, though Transformers are increasingly popular. E2E approaches face challenges such as limited data and computational resources, especially with Transformers. TL addresses these issues by providing more robust PD diagnosis and better generalizability across languages. DAFE aims to improve the explainability and interpretability of results by examining the specific effects of deep features on both other DL approaches and more traditional machine learning (ML) methods. However, it often underperforms compared to E2E and TL approaches.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item98">
          [98]
         </a>
         <a href="/abs/2407.18521" id="2407.18521" title="Abstract">
          arXiv:2407.18521
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2407.18521" href="/pdf/2407.18521" id="pdf-2407.18521" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="oth-2407.18521" href="/format/2407.18521" id="oth-2407.18521" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Patched MOA: optimizing inference for diverse software development tasks
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sharma,+A">
            Asankhaya Sharma
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Software Engineering (cs.SE)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           This paper introduces Patched MOA (Mixture of Agents), an inference optimization technique that significantly enhances the performance of large language models (LLMs) across diverse software development tasks. We evaluate three inference optimization algorithms - Best of N, Mixture of Agents, and Monte Carlo Tree Search and demonstrate that Patched MOA can boost the performance of smaller models to surpass that of larger, more expensive models. Notably, our approach improves the gpt-4o-mini model's performance on the Arena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of the cost. We also apply Patched MOA to various software development workflows, showing consistent improvements in task completion rates. Our method is model-agnostic, transparent to end-users, and can be easily integrated into existing LLM pipelines. This work contributes to the growing field of LLM optimization, offering a cost-effective solution for enhancing model performance without the need for fine-tuning or larger models. Our implementation is open-source and available at
           <a class="link-external link-https" href="https://github.com/codelion/optillm" rel="external noopener nofollow">
            this https URL
           </a>
           .
          </p>
         </div>
        </dd>
        <dt>
         <a name="item99">
          [99]
         </a>
         <a href="/abs/2408.02049" id="2408.02049" title="Abstract">
          arXiv:2408.02049
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2408.02049" href="/pdf/2408.02049" id="pdf-2408.02049" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2408.02049" href="https://arxiv.org/html/2408.02049v3" id="html-2408.02049" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2408.02049" href="/format/2408.02049" id="oth-2408.02049" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           3D Single-object Tracking in Point Clouds with High Temporal Variation
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+Q">
            Qiao Wu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+K">
            Kun Sun
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=An,+P">
            Pei An
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Salzmann,+M">
            Mathieu Salzmann
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Y">
            Yanning Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+J">
            Jiaqi Yang
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Accepted by ECCV24
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computer Vision and Pattern Recognition (cs.CV)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           The high temporal variation of the point clouds is the key challenge of 3D single-object tracking (3D SOT). Existing approaches rely on the assumption that the shape variation of the point clouds and the motion of the objects across neighboring frames are smooth, failing to cope with high temporal variation data. In this paper, we present a novel framework for 3D SOT in point clouds with high temporal variation, called HVTrack. HVTrack proposes three novel components to tackle the challenges in the high temporal variation scenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud shape variations; 2) a Base-Expansion Feature Cross-Attention module to deal with similar object distractions in expanded search areas; 3) a Contextual Point Guided Self-Attention module for suppressing heavy background noise. We construct a dataset with high temporal variation (KITTI-HV) by setting different frame intervals for sampling in the KITTI dataset. On the KITTI-HV with 5 frame intervals, our HVTrack surpasses the state-of-the-art tracker CXTracker by 11.3%/15.7% in Success/Precision.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item100">
          [100]
         </a>
         <a href="/abs/2408.02402" id="2408.02402" title="Abstract">
          arXiv:2408.02402
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2408.02402" href="/pdf/2408.02402" id="pdf-2408.02402" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="oth-2408.02402" href="/format/2408.02402" id="oth-2408.02402" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Enhancing AI-based Generation of Software Exploits with Contextual Information
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liguori,+P">
            Pietro Liguori
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Improta,+C">
            Cristina Improta
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Natella,+R">
            Roberto Natella
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cukic,+B">
            Bojan Cukic
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cotroneo,+D">
            Domenico Cotroneo
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Accepted for publication at The 35th IEEE International Symposium on Software Reliability Engineering
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Software Engineering (cs.SE)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           This practical experience report explores Neural Machine Translation (NMT) models' capability to generate offensive security code from natural language (NL) descriptions, highlighting the significance of contextual understanding and its impact on model performance. Our study employs a dataset comprising real shellcodes to evaluate the models across various scenarios, including missing information, necessary context, and unnecessary context. The experiments are designed to assess the models' resilience against incomplete descriptions, their proficiency in leveraging context for enhanced accuracy, and their ability to discern irrelevant information. The findings reveal that the introduction of contextual data significantly improves performance. However, the benefits of additional context diminish beyond a certain point, indicating an optimal level of contextual information for model training. Moreover, the models demonstrate an ability to filter out unnecessary context, maintaining high levels of accuracy in the generation of offensive security code. This study paves the way for future research on optimizing context use in AI-driven code generation, particularly for applications requiring a high degree of technical precision such as the generation of offensive code.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item101">
          [101]
         </a>
         <a href="/abs/2408.07482" id="2408.07482" title="Abstract">
          arXiv:2408.07482
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2408.07482" href="/pdf/2408.07482" id="pdf-2408.07482" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2408.07482" href="https://arxiv.org/html/2408.07482v2" id="html-2408.07482" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2408.07482" href="/format/2408.07482" id="oth-2408.07482" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Training Overhead Ratio: A Practical Reliability Metric for Large Language Model Training Systems
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+N">
            Ning Lu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+Q">
            Qian Xie
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+H">
            Hao Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fang,+W">
            Wenyi Fang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+Y">
            Yang Zheng
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+Z">
            Zheng Hu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+J">
            Jiantao Ma
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           To be published in: IEEE International Symposium on Software Reliability Engineering (ISSRE2024) workshop
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Distributed, Parallel, and Cluster Computing (cs.DC)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Large Language Models (LLMs) are revolutionizing the AI industry with their superior capabilities. Training these models requires large-scale GPU clusters and significant computing time, leading to frequent failures that significantly increase training costs. Despite its significance, this field lacks a metric for evaluating reliability. In this work, we introduce a novel reliability metric called \emph{Training Overhead Ratio} (TOR) to evaluate the reliability of fault-tolerant LLM training systems. TOR is defined as the ratio of optimal training time to the observed training time of a system, serving as a practical tool for users to estimate the actual time required to train an LLM on a given system. Furthermore, our investigation identifies the key factor for enhancing reliability and present TOR equations for various types of failures encountered in practice.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item102">
          [102]
         </a>
         <a href="/abs/2408.08632" id="2408.08632" title="Abstract">
          arXiv:2408.08632
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2408.08632" href="/pdf/2408.08632" id="pdf-2408.08632" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2408.08632" href="https://arxiv.org/html/2408.08632v2" id="html-2408.08632" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2408.08632" href="/format/2408.08632" id="oth-2408.08632" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           A Survey on Benchmarks of Multimodal Large Language Models
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J">
            Jian Li
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+W">
            Weiheng Lu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fei,+H">
            Hao Fei
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luo,+M">
            Meng Luo
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dai,+M">
            Ming Dai
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xia,+M">
            Min Xia
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jin,+Y">
            Yizhang Jin
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gan,+Z">
            Zhenye Gan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qi,+D">
            Ding Qi
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fu,+C">
            Chaoyou Fu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tai,+Y">
            Ying Tai
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+W">
            Wankou Yang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y">
            Yabiao Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+C">
            Chengjie Wang
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)
          </div>
          <p class="mathjax">
           Multimodal Large Language Models (MLLMs) are gaining increasing popularity in both academia and industry due to their remarkable performance in various applications such as visual question answering, visual perception, understanding, and reasoning. Over the past few years, significant efforts have been made to examine MLLMs from multiple perspectives. This paper presents a comprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on (1)perception and understanding, (2)cognition and reasoning, (3)specific domains, (4)key capabilities, and (5)other modalities. Finally, we discuss the limitations of the current evaluation methods for MLLMs and explore promising future directions. Our key argument is that evaluation should be regarded as a crucial discipline to support the development of MLLMs better. For more details, please visit our GitHub repository:
           <a class="link-external link-https" href="https://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey" rel="external noopener nofollow">
            this https URL
           </a>
           .
          </p>
         </div>
        </dd>
        <dt>
         <a name="item103">
          [103]
         </a>
         <a href="/abs/2408.08651" id="2408.08651" title="Abstract">
          arXiv:2408.08651
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2408.08651" href="/pdf/2408.08651" id="pdf-2408.08651" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2408.08651" href="https://arxiv.org/html/2408.08651v2" id="html-2408.08651" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2408.08651" href="/format/2408.08651" id="oth-2408.08651" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Reasoning Beyond Bias: A Study on Counterfactual Prompting and Chain of Thought Reasoning
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Moore,+K">
            Kyle Moore
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Roberts,+J">
            Jesse Roberts
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pham,+T">
            Thao Pham
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fisher,+D">
            Douglas Fisher
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Language models are known to absorb biases from their training data, leading to predictions driven by statistical regularities rather than semantic relevance. We investigate the impact of these biases on answer choice preferences in the Massive Multi-Task Language Understanding (MMLU) task. Our findings reveal that differences in learned regularities across answer options are predictive of model preferences and mirror human test-taking strategies. To address this issue, we introduce two novel methods: Counterfactual Prompting with Chain of Thought (CoT) and Counterfactual Prompting with Agnostically Primed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with CoT alone is insufficient to mitigate bias, our novel Primed Counterfactual Prompting with CoT approach effectively reduces the influence of base-rate probabilities while improving overall accuracy. Our results suggest that mitigating bias requires a "System-2" like process and that CoT reasoning is susceptible to confirmation bias under some prompting methodologies. Our contributions offer practical solutions for developing more robust and fair language models.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item104">
          [104]
         </a>
         <a href="/abs/2408.09236" id="2408.09236" title="Abstract">
          arXiv:2408.09236
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2408.09236" href="/pdf/2408.09236" id="pdf-2408.09236" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="oth-2408.09236" href="/format/2408.09236" id="oth-2408.09236" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Hybrid Semantic Search: Unveiling User Intent Beyond Keywords
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ahluwalia,+A">
            Aman Ahluwalia
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sutradhar,+B">
            Bishwajit Sutradhar
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ghosh,+K">
            Karishma Ghosh
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yadav,+I">
            Indrapal Yadav
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sheetal,+A">
            Arpan Sheetal
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Patil,+P">
            Prashant Patil
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Information Retrieval (cs.IR)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           This paper addresses the limitations of traditional keyword-based search in understanding user intent and introduces a novel hybrid search approach that leverages the strengths of non-semantic search engines, Large Language Models (LLMs), and embedding models. The proposed system integrates keyword matching, semantic vector embeddings, and LLM-generated structured queries to deliver highly relevant and contextually appropriate search results. By combining these complementary methods, the hybrid approach effectively captures both explicit and implicit user intent.The paper further explores techniques to optimize query execution for faster response times and demonstrates the effectiveness of this hybrid search model in producing comprehensive and accurate search outcomes.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item105">
          [105]
         </a>
         <a href="/abs/2408.12787" id="2408.12787" title="Abstract">
          arXiv:2408.12787
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2408.12787" href="/pdf/2408.12787" id="pdf-2408.12787" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2408.12787" href="https://arxiv.org/html/2408.12787v2" id="html-2408.12787" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2408.12787" href="/format/2408.12787" id="oth-2408.12787" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           LLM-PBE: Assessing Data Privacy in Large Language Models
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Q">
            Qinbin Li
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hong,+J">
            Junyuan Hong
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+C">
            Chulin Xie
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tan,+J">
            Jeffrey Tan
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xin,+R">
            Rachel Xin
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hou,+J">
            Junyi Hou
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yin,+X">
            Xavier Yin
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">
            Zhun Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hendrycks,+D">
            Dan Hendrycks
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">
            Zhangyang Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+B">
            Bo Li
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+B">
            Bingsheng He
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+D">
            Dawn Song
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Cryptography and Security (cs.CR)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Large Language Models (LLMs) have become integral to numerous domains, significantly advancing applications in data management, mining, and analysis. Their profound capabilities in processing and interpreting complex language data, however, bring to light pressing concerns regarding data privacy, especially the risk of unintentional training data leakage. Despite the critical nature of this issue, there has been no existing literature to offer a comprehensive assessment of data privacy risks in LLMs. Addressing this gap, our paper introduces LLM-PBE, a toolkit crafted specifically for the systematic evaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze privacy across the entire lifecycle of LLMs, incorporating diverse attack and defense strategies, and handling various data types and metrics. Through detailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth exploration of data privacy concerns, shedding light on influential factors such as model size, data characteristics, and evolving temporal dimensions. This study not only enriches the understanding of privacy issues in LLMs but also serves as a vital resource for future research in the field. Aimed at enhancing the breadth of knowledge in this area, the findings, resources, and our full technical report are made available at
           <a class="link-external link-https" href="https://llm-pbe.github.io/" rel="external noopener nofollow">
            this https URL
           </a>
           , providing an open platform for academic and practical advancements in LLM privacy assessment.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item106">
          [106]
         </a>
         <a href="/abs/2408.14042" id="2408.14042" title="Abstract">
          arXiv:2408.14042
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2408.14042" href="/pdf/2408.14042" id="pdf-2408.14042" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2408.14042" href="https://arxiv.org/html/2408.14042v2" id="html-2408.14042" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2408.14042" href="/format/2408.14042" id="oth-2408.14042" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           PAGE: Parametric Generative Explainer for Graph Neural Network
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qiu,+Y">
            Yang Qiu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+W">
            Wei Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+J">
            Jun Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+R">
            Ruixuan Li
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           This article introduces PAGE, a parameterized generative interpretive framework. PAGE is capable of providing faithful explanations for any graph neural network without necessitating prior knowledge or internal details. Specifically, we train the auto-encoder to generate explanatory substructures by designing appropriate training strategy. Due to the dimensionality reduction of features in the latent space of the auto-encoder, it becomes easier to extract causal features leading to the model's output, which can be easily employed to generate explanations. To accomplish this, we introduce an additional discriminator to capture the causality between latent causal features and the model's output. By designing appropriate optimization objectives, the well-trained discriminator can be employed to constrain the encoder in generating enhanced causal features. Finally, these features are mapped to substructures of the input graph through the decoder to serve as explanations. Compared to existing methods, PAGE operates at the sample scale rather than nodes or edges, eliminating the need for perturbation or encoding processes as seen in previous methods. Experimental results on both artificially synthesized and real-world datasets demonstrate that our approach not only exhibits the highest faithfulness and accuracy but also significantly outperforms baseline models in terms of efficiency.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item107">
          [107]
         </a>
         <a href="/abs/2408.14418" id="2408.14418" title="Abstract">
          arXiv:2408.14418
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2408.14418" href="/pdf/2408.14418" id="pdf-2408.14418" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2408.14418" href="https://arxiv.org/html/2408.14418v2" id="html-2408.14418" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2408.14418" href="/format/2408.14418" id="oth-2408.14418" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Binici,+K">
            Kuluhan Binici
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kashyap,+A+R">
            Abhinav Ramesh Kashyap
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schlegel,+V">
            Viktor Schlegel
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+A+T">
            Andy T. Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dwivedi,+V+P">
            Vijay Prakash Dwivedi
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nguyen,+T">
            Thanh-Tung Nguyen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+X">
            Xiaoxue Gao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+N+F">
            Nancy F. Chen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Winkler,+S">
            Stefan Winkler
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech into text, yet the errors they introduce can significantly degrade the performance of downstream tasks like summarization. This issue is particularly pronounced in clinical dialogue summarization, a low-resource domain where supervised data for fine-tuning is scarce, necessitating the use of ASR models as black-box solutions. Employing conventional data augmentation for enhancing the noise robustness of summarization models is not feasible either due to the unavailability of sufficient medical dialogue audio recordings and corresponding ASR transcripts. To address this challenge, we propose MEDSAGE, an approach for generating synthetic samples for data augmentation using Large Language Models (LLMs). Specifically, we leverage the in-context learning capabilities of LLMs and instruct them to generate ASR-like errors based on a few available medical dialogue examples with audio recordings. Experimental results show that LLMs can effectively model ASR noise, and incorporating this noisy data into the training process significantly improves the robustness and accuracy of medical dialogue summarization systems. This approach addresses the challenges of noisy ASR outputs in critical applications, offering a robust solution to enhance the reliability of clinical dialogue summarization.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item108">
          [108]
         </a>
         <a href="/abs/2408.15508" id="2408.15508" title="Abstract">
          arXiv:2408.15508
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2408.15508" href="/pdf/2408.15508" id="pdf-2408.15508" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2408.15508" href="https://arxiv.org/html/2408.15508v2" id="html-2408.15508" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2408.15508" href="/format/2408.15508" id="oth-2408.15508" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           EmoAttack: Utilizing Emotional Voice Conversion for Speech Backdoor Attacks on Deep Speech Classification Models
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yao,+W">
            Wenhan Yao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Z+X">
            Zedong XingXiarun Chen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J">
            Jia Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+y">
            yongqiang He
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wen,+W">
            Weiping Wen
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Submitted to ICASSP 2025
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Sound (cs.SD)
           </span>
           ; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)
          </div>
          <p class="mathjax">
           Deep speech classification tasks, mainly including keyword spotting and speaker verification, play a crucial role in speech-based human-computer interaction. Recently, the security of these technologies has been demonstrated to be vulnerable to backdoor attacks. Specifically speaking, speech samples are attacked by noisy disruption and component modification in present triggers. We suggest that speech backdoor attacks can strategically focus on emotion, a higher-level subjective perceptual attribute inherent in speech. Furthermore, we proposed that emotional voice conversion technology can serve as the speech backdoor attack trigger, and the method is called EmoAttack. Based on this, we conducted attack experiments on two speech classification tasks, showcasing that EmoAttack method owns impactful trigger effectiveness and its remarkable attack success rate and accuracy variance. Additionally, the ablation experiments found that speech with intensive emotion is more suitable to be targeted for attacks.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item109">
          [109]
         </a>
         <a href="/abs/2408.16966" id="2408.16966" title="Abstract">
          arXiv:2408.16966
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2408.16966" href="/pdf/2408.16966" id="pdf-2408.16966" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2408.16966" href="https://arxiv.org/html/2408.16966v2" id="html-2408.16966" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2408.16966" href="/format/2408.16966" id="oth-2408.16966" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           UserSumBench: A Benchmark Framework for Evaluating User Summarization Approaches
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+C">
            Chao Wang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+N">
            Neo Wu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ning,+L">
            Lin Ning
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+J">
            Jiaxing Wu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+L">
            Luyang Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+J">
            Jun Xie
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=O'Banion,+S">
            Shawn O'Banion
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Green,+B">
            Bradley Green
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)
          </div>
          <p class="mathjax">
           Large language models (LLMs) have shown remarkable capabilities in generating user summaries from a long list of raw user activity data. These summaries capture essential user information such as preferences and interests, and therefore are invaluable for LLM-based personalization applications, such as explainable recommender systems. However, the development of new summarization techniques is hindered by the lack of ground-truth labels, the inherent subjectivity of user summaries, and human evaluation which is often costly and time-consuming. To address these challenges, we introduce \UserSumBench, a benchmark framework designed to facilitate iterative development of LLM-based summarization approaches. This framework offers two key components: (1) A reference-free summary quality metric. We show that this metric is effective and aligned with human preferences across three diverse datasets (MovieLens, Yelp and Amazon Review). (2) A novel robust summarization method that leverages time-hierarchical summarizer and self-critique verifier to produce high-quality summaries while eliminating hallucination. This method serves as a strong baseline for further innovation in summarization techniques.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item110">
          [110]
         </a>
         <a href="/abs/2409.00125" id="2409.00125" title="Abstract">
          arXiv:2409.00125
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2409.00125" href="/pdf/2409.00125" id="pdf-2409.00125" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.00125" href="https://arxiv.org/html/2409.00125v3" id="html-2409.00125" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.00125" href="/format/2409.00125" id="oth-2409.00125" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           A Hybrid Framework for Spatial Interpolation: Merging Data-driven with Domain Knowledge
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+C">
            Cong Zhang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Du,+S">
            Shuyi Du
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+H">
            Hongqing Song
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y">
            Yuhe Wang
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           21 pages, 13 figures; typos corrected, references updated; few typos in few equations corrected, changed to Tex source
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
          </div>
          <p class="mathjax">
           Estimating spatially distributed information through the interpolation of scattered observation datasets often overlooks the critical role of domain knowledge in understanding spatial dependencies. Additionally, the features of these data sets are typically limited to the spatial coordinates of the scattered observation locations. In this paper, we propose a hybrid framework that integrates data-driven spatial dependency feature extraction with rule-assisted spatial dependency function mapping to augment domain knowledge. We demonstrate the superior performance of our framework in two comparative application scenarios, highlighting its ability to capture more localized spatial features in the reconstructed distribution fields. Furthermore, we underscore its potential to enhance nonlinear estimation capabilities through the application of transformed fuzzy rules and to quantify the inherent uncertainties associated with the observation data sets. Our framework introduces an innovative approach to spatial information estimation by synergistically combining observational data with rule-assisted domain knowledge.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item111">
          [111]
         </a>
         <a href="/abs/2409.01345" id="2409.01345" title="Abstract">
          arXiv:2409.01345
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2409.01345" href="/pdf/2409.01345" id="pdf-2409.01345" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.01345" href="https://arxiv.org/html/2409.01345v2" id="html-2409.01345" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.01345" href="/format/2409.01345" id="oth-2409.01345" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Language Models Benefit from Preparation with Elicited Knowledge
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+J">
            Jiacan Yu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=An,+H">
            Hannah An
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schubert,+L+K">
            Lenhart K. Schubert
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           The zero-shot chain of thought (CoT) approach is often used in question answering (QA) by language models (LMs) for tasks that require multiple reasoning steps, typically enhanced by the prompt "Let's think step by step." However, some QA tasks hinge more on accessing relevant knowledge than on chaining reasoning steps. We introduce a simple general prompting technique, called PREP, that involves using two instances of LMs: the first (LM1) generates relevant information, and the second (LM2) answers the question based on this information. PREP is designed to be general and independent of the user's domain knowledge, making it applicable across various QA tasks without the need for specialized prompt engineering. To evaluate the effectiveness of our prompting method, we create a dataset of 100 binary-choice questions, derived from an extensive schematic dataset on artifact parts and material composition. These questions ask which of two artifacts is less likely to share materials with another artifact. Such questions probe the LM's knowledge of shared materials in the part structure of different artifacts. We test our method on our dataset and three published commonsense reasoning datasets. The average accuracy of our method is consistently higher than that of all the other tested methods across all the tested datasets.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item112">
          [112]
         </a>
         <a href="/abs/2409.01588" id="2409.01588" title="Abstract">
          arXiv:2409.01588
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2409.01588" href="/pdf/2409.01588" id="pdf-2409.01588" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.01588" href="https://arxiv.org/html/2409.01588v2" id="html-2409.01588" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.01588" href="/format/2409.01588" id="oth-2409.01588" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Large-scale Urban Facility Location Selection with Knowledge-informed Reinforcement Learning
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Su,+H">
            Hongyuan Su
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+Y">
            Yu Zheng
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ding,+J">
            Jingtao Ding
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jin,+D">
            Depeng Jin
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y">
            Yong Li
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           Sigspatial2024
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)
          </div>
          <p class="mathjax">
           The facility location problem (FLP) is a classical combinatorial optimization challenge aimed at strategically laying out facilities to maximize their accessibility. In this paper, we propose a reinforcement learning method tailored to solve large-scale urban FLP, capable of producing near-optimal solutions at superfast inference speed. We distill the essential swap operation from local search, and simulate it by intelligently selecting edges on a graph of urban regions, guided by a knowledge-informed graph neural network, thus sidestepping the need for heavy computation of local search. Extensive experiments on four US cities with different geospatial conditions demonstrate that our approach can achieve comparable performance to commercial solvers with less than 5\% accessibility loss, while displaying up to 1000 times speedup. We deploy our model as an online geospatial application at
           <a class="link-external link-https" href="https://huggingface.co/spaces/randommmm/MFLP" rel="external noopener nofollow">
            this https URL
           </a>
           .
          </p>
         </div>
        </dd>
        <dt>
         <a name="item113">
          [113]
         </a>
         <a href="/abs/2409.01668" id="2409.01668" title="Abstract">
          arXiv:2409.01668
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2409.01668" href="/pdf/2409.01668" id="pdf-2409.01668" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.01668" href="https://arxiv.org/html/2409.01668v2" id="html-2409.01668" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.01668" href="/format/2409.01668" id="oth-2409.01668" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Pureformer-VC: Non-parallel One-Shot Voice Conversion with Pure Transformer Blocks and Triplet Discriminative Training
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yao,+W">
            Wenhan Yao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xing,+Z">
            Zedong Xing
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+X">
            Xiarun Chen
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J">
            Jia Liu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+Y">
            Yongqiang He
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wen,+W">
            Weiping Wen
           </a>
          </div>
          <div class="list-comments mathjax">
           <span class="descriptor">
            Comments:
           </span>
           submmited to ICASSP 2025
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Sound (cs.SD)
           </span>
           ; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)
          </div>
          <p class="mathjax">
           One-shot voice conversion(VC) aims to change the timbre of any source speech to match that of the target speaker with only one speech sample. Existing style transfer-based VC methods relied on speech representation disentanglement and suffered from accurately and independently encoding each speech component and recomposing back to converted speech effectively. To tackle this, we proposed Pureformer-VC, which utilizes Conformer blocks to build a disentangled encoder, and Zipformer blocks to build a style transfer decoder as the generator. In the decoder, we used effective styleformer blocks to integrate speaker characteristics effectively into the generated speech. The models used the generative VAE loss for encoding components and triplet loss for unsupervised discriminative training. We applied the styleformer method to Zipformer's shared weights for style transfer. The experimental results show that the proposed model achieves comparable subjective scores and exhibits improvements in objective metrics compared to existing methods in a one-shot voice conversion scenario.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item114">
          [114]
         </a>
         <a href="/abs/2409.02850" id="2409.02850" title="Abstract">
          arXiv:2409.02850
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2409.02850" href="/pdf/2409.02850" id="pdf-2409.02850" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.02850" href="https://arxiv.org/html/2409.02850v2" id="html-2409.02850" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.02850" href="/format/2409.02850" id="oth-2409.02850" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Oops, I Sampled it Again: Reinterpreting Confidence Intervals in Few-Shot Learning
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lafargue,+R">
            Raphael Lafargue
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Smith,+L">
            Luke Smith
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vermet,+F">
            Franck Vermet
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=L%C3%B6we,+M">
            Mathias LÃ¶we
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Reid,+I">
            Ian Reid
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gripon,+V">
            Vincent Gripon
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Valmadre,+J">
            Jack Valmadre
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Machine Learning (cs.LG)
           </span>
           ; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
          </div>
          <p class="mathjax">
           The predominant method for computing confidence intervals (CI) in few-shot learning (FSL) is based on sampling the tasks with replacement, i.e.\ allowing the same samples to appear in multiple tasks. This makes the CI misleading in that it takes into account the randomness of the sampler but not the data itself. To quantify the extent of this problem, we conduct a comparative analysis between CIs computed with and without replacement. These reveal a notable underestimation by the predominant method. This observation calls for a reevaluation of how we interpret confidence intervals and the resulting conclusions in FSL comparative studies. Our research demonstrates that the use of paired tests can partially address this issue. Additionally, we explore methods to further reduce the (size of the) CI by strategically sampling tasks of a specific size. We also introduce a new optimized benchmark, which can be accessed at
           <a class="link-external link-https" href="https://github.com/RafLaf/FSL-benchmark-again" rel="external noopener nofollow">
            this https URL
           </a>
          </p>
         </div>
        </dd>
        <dt>
         <a name="item115">
          [115]
         </a>
         <a href="/abs/2409.03274" id="2409.03274" title="Abstract">
          arXiv:2409.03274
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2409.03274" href="/pdf/2409.03274" id="pdf-2409.03274" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03274" href="https://arxiv.org/html/2409.03274v2" id="html-2409.03274" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03274" href="/format/2409.03274" id="oth-2409.03274" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           Recent Advances in Attack and Defense Approaches of Large Language Models
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cui,+J">
            Jing Cui
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Y">
            Yishi Xu
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+Z">
            Zhewei Huang
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+S">
            Shuchang Zhou
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiao,+J">
            Jianbin Jiao
           </a>
           ,
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+J">
            Junge Zhang
           </a>
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Cryptography and Security (cs.CR)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Large Language Models (LLMs) have revolutionized artificial intelligence and machine learning through their advanced text processing and generating capabilities. However, their widespread deployment has raised significant safety and reliability concerns. Established vulnerabilities in deep neural networks, coupled with emerging threat models, may compromise security evaluations and create a false sense of security. Given the extensive research in the field of LLM security, we believe that summarizing the current state of affairs will help the research community better understand the present landscape and inform future developments. This paper reviews current research on LLM vulnerabilities and threats, and evaluates the effectiveness of contemporary defense mechanisms. We analyze recent studies on attack vectors and model weaknesses, providing insights into attack mechanisms and the evolving threat landscape. We also examine current defense strategies, highlighting their strengths and limitations. By contrasting advancements in attack and defense methodologies, we identify research gaps and propose future directions to enhance LLM security. Our goal is to advance the understanding of LLM safety challenges and guide the development of more robust security measures.
          </p>
         </div>
        </dd>
        <dt>
         <a name="item116">
          [116]
         </a>
         <a href="/abs/2409.03381" id="2409.03381" title="Abstract">
          arXiv:2409.03381
         </a>
         (replaced)

        [
         <a aria-labelledby="pdf-2409.03381" href="/pdf/2409.03381" id="pdf-2409.03381" title="Download PDF">
          pdf
         </a>
         ,
         <a aria-labelledby="html-2409.03381" href="https://arxiv.org/html/2409.03381v2" id="html-2409.03381" rel="noopener noreferrer" target="_blank" title="View HTML">
          html
         </a>
         ,
         <a aria-labelledby="oth-2409.03381" href="/format/2409.03381" id="oth-2409.03381" title="Other formats">
          other
         </a>
         ]
        </dt>
        <dd>
         <div class="meta">
          <div class="list-title mathjax">
           <span class="descriptor">
            Title:
           </span>
           CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks
          </div>
          <div class="list-authors">
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+Y">
            Yongxin Deng
           </a>
           (1),
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qiu,+X">
            Xihe Qiu
           </a>
           (1),
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tan,+X">
            Xiaoyu Tan
           </a>
           (2),
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qu,+C">
            Chao Qu
           </a>
           (2),
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pan,+J">
            Jing Pan
           </a>
           (3),
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng,+Y">
            Yuan Cheng
           </a>
           (2),
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Y">
            Yinghui Xu
           </a>
           (4),
           <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chu,+W">
            Wei Chu
           </a>
           (2) ((1) School of Electronic and Electrical Engineering, Shanghai University of Engineering Science, Shanghai, China, (2) INF Technology (Shanghai) Co., Ltd., Shanghai, China, (3) School of Art, Design and Architecture, Monash University, Melbourne, Australia, (4) Artificial Intelligence Innovation and Incubation Institute, Fudan University, Shanghai, China)
          </div>
          <div class="list-subjects">
           <span class="descriptor">
            Subjects:
           </span>
           <span class="primary-subject">
            Computation and Language (cs.CL)
           </span>
           ; Artificial Intelligence (cs.AI)
          </div>
          <p class="mathjax">
           Cognitive psychology investigates perception, attention, memory, language, problem-solving, decision-making, and reasoning. Kahneman's dual-system theory elucidates the human decision-making process, distinguishing between the rapid, intuitive System 1 and the deliberative, rational System 2. Recent advancements have positioned large language Models (LLMs) as formidable tools nearing human-level proficiency in various cognitive tasks. Nonetheless, the presence of a dual-system framework analogous to human cognition in LLMs remains unexplored. This study introduces the \textbf{CogniDual Framework for LLMs} (CFLLMs), designed to assess whether LLMs can, through self-training, evolve from deliberate deduction to intuitive responses, thereby emulating the human process of acquiring and mastering new information. Our findings reveal the cognitive mechanisms behind LLMs' response generation, enhancing our understanding of their capabilities in cognitive psychology. Practically, self-trained models can provide faster responses to certain queries, reducing computational demands during inference.
          </p>
         </div>
        </dd>
       </dl>
       <div class="paging">
        Total of 116 entries
       </div>
       <div class="morefewer">
        Showing up to 2000 entries per page:
        <a href="/list/cs.AI/new?skip=0&amp;show=1000">
         fewer
        </a>
        |
        <span style="color: #454545">
         more
        </span>
        |
        <span style="color: #454545">
         all
        </span>
       </div>
      </div>
     </div>
    </div>
   </main>
   <footer style="clear: both;">
    <div aria-label="Secondary" class="columns is-desktop" role="navigation" style="margin: -0.75em -0.75em 0.75em -0.75em">
     <!-- Macro-Column 1 -->
     <div class="column" style="padding: 0;">
      <div class="columns">
       <div class="column">
        <ul style="list-style: none; line-height: 2;">
         <li>
          <a href="https://info.arxiv.org/about">
           About
          </a>
         </li>
         <li>
          <a href="https://info.arxiv.org/help">
           Help
          </a>
         </li>
        </ul>
       </div>
       <div class="column">
        <ul style="list-style: none; line-height: 2;">
         <li>
          <svg class="icon filter-black" role="presentation" viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg">
           <title>
            contact arXiv
           </title>
           <desc>
            Click here to contact arXiv
           </desc>
           <path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z">
           </path>
          </svg>
          <a href="https://info.arxiv.org/help/contact.html">
           Contact
          </a>
         </li>
         <li>
          <svg class="icon filter-black" role="presentation" viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg">
           <title>
            subscribe to arXiv mailings
           </title>
           <desc>
            Click here to subscribe
           </desc>
           <path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z">
           </path>
          </svg>
          <a href="https://info.arxiv.org/help/subscribe">
           Subscribe
          </a>
         </li>
        </ul>
       </div>
      </div>
     </div>
     <!-- End Macro-Column 1 -->
     <!-- Macro-Column 2 -->
     <div class="column" style="padding: 0;">
      <div class="columns">
       <div class="column">
        <ul style="list-style: none; line-height: 2;">
         <li>
          <a href="https://info.arxiv.org/help/license/index.html">
           Copyright
          </a>
         </li>
         <li>
          <a href="https://info.arxiv.org/help/policies/privacy_policy.html">
           Privacy Policy
          </a>
         </li>
        </ul>
       </div>
       <div class="column sorry-app-links">
        <ul style="list-style: none; line-height: 2;">
         <li>
          <a href="https://info.arxiv.org/help/web_accessibility.html">
           Web Accessibility Assistance
          </a>
         </li>
         <li>
          <p class="help">
           <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">
            arXiv Operational Status
            <svg class="icon filter-dark_grey" role="presentation" viewbox="0 0 256 512" xmlns="http://www.w3.org/2000/svg">
             <path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z">
             </path>
            </svg>
           </a>
           <br/>
           Get status notifications via
           <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank">
            <svg class="icon filter-black" role="presentation" viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg">
             <path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z">
             </path>
            </svg>
            email
           </a>
           or
           <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank">
            <svg class="icon filter-black" role="presentation" viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg">
             <path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z">
             </path>
            </svg>
            slack
           </a>
          </p>
         </li>
        </ul>
       </div>
      </div>
     </div>
     <!-- end MetaColumn 2 -->
     <!-- End Macro-Column 2 -->
    </div>
   </footer>
  </div>
  <script src="/static/base/1.0.1/js/member_acknowledgement.js">
  </script>
 </body>
</html>
